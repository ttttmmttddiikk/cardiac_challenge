{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "# from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run func_DL.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#---------------------------------------------\n",
    "#var\n",
    "path_dir_X = \"../data_X\"\n",
    "path_dir_Y = \"../data_Y_Task3\"\n",
    "n_test = 100\n",
    "n_val = 100\n",
    "batch_size = 100 #5000\n",
    "\n",
    "#---------------------------------------------\n",
    "#instance\n",
    "dataset = CustomDataset(path_dir_X=path_dir_X, path_dir_Y=path_dir_Y, n_test=n_test, n_val=n_val, batch_size=batch_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomDataset at 0x7ffebd10eee0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### var, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "#var (condition)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#----------------------------\n",
    "#var (train)\n",
    "num_epochs = 1000\n",
    "n_print_train_result = 1\n",
    "val_flag = True\n",
    "\n",
    "#----------------------------\n",
    "#init (model)\n",
    "in_channels = dataset.return_shape_X()[0]\n",
    "in_length = dataset.return_shape_X()[1]\n",
    "out_channels = dataset.return_shape_Y()[0]\n",
    "out_length = dataset.return_shape_Y()[1]\n",
    "model = model_task3(in_channels, in_length, out_channels, out_length, batch_size).to(device)\n",
    "#init model weight\n",
    "model.apply(init_normal_dist)\n",
    "#----------------------------\n",
    "#init (optimizer, scheduler)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,30,50,100,300,500], gamma=0.95)\n",
    "#----------------------------\n",
    "#init (loss_func)\n",
    "#https://neptune.ai/blog/pytorch-loss-functions\n",
    "loss_func = nn.MSELoss()\n",
    "#loss_func = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "#model.load_state_dict(torch.load('../models/model_task3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "1 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "2 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "3 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "4 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "5 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "6 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "7 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "8 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "9 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "10 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "11 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "12 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "13 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "14 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "15 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "16 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "17 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "18 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "19 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "20 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "21 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "22 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "23 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "24 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "25 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "26 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "27 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "28 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "29 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "30 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "31 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "32 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "33 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "34 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "35 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "36 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "37 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "38 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "39 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "40 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "41 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "42 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "43 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "44 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "45 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "46 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "47 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "48 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "49 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "50 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "51 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "52 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "53 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "54 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "55 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "56 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "57 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "58 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "59 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "60 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "61 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "62 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "63 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "64 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "65 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "66 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "67 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "68 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "69 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "70 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "71 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "72 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "73 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "74 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "75 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "76 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "77 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "78 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "79 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "80 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "81 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "82 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "83 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "84 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "85 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "86 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "87 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "88 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "89 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "90 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "91 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "92 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "93 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "94 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "95 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "96 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "97 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "98 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "99 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "100 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "101 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "102 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "103 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "104 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "105 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "106 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "107 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "108 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "109 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "110 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "111 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "112 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "113 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "114 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "115 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "116 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "117 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "118 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "119 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "120 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "121 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "122 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "123 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "124 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "125 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "126 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "127 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "128 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "129 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "130 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "131 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "132 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "133 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "134 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "135 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "136 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "137 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "138 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "139 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "140 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "141 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "142 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "143 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "144 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "145 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "146 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "147 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "148 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "149 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "150 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "151 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "152 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "153 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "154 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "155 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "156 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "157 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "158 torch.Size([100, 12, 500]) torch.Size([100, 75, 1])\n",
      "159 torch.Size([17, 12, 500]) torch.Size([17, 75, 1])\n"
     ]
    }
   ],
   "source": [
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for i, (x, y) in enumerate(dataloader):\n",
    "    x = dataset.change_data_setting_to_train(x)\n",
    "    y = dataset.change_data_setting_to_train(y)\n",
    "    print(i,x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_features, model_dim, num_heads, num_layers, seq_length, output_dim=75):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embed = nn.Linear(input_features, model_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, seq_length, model_dim))\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.output_layer = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # print(\"Input src shape:\", src.shape)\n",
    "        src = self.input_embed(src)\n",
    "        # print(\"After embedding shape:\", src.shape)\n",
    "        src += self.pos_encoder\n",
    "        # print(\"After pos_encoder shape:\", src.shape)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.mean(dim=1)\n",
    "        return self.output_layer(output)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerModel(input_features=12, model_dim=512, num_heads=8, num_layers=3, seq_length=500, output_dim=75).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss here 0.28956422209739685\n",
      "Loss here 1.5389354228973389\n",
      "Loss here 0.39009106159210205\n",
      "Loss here 0.11593693494796753\n",
      "Loss here 0.10428684204816818\n",
      "Loss here 0.10883431136608124\n",
      "Loss here 0.0948408916592598\n",
      "Loss here 0.068019337952137\n",
      "Loss here 0.04966546222567558\n",
      "Loss here 0.04819556325674057\n",
      "Loss here 0.049008022993803024\n",
      "Loss here 0.04860890284180641\n",
      "Loss here 0.04715702682733536\n",
      "Loss here 0.04570722207427025\n",
      "Loss here 0.041860587894916534\n",
      "Loss here 0.04470888525247574\n",
      "Loss here 0.04237836226820946\n",
      "Loss here 0.04200417920947075\n",
      "Loss here 0.04181491583585739\n",
      "Loss here 0.04511535167694092\n",
      "Loss here 0.04606694355607033\n",
      "Loss here 0.03679347038269043\n",
      "Loss here 0.041614074259996414\n",
      "Loss here 0.0377531535923481\n",
      "Loss here 0.036211226135492325\n",
      "Loss here 0.03705573454499245\n",
      "Loss here 0.03999919816851616\n",
      "Loss here 0.03978952765464783\n",
      "Loss here 0.03763972222805023\n",
      "Loss here 0.03996758908033371\n",
      "Loss here 0.03807364031672478\n",
      "Loss here 0.03516411408782005\n",
      "Loss here 0.03427175059914589\n",
      "Loss here 0.03546901047229767\n",
      "Loss here 0.036187198013067245\n",
      "Loss here 0.031517885625362396\n",
      "Loss here 0.035846907645463943\n",
      "Loss here 0.041500274091959\n",
      "Loss here 0.03743123263120651\n",
      "Loss here 0.03584668040275574\n",
      "Loss here 0.03463302552700043\n",
      "Loss here 0.037171319127082825\n",
      "Loss here 0.03281985595822334\n",
      "Loss here 0.03768888860940933\n",
      "Loss here 0.03509610518813133\n",
      "Loss here 0.03651908412575722\n",
      "Loss here 0.035479310899972916\n",
      "Loss here 0.033292610198259354\n",
      "Loss here 0.035550400614738464\n",
      "Loss here 0.03376298397779465\n",
      "Loss here 0.03374238312244415\n",
      "Loss here 0.03499860689043999\n",
      "Loss here 0.034641511738300323\n",
      "Loss here 0.032299526035785675\n",
      "Loss here 0.03785170987248421\n",
      "Loss here 0.037141695618629456\n",
      "Loss here 0.03382619470357895\n",
      "Loss here 0.03643553704023361\n",
      "Loss here 0.03314707800745964\n",
      "Loss here 0.03878892585635185\n",
      "Loss here 0.03883499652147293\n",
      "Loss here 0.036991242319345474\n",
      "Loss here 0.03688430413603783\n",
      "Loss here 0.035938531160354614\n",
      "Loss here 0.037266433238983154\n",
      "Loss here 0.03269678354263306\n",
      "Loss here 0.03556807339191437\n",
      "Loss here 0.03660406172275543\n",
      "Loss here 0.0369512103497982\n",
      "Loss here 0.034869756549596786\n",
      "Loss here 0.034726351499557495\n",
      "Loss here 0.034410830587148666\n",
      "Loss here 0.031836334615945816\n",
      "Loss here 0.032283883541822433\n",
      "Loss here 0.0337335541844368\n",
      "Loss here 0.03934699296951294\n",
      "Loss here 0.037289880216121674\n",
      "Loss here 0.03453453257679939\n",
      "Loss here 0.034490566700696945\n",
      "Loss here 0.03783812001347542\n",
      "Loss here 0.03665461763739586\n",
      "Loss here 0.03581954911351204\n",
      "Loss here 0.037735238671302795\n",
      "Loss here 0.03636143356561661\n",
      "Loss here 0.03777679055929184\n",
      "Loss here 0.034878894686698914\n",
      "Loss here 0.03342754766345024\n",
      "Loss here 0.03908379375934601\n",
      "Loss here 0.032795388251543045\n",
      "Loss here 0.03589243441820145\n",
      "Loss here 0.035743359476327896\n",
      "Loss here 0.03430509939789772\n",
      "Loss here 0.0339706689119339\n",
      "Loss here 0.03994986042380333\n",
      "Loss here 0.03613293543457985\n",
      "Loss here 0.035296011716127396\n",
      "Loss here 0.032691847532987595\n",
      "Loss here 0.03676249831914902\n",
      "Loss here 0.03366051986813545\n",
      "Loss here 0.03219291567802429\n",
      "Loss here 0.036042265594005585\n",
      "Loss here 0.03699984401464462\n",
      "Loss here 0.036807503551244736\n",
      "Loss here 0.03319162502884865\n",
      "Loss here 0.03556304797530174\n",
      "Loss here 0.034459035843610764\n",
      "Loss here 0.03684558346867561\n",
      "Loss here 0.03559691458940506\n",
      "Loss here 0.0357656255364418\n",
      "Loss here 0.03520070016384125\n",
      "Loss here 0.035655636340379715\n",
      "Loss here 0.03456205874681473\n",
      "Loss here 0.03323770686984062\n",
      "Loss here 0.030789297074079514\n",
      "Loss here 0.038319218903779984\n",
      "Loss here 0.035853151232004166\n",
      "Loss here 0.031542543321847916\n",
      "Loss here 0.03643249347805977\n",
      "Loss here 0.03421548753976822\n",
      "Loss here 0.04014834016561508\n",
      "Loss here 0.03284820169210434\n",
      "Loss here 0.03305251896381378\n",
      "Loss here 0.034212734550237656\n",
      "Loss here 0.03188781812787056\n",
      "Loss here 0.03341598063707352\n",
      "Loss here 0.028526004403829575\n",
      "Loss here 0.031374797224998474\n",
      "Loss here 0.03341710940003395\n",
      "Loss here 0.033251356333494186\n",
      "Loss here 0.029466383159160614\n",
      "Loss here 0.033256493508815765\n",
      "Loss here 0.029432158917188644\n",
      "Loss here 0.026985833421349525\n",
      "Loss here 0.029211437329649925\n",
      "Loss here 0.028713440522551537\n",
      "Loss here 0.02444448322057724\n",
      "Loss here 0.029796546325087547\n",
      "Loss here 0.026161663234233856\n",
      "Loss here 0.028387099504470825\n",
      "Loss here 0.028526857495307922\n",
      "Loss here 0.027875229716300964\n",
      "Loss here 0.026706192642450333\n",
      "Loss here 0.02509586326777935\n",
      "Loss here 0.02475753054022789\n",
      "Loss here 0.023792903870344162\n",
      "Loss here 0.022077729925513268\n",
      "Loss here 0.02241312712430954\n",
      "Loss here 0.022552775219082832\n",
      "Loss here 0.02279810979962349\n",
      "Loss here 0.018405316397547722\n",
      "Loss here 0.021079977974295616\n",
      "Loss here 0.020552534610033035\n",
      "Loss here 0.019197139889001846\n",
      "Loss here 0.023184603080153465\n",
      "Loss here 0.01915217936038971\n",
      "Loss here 0.018982229754328728\n",
      "Loss here 0.019341610372066498\n",
      "Loss here 0.01904490776360035\n",
      "Loss here 0.017474176362156868\n",
      "Loss here 0.022733336314558983\n",
      "Epoch 1, Loss: 0.0493\n",
      "Loss here 0.018666043877601624\n",
      "Loss here 0.020894547924399376\n",
      "Loss here 0.021551046520471573\n",
      "Loss here 0.016756411641836166\n",
      "Loss here 0.01768222264945507\n",
      "Loss here 0.01783340610563755\n",
      "Loss here 0.018587643280625343\n",
      "Loss here 0.020104968920350075\n",
      "Loss here 0.01773606427013874\n",
      "Loss here 0.016332263126969337\n",
      "Loss here 0.021434687077999115\n",
      "Loss here 0.020375359803438187\n",
      "Loss here 0.018892301246523857\n",
      "Loss here 0.021032560616731644\n",
      "Loss here 0.019694635644555092\n",
      "Loss here 0.017928777262568474\n",
      "Loss here 0.019681889563798904\n",
      "Loss here 0.019841400906443596\n",
      "Loss here 0.01635255292057991\n",
      "Loss here 0.019682368263602257\n",
      "Loss here 0.02012290060520172\n",
      "Loss here 0.017729712650179863\n",
      "Loss here 0.020671388134360313\n",
      "Loss here 0.021049879491329193\n",
      "Loss here 0.019995735958218575\n",
      "Loss here 0.022418029606342316\n",
      "Loss here 0.01839589886367321\n",
      "Loss here 0.019256005063652992\n",
      "Loss here 0.017574617639183998\n",
      "Loss here 0.020606493577361107\n",
      "Loss here 0.01867264322936535\n",
      "Loss here 0.017836475744843483\n",
      "Loss here 0.01889128051698208\n",
      "Loss here 0.017844196408987045\n",
      "Loss here 0.019776061177253723\n",
      "Loss here 0.019959580153226852\n",
      "Loss here 0.015540020540356636\n",
      "Loss here 0.01829068921506405\n",
      "Loss here 0.014704651199281216\n",
      "Loss here 0.017088649794459343\n",
      "Loss here 0.017294179648160934\n",
      "Loss here 0.017966249957680702\n",
      "Loss here 0.019917745143175125\n",
      "Loss here 0.018582837656140327\n",
      "Loss here 0.017692934721708298\n",
      "Loss here 0.01632385514676571\n",
      "Loss here 0.01937013491988182\n",
      "Loss here 0.016622085124254227\n",
      "Loss here 0.019004497677087784\n",
      "Loss here 0.016114691272377968\n",
      "Loss here 0.01619713380932808\n",
      "Loss here 0.016763288527727127\n",
      "Loss here 0.018681121990084648\n",
      "Loss here 0.015811413526535034\n",
      "Loss here 0.01828084886074066\n",
      "Loss here 0.018495887517929077\n",
      "Loss here 0.016143133863806725\n",
      "Loss here 0.020074129104614258\n",
      "Loss here 0.018363898620009422\n",
      "Loss here 0.02040797472000122\n",
      "Loss here 0.017121318727731705\n",
      "Loss here 0.02208901010453701\n",
      "Loss here 0.016771366819739342\n",
      "Loss here 0.01766069419682026\n",
      "Loss here 0.01954599656164646\n",
      "Loss here 0.018233647570014\n",
      "Loss here 0.019326360896229744\n",
      "Loss here 0.01812516339123249\n",
      "Loss here 0.016529304906725883\n",
      "Loss here 0.01728370599448681\n",
      "Loss here 0.020501114428043365\n",
      "Loss here 0.01790558360517025\n",
      "Loss here 0.017611512914299965\n",
      "Loss here 0.016518371179699898\n",
      "Loss here 0.018237166106700897\n",
      "Loss here 0.01527232676744461\n",
      "Loss here 0.01821751706302166\n",
      "Loss here 0.014679349027574062\n",
      "Loss here 0.014583333395421505\n",
      "Loss here 0.015341698192059994\n",
      "Loss here 0.01628774031996727\n",
      "Loss here 0.014964250847697258\n",
      "Loss here 0.016826000064611435\n",
      "Loss here 0.016111532226204872\n",
      "Loss here 0.015274381265044212\n",
      "Loss here 0.01677241176366806\n",
      "Loss here 0.015781261026859283\n",
      "Loss here 0.014466723427176476\n",
      "Loss here 0.014901483431458473\n",
      "Loss here 0.014442065730690956\n",
      "Loss here 0.01681927777826786\n",
      "Loss here 0.01697479747235775\n",
      "Loss here 0.017281850799918175\n",
      "Loss here 0.014630951918661594\n",
      "Loss here 0.01681942492723465\n",
      "Loss here 0.015789998695254326\n",
      "Loss here 0.0153639055788517\n",
      "Loss here 0.016842596232891083\n",
      "Loss here 0.015271637588739395\n",
      "Loss here 0.01463882252573967\n",
      "Loss here 0.017180852591991425\n",
      "Loss here 0.0174851194024086\n",
      "Loss here 0.015344914048910141\n",
      "Loss here 0.015434200875461102\n",
      "Loss here 0.01558521669358015\n",
      "Loss here 0.014466977678239346\n",
      "Loss here 0.014218809083104134\n",
      "Loss here 0.015862014144659042\n",
      "Loss here 0.016682062298059464\n",
      "Loss here 0.015484643168747425\n",
      "Loss here 0.01403468381613493\n",
      "Loss here 0.01591305248439312\n",
      "Loss here 0.015005159191787243\n",
      "Loss here 0.01778532937169075\n",
      "Loss here 0.015287362970411777\n",
      "Loss here 0.017305824905633926\n",
      "Loss here 0.016505042091012\n",
      "Loss here 0.014093347825109959\n",
      "Loss here 0.016138119623064995\n",
      "Loss here 0.014277233742177486\n",
      "Loss here 0.01573868840932846\n",
      "Loss here 0.014796224422752857\n",
      "Loss here 0.01569477468729019\n",
      "Loss here 0.015206501819193363\n",
      "Loss here 0.014296647161245346\n",
      "Loss here 0.01527426391839981\n",
      "Loss here 0.014533876441419125\n",
      "Loss here 0.014806634746491909\n",
      "Loss here 0.015363778918981552\n",
      "Loss here 0.01386090088635683\n",
      "Loss here 0.015062052756547928\n",
      "Loss here 0.014570467174053192\n",
      "Loss here 0.014797584153711796\n",
      "Loss here 0.015122360549867153\n",
      "Loss here 0.013267094269394875\n",
      "Loss here 0.013943402096629143\n",
      "Loss here 0.014247269369661808\n",
      "Loss here 0.013279122300446033\n",
      "Loss here 0.015209568664431572\n",
      "Loss here 0.01730463281273842\n",
      "Loss here 0.01656850427389145\n",
      "Loss here 0.014977354556322098\n",
      "Loss here 0.013726725243031979\n",
      "Loss here 0.014789247885346413\n",
      "Loss here 0.015618421137332916\n",
      "Loss here 0.016267888247966766\n",
      "Loss here 0.01557300053536892\n",
      "Loss here 0.016033878549933434\n",
      "Loss here 0.016738830134272575\n",
      "Loss here 0.01399119570851326\n",
      "Loss here 0.015506125055253506\n",
      "Loss here 0.01684381440281868\n",
      "Loss here 0.01721702329814434\n",
      "Loss here 0.015461678616702557\n",
      "Loss here 0.016514582559466362\n",
      "Loss here 0.014593102969229221\n",
      "Loss here 0.014405282214283943\n",
      "Loss here 0.013118074275553226\n",
      "Loss here 0.013904603198170662\n",
      "Loss here 0.012702235952019691\n",
      "Epoch 2, Loss: 0.0169\n",
      "Loss here 0.014576639980077744\n",
      "Loss here 0.015821220353245735\n",
      "Loss here 0.016112666577100754\n",
      "Loss here 0.015492213889956474\n",
      "Loss here 0.015632666647434235\n",
      "Loss here 0.014792411588132381\n",
      "Loss here 0.015804709866642952\n",
      "Loss here 0.015002924017608166\n",
      "Loss here 0.015170386992394924\n",
      "Loss here 0.015849584713578224\n",
      "Loss here 0.016398489475250244\n",
      "Loss here 0.01438018400222063\n",
      "Loss here 0.013547958806157112\n",
      "Loss here 0.01688806340098381\n",
      "Loss here 0.01454341970384121\n",
      "Loss here 0.014393999241292477\n",
      "Loss here 0.013629399240016937\n",
      "Loss here 0.01905631832778454\n",
      "Loss here 0.015437183901667595\n",
      "Loss here 0.016861164942383766\n",
      "Loss here 0.015506736002862453\n",
      "Loss here 0.017438672482967377\n",
      "Loss here 0.013845911249518394\n",
      "Loss here 0.01712556555867195\n",
      "Loss here 0.016649356111884117\n",
      "Loss here 0.017550865188241005\n",
      "Loss here 0.016367781907320023\n",
      "Loss here 0.01463417336344719\n",
      "Loss here 0.017410142347216606\n",
      "Loss here 0.014499291777610779\n",
      "Loss here 0.014677450992166996\n",
      "Loss here 0.014015101827681065\n",
      "Loss here 0.015208151191473007\n",
      "Loss here 0.016070637851953506\n",
      "Loss here 0.014003954827785492\n",
      "Loss here 0.014540634118020535\n",
      "Loss here 0.015657896175980568\n",
      "Loss here 0.014297617599368095\n",
      "Loss here 0.013571355491876602\n",
      "Loss here 0.013326364569365978\n",
      "Loss here 0.012630595825612545\n",
      "Loss here 0.014444684609770775\n",
      "Loss here 0.012942991219460964\n",
      "Loss here 0.014036079868674278\n",
      "Loss here 0.012533008120954037\n",
      "Loss here 0.014132106676697731\n",
      "Loss here 0.014311748556792736\n",
      "Loss here 0.013301440514624119\n",
      "Loss here 0.013631323352456093\n",
      "Loss here 0.012483691796660423\n",
      "Loss here 0.013533532619476318\n",
      "Loss here 0.011677541770040989\n",
      "Loss here 0.014776465483009815\n",
      "Loss here 0.013379513286054134\n",
      "Loss here 0.014687410555779934\n",
      "Loss here 0.013443403877317905\n",
      "Loss here 0.013172527775168419\n",
      "Loss here 0.013955323025584221\n",
      "Loss here 0.012586879543960094\n",
      "Loss here 0.012182401493191719\n",
      "Loss here 0.01407990325242281\n",
      "Loss here 0.013087252154946327\n",
      "Loss here 0.011963200755417347\n",
      "Loss here 0.013296181336045265\n",
      "Loss here 0.012140446342527866\n",
      "Loss here 0.012882871553301811\n",
      "Loss here 0.01351956371217966\n",
      "Loss here 0.013173245824873447\n",
      "Loss here 0.01368925254791975\n",
      "Loss here 0.015037009492516518\n",
      "Loss here 0.013542788103222847\n",
      "Loss here 0.01256052777171135\n",
      "Loss here 0.015123887918889523\n",
      "Loss here 0.013961242511868477\n",
      "Loss here 0.014388678595423698\n",
      "Loss here 0.013451224192976952\n",
      "Loss here 0.0134503822773695\n",
      "Loss here 0.012699226848781109\n",
      "Loss here 0.013526243157684803\n",
      "Loss here 0.01337687112390995\n",
      "Loss here 0.012479810975492\n",
      "Loss here 0.01345072966068983\n",
      "Loss here 0.012491135857999325\n",
      "Loss here 0.012688405811786652\n",
      "Loss here 0.013201244175434113\n",
      "Loss here 0.01337437890470028\n",
      "Loss here 0.013017991557717323\n",
      "Loss here 0.01374777127057314\n",
      "Loss here 0.01332100760191679\n",
      "Loss here 0.014026233926415443\n",
      "Loss here 0.013448333367705345\n",
      "Loss here 0.013100752606987953\n",
      "Loss here 0.014002538286149502\n",
      "Loss here 0.012617325410246849\n",
      "Loss here 0.013113356195390224\n",
      "Loss here 0.011383771896362305\n",
      "Loss here 0.01216513104736805\n",
      "Loss here 0.011646119877696037\n",
      "Loss here 0.011625214479863644\n",
      "Loss here 0.01154374610632658\n",
      "Loss here 0.01236013788729906\n",
      "Loss here 0.012226263992488384\n",
      "Loss here 0.010736199095845222\n",
      "Loss here 0.012236479669809341\n",
      "Loss here 0.011483543552458286\n",
      "Loss here 0.012458662502467632\n",
      "Loss here 0.012970269657671452\n",
      "Loss here 0.011320277117192745\n",
      "Loss here 0.012376038357615471\n",
      "Loss here 0.01281063724309206\n",
      "Loss here 0.012866591103374958\n",
      "Loss here 0.012940392829477787\n",
      "Loss here 0.011439541354775429\n",
      "Loss here 0.012258186005055904\n",
      "Loss here 0.013323867693543434\n",
      "Loss here 0.011121716350317001\n",
      "Loss here 0.010772543959319592\n",
      "Loss here 0.011292532086372375\n",
      "Loss here 0.013283948414027691\n",
      "Loss here 0.01155715063214302\n",
      "Loss here 0.011540618725121021\n",
      "Loss here 0.012110497802495956\n",
      "Loss here 0.011418052017688751\n",
      "Loss here 0.01139459665864706\n",
      "Loss here 0.012079883366823196\n",
      "Loss here 0.013194176368415356\n",
      "Loss here 0.011475162580609322\n",
      "Loss here 0.009970859624445438\n",
      "Loss here 0.013938361778855324\n",
      "Loss here 0.012056340463459492\n",
      "Loss here 0.01196297537535429\n",
      "Loss here 0.012823780067265034\n",
      "Loss here 0.010910982266068459\n",
      "Loss here 0.011877521872520447\n",
      "Loss here 0.010653609409928322\n",
      "Loss here 0.011990780010819435\n",
      "Loss here 0.0110783651471138\n",
      "Loss here 0.011903324164450169\n",
      "Loss here 0.011091773398220539\n",
      "Loss here 0.012191115878522396\n",
      "Loss here 0.011023376137018204\n",
      "Loss here 0.010996061377227306\n",
      "Loss here 0.011288578622043133\n",
      "Loss here 0.012985695153474808\n",
      "Loss here 0.011240476742386818\n",
      "Loss here 0.011398928239941597\n",
      "Loss here 0.010770258493721485\n",
      "Loss here 0.010707399807870388\n",
      "Loss here 0.011370470747351646\n",
      "Loss here 0.012027368880808353\n",
      "Loss here 0.010574010200798512\n",
      "Loss here 0.013111722655594349\n",
      "Loss here 0.010322414338588715\n",
      "Loss here 0.012536128051578999\n",
      "Loss here 0.011598529294133186\n",
      "Loss here 0.011943508870899677\n",
      "Loss here 0.01206742599606514\n",
      "Loss here 0.012517913244664669\n",
      "Loss here 0.01188646350055933\n",
      "Loss here 0.009397318586707115\n",
      "Epoch 3, Loss: 0.0132\n",
      "Loss here 0.01388031430542469\n",
      "Loss here 0.012268168851733208\n",
      "Loss here 0.012292630970478058\n",
      "Loss here 0.013059197925031185\n",
      "Loss here 0.012332224287092686\n",
      "Loss here 0.010261241346597672\n",
      "Loss here 0.012425262480974197\n",
      "Loss here 0.013036346063017845\n",
      "Loss here 0.010983657091856003\n",
      "Loss here 0.0115150585770607\n",
      "Loss here 0.01234759483486414\n",
      "Loss here 0.01263625267893076\n",
      "Loss here 0.01231657899916172\n",
      "Loss here 0.012736414559185505\n",
      "Loss here 0.012301440350711346\n",
      "Loss here 0.01218405645340681\n",
      "Loss here 0.01105774287134409\n",
      "Loss here 0.011146659962832928\n",
      "Loss here 0.011809238232672215\n",
      "Loss here 0.012446221895515919\n",
      "Loss here 0.010276147164404392\n",
      "Loss here 0.01088914554566145\n",
      "Loss here 0.011051627807319164\n",
      "Loss here 0.010466030798852444\n",
      "Loss here 0.012010278180241585\n",
      "Loss here 0.011793680489063263\n",
      "Loss here 0.011677364818751812\n",
      "Loss here 0.011264132335782051\n",
      "Loss here 0.011432057246565819\n",
      "Loss here 0.011423311196267605\n",
      "Loss here 0.011085402220487595\n",
      "Loss here 0.010700929909944534\n",
      "Loss here 0.01067513506859541\n",
      "Loss here 0.010884443297982216\n",
      "Loss here 0.010799814015626907\n",
      "Loss here 0.010628302581608295\n",
      "Loss here 0.011076529510319233\n",
      "Loss here 0.010977085679769516\n",
      "Loss here 0.011212251149117947\n",
      "Loss here 0.010802896693348885\n",
      "Loss here 0.01108210626989603\n",
      "Loss here 0.009848007932305336\n",
      "Loss here 0.011091995052993298\n",
      "Loss here 0.011239279992878437\n",
      "Loss here 0.010639654472470284\n",
      "Loss here 0.01260337047278881\n",
      "Loss here 0.011011443100869656\n",
      "Loss here 0.011025669053196907\n",
      "Loss here 0.010444572195410728\n",
      "Loss here 0.012122788466513157\n",
      "Loss here 0.011069690808653831\n",
      "Loss here 0.010360203683376312\n",
      "Loss here 0.010777083225548267\n",
      "Loss here 0.010612237267196178\n",
      "Loss here 0.011251639574766159\n",
      "Loss here 0.010593275539577007\n",
      "Loss here 0.010562448762357235\n",
      "Loss here 0.011698324233293533\n",
      "Loss here 0.011166861280798912\n",
      "Loss here 0.010759624652564526\n",
      "Loss here 0.010414146818220615\n",
      "Loss here 0.011222162283957005\n",
      "Loss here 0.01123074535280466\n",
      "Loss here 0.010590460151433945\n",
      "Loss here 0.009967871010303497\n",
      "Loss here 0.010513911955058575\n",
      "Loss here 0.01138010248541832\n",
      "Loss here 0.0102493641898036\n",
      "Loss here 0.011121498420834541\n",
      "Loss here 0.011019189842045307\n",
      "Loss here 0.011940532363951206\n",
      "Loss here 0.010828342288732529\n",
      "Loss here 0.01041068322956562\n",
      "Loss here 0.009498974308371544\n",
      "Loss here 0.010627679526805878\n",
      "Loss here 0.010319662280380726\n",
      "Loss here 0.011250610463321209\n",
      "Loss here 0.010363454930484295\n",
      "Loss here 0.011114194989204407\n",
      "Loss here 0.00950914341956377\n",
      "Loss here 0.011294987052679062\n",
      "Loss here 0.010934073477983475\n",
      "Loss here 0.009062713012099266\n",
      "Loss here 0.010314024053514004\n",
      "Loss here 0.009386006742715836\n",
      "Loss here 0.008797948248684406\n",
      "Loss here 0.009244429878890514\n",
      "Loss here 0.010560990311205387\n",
      "Loss here 0.009415138512849808\n",
      "Loss here 0.011650605127215385\n",
      "Loss here 0.010463572107255459\n",
      "Loss here 0.009927560575306416\n",
      "Loss here 0.010136745870113373\n",
      "Loss here 0.010254984721541405\n",
      "Loss here 0.009774408303201199\n",
      "Loss here 0.010319146327674389\n",
      "Loss here 0.009642377495765686\n",
      "Loss here 0.009983216412365437\n",
      "Loss here 0.009988559409976006\n",
      "Loss here 0.010634171776473522\n",
      "Loss here 0.01082833856344223\n",
      "Loss here 0.010672115720808506\n",
      "Loss here 0.00940200500190258\n",
      "Loss here 0.010355139151215553\n",
      "Loss here 0.010671773925423622\n",
      "Loss here 0.010677987709641457\n",
      "Loss here 0.010719702579081059\n",
      "Loss here 0.010001500137150288\n",
      "Loss here 0.011385769583284855\n",
      "Loss here 0.010193723253905773\n",
      "Loss here 0.01061597652733326\n",
      "Loss here 0.01028160285204649\n",
      "Loss here 0.01056146714836359\n",
      "Loss here 0.010286632925271988\n",
      "Loss here 0.011153086088597775\n",
      "Loss here 0.010097824037075043\n",
      "Loss here 0.010462911799550056\n",
      "Loss here 0.010266098193824291\n",
      "Loss here 0.009286049753427505\n",
      "Loss here 0.01148184109479189\n",
      "Loss here 0.010539175011217594\n",
      "Loss here 0.009985418058931828\n",
      "Loss here 0.010201147757470608\n",
      "Loss here 0.01151721365749836\n",
      "Loss here 0.010310731828212738\n",
      "Loss here 0.009525042958557606\n",
      "Loss here 0.010064760223031044\n",
      "Loss here 0.010174430906772614\n",
      "Loss here 0.009988170117139816\n",
      "Loss here 0.009870383888483047\n",
      "Loss here 0.010357961058616638\n",
      "Loss here 0.009379143826663494\n",
      "Loss here 0.010600865818560123\n",
      "Loss here 0.009893749840557575\n",
      "Loss here 0.009944647550582886\n",
      "Loss here 0.010155009105801582\n",
      "Loss here 0.010625780560076237\n",
      "Loss here 0.00925410632044077\n",
      "Loss here 0.008409709669649601\n",
      "Loss here 0.010003887116909027\n",
      "Loss here 0.00986784789711237\n",
      "Loss here 0.009496847167611122\n",
      "Loss here 0.010557939298450947\n",
      "Loss here 0.01003696583211422\n",
      "Loss here 0.009519696235656738\n",
      "Loss here 0.010412986390292645\n",
      "Loss here 0.009780107997357845\n",
      "Loss here 0.010525320656597614\n",
      "Loss here 0.010012773796916008\n",
      "Loss here 0.010026038624346256\n",
      "Loss here 0.008798188529908657\n",
      "Loss here 0.011104482226073742\n",
      "Loss here 0.00967387855052948\n",
      "Loss here 0.009195568040013313\n",
      "Loss here 0.011109651997685432\n",
      "Loss here 0.009233074262738228\n",
      "Loss here 0.010777194052934647\n",
      "Loss here 0.009516402147710323\n",
      "Loss here 0.010075443424284458\n",
      "Loss here 0.009994733147323132\n",
      "Epoch 4, Loss: 0.0107\n",
      "Loss here 0.010649735108017921\n",
      "Loss here 0.010465540923178196\n",
      "Loss here 0.010014322586357594\n",
      "Loss here 0.011885780841112137\n",
      "Loss here 0.011839637532830238\n",
      "Loss here 0.011619791388511658\n",
      "Loss here 0.010945543646812439\n",
      "Loss here 0.011222568340599537\n",
      "Loss here 0.011178793385624886\n",
      "Loss here 0.010838271118700504\n",
      "Loss here 0.010194605216383934\n",
      "Loss here 0.011387246660888195\n",
      "Loss here 0.01136156264692545\n",
      "Loss here 0.011388219892978668\n",
      "Loss here 0.009994524531066418\n",
      "Loss here 0.010231810621917248\n",
      "Loss here 0.01136273704469204\n",
      "Loss here 0.009340300224721432\n",
      "Loss here 0.011710410937666893\n",
      "Loss here 0.009792236611247063\n",
      "Loss here 0.010470052249729633\n",
      "Loss here 0.009657601825892925\n",
      "Loss here 0.010507646016776562\n",
      "Loss here 0.009873127564787865\n",
      "Loss here 0.009253636933863163\n",
      "Loss here 0.010649553500115871\n",
      "Loss here 0.010117870755493641\n",
      "Loss here 0.010005759075284004\n",
      "Loss here 0.010372265242040157\n",
      "Loss here 0.010656729340553284\n",
      "Loss here 0.009766187518835068\n",
      "Loss here 0.010194542817771435\n",
      "Loss here 0.010171975009143353\n",
      "Loss here 0.00930358748883009\n",
      "Loss here 0.009676399640738964\n",
      "Loss here 0.010105494409799576\n",
      "Loss here 0.009964065626263618\n",
      "Loss here 0.0097085265442729\n",
      "Loss here 0.010197710245847702\n",
      "Loss here 0.009977844543755054\n",
      "Loss here 0.00950058177113533\n",
      "Loss here 0.010198717936873436\n",
      "Loss here 0.01016645785421133\n",
      "Loss here 0.009502225555479527\n",
      "Loss here 0.010237039066851139\n",
      "Loss here 0.009321609511971474\n",
      "Loss here 0.010795500129461288\n",
      "Loss here 0.009664995595812798\n",
      "Loss here 0.009534837678074837\n",
      "Loss here 0.009348771534860134\n",
      "Loss here 0.009665547870099545\n",
      "Loss here 0.011128445155918598\n",
      "Loss here 0.010026603005826473\n",
      "Loss here 0.009762967005372047\n",
      "Loss here 0.009907692670822144\n",
      "Loss here 0.00889707449823618\n",
      "Loss here 0.010263405740261078\n",
      "Loss here 0.009933285415172577\n",
      "Loss here 0.008933985605835915\n",
      "Loss here 0.008805986493825912\n",
      "Loss here 0.009318048134446144\n",
      "Loss here 0.009750974364578724\n",
      "Loss here 0.009299342520534992\n",
      "Loss here 0.008718019351363182\n",
      "Loss here 0.009715551510453224\n",
      "Loss here 0.010013560764491558\n",
      "Loss here 0.010588238947093487\n",
      "Loss here 0.009231554344296455\n",
      "Loss here 0.008789976127445698\n",
      "Loss here 0.009238270111382008\n",
      "Loss here 0.008875885047018528\n",
      "Loss here 0.009393767453730106\n",
      "Loss here 0.009093724191188812\n",
      "Loss here 0.009416787885129452\n",
      "Loss here 0.010203707031905651\n",
      "Loss here 0.00984787568449974\n",
      "Loss here 0.009587697684764862\n",
      "Loss here 0.010458202101290226\n",
      "Loss here 0.010866251774132252\n",
      "Loss here 0.010027351789176464\n",
      "Loss here 0.009272084571421146\n",
      "Loss here 0.010100239887833595\n",
      "Loss here 0.009922977536916733\n",
      "Loss here 0.010007994249463081\n",
      "Loss here 0.008736738003790379\n",
      "Loss here 0.01057770848274231\n",
      "Loss here 0.009890723042190075\n",
      "Loss here 0.009072426706552505\n",
      "Loss here 0.010375048033893108\n",
      "Loss here 0.010423804633319378\n",
      "Loss here 0.0096695926040411\n",
      "Loss here 0.009509732946753502\n",
      "Loss here 0.010230408050119877\n",
      "Loss here 0.010068613104522228\n",
      "Loss here 0.00940330233424902\n",
      "Loss here 0.009677582420408726\n",
      "Loss here 0.010083182714879513\n",
      "Loss here 0.009506204165518284\n",
      "Loss here 0.010384065099060535\n",
      "Loss here 0.009244304150342941\n",
      "Loss here 0.009612679481506348\n",
      "Loss here 0.009240323677659035\n",
      "Loss here 0.009460262954235077\n",
      "Loss here 0.009673116728663445\n",
      "Loss here 0.010119181126356125\n",
      "Loss here 0.010536269284784794\n",
      "Loss here 0.009485210292041302\n",
      "Loss here 0.00926932692527771\n",
      "Loss here 0.009484991431236267\n",
      "Loss here 0.00990906823426485\n",
      "Loss here 0.009575261734426022\n",
      "Loss here 0.009583311155438423\n",
      "Loss here 0.010510461404919624\n",
      "Loss here 0.010931749828159809\n",
      "Loss here 0.01009922381490469\n",
      "Loss here 0.008402513340115547\n",
      "Loss here 0.010696165263652802\n",
      "Loss here 0.008768806234002113\n",
      "Loss here 0.01030797977000475\n",
      "Loss here 0.010448213666677475\n",
      "Loss here 0.009654569439589977\n",
      "Loss here 0.009911949746310711\n",
      "Loss here 0.009976278059184551\n",
      "Loss here 0.010132363066077232\n",
      "Loss here 0.009120311588048935\n",
      "Loss here 0.009326901286840439\n",
      "Loss here 0.009333829395473003\n",
      "Loss here 0.010540264658629894\n",
      "Loss here 0.009451955556869507\n",
      "Loss here 0.009388785809278488\n",
      "Loss here 0.009781542234122753\n",
      "Loss here 0.00928199477493763\n",
      "Loss here 0.010075142607092857\n",
      "Loss here 0.009940741583704948\n",
      "Loss here 0.009120123460888863\n",
      "Loss here 0.009516259655356407\n",
      "Loss here 0.008799667470157146\n",
      "Loss here 0.008543789386749268\n",
      "Loss here 0.009267260320484638\n",
      "Loss here 0.009338519535958767\n",
      "Loss here 0.010107661597430706\n",
      "Loss here 0.00952714029699564\n",
      "Loss here 0.00889688078314066\n",
      "Loss here 0.00900278054177761\n",
      "Loss here 0.008881865069270134\n",
      "Loss here 0.01004808396100998\n",
      "Loss here 0.010104982182383537\n",
      "Loss here 0.008801842108368874\n",
      "Loss here 0.01000829879194498\n",
      "Loss here 0.009708646684885025\n",
      "Loss here 0.009737566113471985\n",
      "Loss here 0.009160071611404419\n",
      "Loss here 0.009420428425073624\n",
      "Loss here 0.008516117930412292\n",
      "Loss here 0.009255570359528065\n",
      "Loss here 0.009313694201409817\n",
      "Loss here 0.009340642020106316\n",
      "Loss here 0.008887018077075481\n",
      "Loss here 0.008911240845918655\n",
      "Loss here 0.006797123234719038\n",
      "Epoch 5, Loss: 0.0098\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in dataloader:\n",
    "        data = data.transpose(1, 2).to(device).float()\n",
    "        target = target.view(-1, 75).to(device).float()\n",
    "        # print(f'target shape: {target.shape}')\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(f' output shape: {output.shape}')\n",
    "        loss = criterion(output, target)\n",
    "        print(f'Loss here {loss}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    dataloder = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss = train_epoch(model, dataloader, optimizer, criterion, device)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss here 0.009548318572342396\n",
      "Loss here 0.009752094745635986\n",
      "Loss here 0.00933888927102089\n",
      "Loss here 0.010239235125482082\n",
      "Loss here 0.009764899499714375\n",
      "Loss here 0.008469054475426674\n",
      "Loss here 0.00886339321732521\n",
      "Loss here 0.008508721366524696\n",
      "Loss here 0.00883712898939848\n",
      "Loss here 0.00906714890152216\n",
      "Loss here 0.008666840381920338\n",
      "Loss here 0.008975629694759846\n",
      "Loss here 0.009716196916997433\n",
      "Loss here 0.008483927696943283\n",
      "Loss here 0.008767160587012768\n",
      "Loss here 0.008574254810810089\n",
      "Loss here 0.010041382163763046\n",
      "Loss here 0.00938682071864605\n",
      "Loss here 0.010001893155276775\n",
      "Loss here 0.008877569809556007\n",
      "Loss here 0.00986435916274786\n",
      "Loss here 0.008681375533342361\n",
      "Loss here 0.010130626149475574\n",
      "Loss here 0.009329861029982567\n",
      "Loss here 0.008625536225736141\n",
      "Loss here 0.009306787513196468\n",
      "Loss here 0.010144539177417755\n",
      "Loss here 0.009352498687803745\n",
      "Loss here 0.00940596405416727\n",
      "Loss here 0.008699106052517891\n",
      "Loss here 0.009487706236541271\n",
      "Loss here 0.008956042118370533\n",
      "Loss here 0.008023613132536411\n",
      "Loss here 0.010310372337698936\n",
      "Loss here 0.009108628146350384\n",
      "Loss here 0.009032323025166988\n",
      "Loss here 0.009452578611671925\n",
      "Loss here 0.00911407545208931\n",
      "Loss here 0.008604935370385647\n",
      "Loss here 0.009654903784394264\n",
      "Loss here 0.009013038128614426\n",
      "Loss here 0.010050667449831963\n",
      "Loss here 0.008972925134003162\n",
      "Loss here 0.009375209920108318\n",
      "Loss here 0.009754211641848087\n",
      "Loss here 0.009733816608786583\n",
      "Loss here 0.008843299932777882\n",
      "Loss here 0.009288018569350243\n",
      "Loss here 0.009379947558045387\n",
      "Loss here 0.008538729511201382\n",
      "Loss here 0.009810632094740868\n",
      "Loss here 0.00934866163879633\n",
      "Loss here 0.00977348256856203\n",
      "Loss here 0.010421441867947578\n",
      "Loss here 0.010153399780392647\n",
      "Loss here 0.009219538420438766\n",
      "Loss here 0.009670873172581196\n",
      "Loss here 0.00861798319965601\n",
      "Loss here 0.009726231917738914\n",
      "Loss here 0.008709871210157871\n",
      "Loss here 0.009321470744907856\n",
      "Loss here 0.009704898111522198\n",
      "Loss here 0.009875200688838959\n",
      "Loss here 0.00804574228823185\n",
      "Loss here 0.00924312137067318\n",
      "Loss here 0.008783220313489437\n",
      "Loss here 0.009459549561142921\n",
      "Loss here 0.009869848378002644\n",
      "Loss here 0.00850919634103775\n",
      "Loss here 0.008782702498137951\n",
      "Loss here 0.009397659450769424\n",
      "Loss here 0.008366846479475498\n",
      "Loss here 0.009921537712216377\n",
      "Loss here 0.008668102324008942\n",
      "Loss here 0.009849599562585354\n",
      "Loss here 0.008591480553150177\n",
      "Loss here 0.008944744244217873\n",
      "Loss here 0.008638783358037472\n",
      "Loss here 0.009240123443305492\n",
      "Loss here 0.008952166885137558\n",
      "Loss here 0.00995695311576128\n",
      "Loss here 0.009205495938658714\n",
      "Loss here 0.008717795833945274\n",
      "Loss here 0.009006884880363941\n",
      "Loss here 0.00893412809818983\n",
      "Loss here 0.008898464031517506\n",
      "Loss here 0.008808160200715065\n",
      "Loss here 0.008849045261740685\n",
      "Loss here 0.010542897507548332\n",
      "Loss here 0.009921368211507797\n",
      "Loss here 0.008619343861937523\n",
      "Loss here 0.008532554842531681\n",
      "Loss here 0.00895035732537508\n",
      "Loss here 0.008670284412801266\n",
      "Loss here 0.009199969470500946\n",
      "Loss here 0.009230147115886211\n",
      "Loss here 0.008011601865291595\n",
      "Loss here 0.008364945650100708\n",
      "Loss here 0.009116096422076225\n",
      "Loss here 0.008520546369254589\n",
      "Loss here 0.00933951698243618\n",
      "Loss here 0.008655753917992115\n"
     ]
    }
   ],
   "source": [
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.transpose(1, 2).to(device).float()\n",
    "            target = target.view(-1, 75).to(device).float()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Assume validation set is part of your dataset, if not, remove validation components\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion, device) if val_flag else None\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    if val_flag:\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}' + (f', Val Loss: {val_loss:.4f}' if val_flag else ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "if val_flag:\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#----------------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#make data (train)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_loss_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_loss_np \u001b[38;5;241m=\u001b[39m train_loss_tensor\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#----------------------------\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#make data (val)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "#----------------------------\n",
    "#make data (train)\n",
    "train_loss_tensor = torch.stack(history[\"train_loss\"])\n",
    "train_loss_np = train_loss_tensor.to('cpu').detach().numpy().copy()\n",
    "#----------------------------\n",
    "#make data (val)\n",
    "val_loss_tensor = torch.stack(history[\"val_loss\"])\n",
    "val_loss_np = val_loss_tensor.to('cpu').detach().numpy().copy()\n",
    "\n",
    "#----------------------------\n",
    "#plot\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(train_loss_np, color=\"black\",label=\"Train\")\n",
    "ax.plot(val_loss_np, color=\"maroon\",label=\"Validation\")\n",
    "#plot (setting)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.set_xlabel(\"Epoch\", fontsize=30)\n",
    "ax.set_ylabel(\"MSE\", fontsize=30)\n",
    "ax.legend(fontsize=25, frameon=False)\n",
    "ax.set_ylim(0,0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "#----------------------------\n",
    "# eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    x, y = dataset.return_test_data()\n",
    "    #----------------------------\n",
    "    #float32, grad==True\n",
    "    x = dataset.change_data_setting_to_train(x)\n",
    "    y = dataset.change_data_setting_to_train(y)\n",
    "    #----------------------------\n",
    "    #change the type\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    output = model(x)\n",
    "    #----------------------------\n",
    "    #change to numpy\n",
    "    output = output.to('cpu').detach().numpy().copy().flatten()\n",
    "    y = y.to('cpu').detach().numpy().copy().flatten()\n",
    "    loss_MSE = mean_squared_error(output, y)\n",
    "    loss_R2 = r2_score(output, y)\n",
    "    #----------------------------\n",
    "    #print loss\n",
    "    print(\"MSE: \", loss_MSE)  \n",
    "    print(\"R2: \", loss_R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_test (true/pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score # \n",
    "from sklearn.metrics import mean_absolute_error #  (MAE)\n",
    "from sklearn.metrics import mean_squared_error #  (MSE)\n",
    "#----------------\n",
    "def plot_true_predict_from_y(y_predict_list:pd.DataFrame, y_true_list:pd.DataFrame,\n",
    "                            title:str, path_save=False) -> None:\n",
    "    #----------------\n",
    "    #calc score\n",
    "    r = np.corrcoef(y_true_list, y_predict_list)[0][1]\n",
    "    R2 = r2_score(y_true=y_true_list, y_pred=y_predict_list) # (R2) #https://bellcurve.jp/statistics/course/9706.html\n",
    "    MAE = mean_absolute_error(y_true=y_true_list, y_pred=y_predict_list) # (MAE)\n",
    "    RMSE = np.sqrt(mean_squared_error(y_true=y_true_list, y_pred=y_predict_list)) # (RMSE)\n",
    "    \n",
    "    #----------------\n",
    "    #fig, ax\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    #----------------\n",
    "    #plot scatter\n",
    "    #ax.scatter(x=y_predict_list, y=y_true_list, s=40, c=\"black\", marker=\"o\", zorder=10)\n",
    "    ax.plot(y_predict_list, y_true_list, c=\"black\", marker='.', linestyle=\"\", ms=3, zorder=10)\n",
    "    #----------------\n",
    "    #plot \n",
    "    x=np.linspace( min(min(y_true_list),min(y_predict_list)), max(max(y_true_list),max(y_predict_list)), 10) #list\n",
    "    y=x\n",
    "    ax.plot(x, y, color = \"black\")\n",
    "    #----------------\n",
    "    #plot text\n",
    "    plt.text(x=0.5, y=0.94, \n",
    "             s=\"$r$={0}, $R^2$={1}, $MAE$={2}, $RMSE$={3}\".format(\"{:.2f}\".format(r),\n",
    "                                                                 \"{:.2f}\".format(R2),\n",
    "                                                                \"{:.2f}\".format(MAE),\n",
    "                                                                \"{:.2f}\".format(RMSE)), \n",
    "             fontdict=dict(fontsize=25, color=\"black\"), ha='center', transform=ax.transAxes,\n",
    "             zorder=20)\n",
    "    #----------------\n",
    "    #setting\n",
    "    ax.tick_params(labelsize = 20)#\n",
    "    ax.set_xlabel(\"True\",fontsize=30)\n",
    "    ax.set_ylabel(\"Predict\",fontsize=30)\n",
    "    plt.title(\"{0}\".format(title), fontsize=30)\n",
    "    #----------------\n",
    "    #save\n",
    "    if path_save != False:\n",
    "        plt.savefig(path_save, bbox_inches='tight')\n",
    "    #----------------\n",
    "    #show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "# eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    x, y = dataset.return_test_data()\n",
    "    #----------------------------\n",
    "    #float32, grad==True\n",
    "    x = dataset.change_data_setting_to_train(x)\n",
    "    y = dataset.change_data_setting_to_train(y)\n",
    "    #----------------------------\n",
    "    #change the type\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    output = model(x)\n",
    "    #----------------------------\n",
    "    #change to numpy\n",
    "    output = output.to('cpu').detach().numpy().copy().flatten() * 185\n",
    "    y = y.to('cpu').detach().numpy().copy().flatten() * 185\n",
    "    #----------------------------\n",
    "    #plot\n",
    "    plot_true_predict_from_y(y_predict_list=output, y_true_list=y, title=\"\", path_save=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_test (sample_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_VmDatas_Task3_all(ActTime, title):\n",
    "    # plot the Activation Time array\n",
    "    plt.imshow(ActTime, cmap='jet', interpolation='nearest', aspect='auto')\n",
    "    plt.title('Activation Time')\n",
    "    cbar = plt.colorbar()\n",
    "    plt.grid(visible=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.minorticks_on()\n",
    "    # not xticks\n",
    "    #plt.xticks([])\n",
    "    plt.grid(visible=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "    plt.title(\"{0}\".format(title), fontsize=20)\n",
    "    plt.xlabel(\"Test data ID\",fontsize=15)\n",
    "    plt.ylabel(\"Number of Activation Map\",fontsize=15)\n",
    "    cbar.set_label('Activation Time', fontsize=15, rotation=270, labelpad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var\n",
    "n_plot = 3\n",
    "\n",
    "#----------------------------\n",
    "# eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    x, y = dataset.return_test_data()\n",
    "    #----------------------------\n",
    "    #float32, grad==True\n",
    "    x = dataset.change_data_setting_to_train(x)\n",
    "    y = dataset.change_data_setting_to_train(y)\n",
    "    #----------------------------\n",
    "    #change the type\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    output = model(x)\n",
    "    #----------------------------\n",
    "    #change to numpy\n",
    "    output = output.to('cpu').view(75, dataset.return_n_test()).detach().numpy().copy() * 185\n",
    "    y = y.to('cpu').view(75, dataset.return_n_test()).detach().numpy().copy() * 185\n",
    "    #----------------------------\n",
    "    #plot\n",
    "    plot_VmDatas_Task3_all(y, title=\"True\")\n",
    "    plot_VmDatas_Task3_all(output, title=\"Predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_test (sample_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_VmDatas_Task3_each(ActTime, title):\n",
    "    # plot the Activation Time array\n",
    "    plt.imshow(ActTime, cmap='jet', interpolation='nearest', aspect='auto')\n",
    "    plt.title('Activation Time')\n",
    "    cbar = plt.colorbar()\n",
    "    plt.grid(visible=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.minorticks_on()\n",
    "    # not xticks\n",
    "    plt.xticks([])\n",
    "    plt.grid(visible=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "    plt.title(\"{0}\".format(title), fontsize=20)\n",
    "    #plt.xlabel(\"Data ID\",fontsize=15)\n",
    "    plt.ylabel(\"Number of Activation Map\",fontsize=15)\n",
    "    cbar.set_label('Activation Time', fontsize=15, rotation=270, labelpad=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var\n",
    "n_plot = 3\n",
    "\n",
    "#----------------------------\n",
    "# eval\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    x, y = dataset.return_test_data()\n",
    "    #----------------------------\n",
    "    #float32, grad==True\n",
    "    x = dataset.change_data_setting_to_train(x)\n",
    "    y = dataset.change_data_setting_to_train(y)\n",
    "    #----------------------------\n",
    "    #change the type\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    output = model(x)\n",
    "    #----------------------------\n",
    "    #change to numpy\n",
    "    output = output.to('cpu').detach().numpy().copy() * 185\n",
    "    y = y.to('cpu').detach().numpy().copy() * 185\n",
    "    print(y.shape)\n",
    "    #----------------------------\n",
    "    #plot\n",
    "    for cnt, (data_true, data_pred) in enumerate(zip(y, output)):\n",
    "        if cnt+1 <= n_plot:\n",
    "            plot_VmDatas_Task3_each(data_true, title=\"True\")\n",
    "            plot_VmDatas_Task3_each(data_pred, title=\"Predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE\n",
    "#save model\n",
    "#torch.save(model.state_dict(), '../models/model_task3.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (CustomDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union\n",
    "\n",
    "#---------------------------------------------\n",
    "# custom dataset\n",
    "#https://discuss.pytorch.org/t/custom-data-loader-for-big-data/129361\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dir_X:str, path_dir_Y:str, n_test:Union[int,float], n_val:Union[int,float], batch_size:int): # n_test -> float:ratio of test, int:number of test\n",
    "        #-----------------\n",
    "        # batch_size\n",
    "        self.batch_size = batch_size\n",
    "        # path_dir_X, path_dir_Y\n",
    "        self.path_dir_X = path_dir_X\n",
    "        self.path_dir_Y = path_dir_Y\n",
    "        # list_file_name_all\n",
    "        self.list_file_name_all = os.listdir(path_dir_X)\n",
    "        # n_data_all\n",
    "        self.n_data_all = len(self.list_file_name_all)\n",
    "        #check\n",
    "        if len(os.listdir(path_dir_X)) != len(os.listdir(path_dir_Y)):\n",
    "            raise ValueError(\"error!!!\")\n",
    "        if len(set(os.listdir(path_dir_X)) - set(os.listdir(path_dir_Y))) != 0:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # suffle\n",
    "        random.shuffle(self.list_file_name_all)\n",
    "        #-----------------\n",
    "        # n_test\n",
    "        if type(n_test)==int:\n",
    "            self.n_test = n_test\n",
    "        elif type(n_test)==float:\n",
    "            self.n_test = int(len(self.list_file_name_all)*n_test)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        # n_val\n",
    "        if type(n_val)==int:\n",
    "            self.n_val = n_val\n",
    "        elif type(n_val)==float:\n",
    "            self.n_val = int(len(self.list_file_name_all)*n_val)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #check\n",
    "        if self.n_data_all <= self.n_test+self.n_val:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # list_file_name_test / _val / _train\n",
    "        self.list_file_name_test = self.list_file_name_all[:self.n_test]\n",
    "        self.list_file_name_val = self.list_file_name_all[self.n_test:self.n_test+self.n_val]\n",
    "        self.list_file_name_train = self.list_file_name_all[self.n_test+self.n_val:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_file_name_train)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        return self.getdata(list_file_name=self.list_file_name_train, index=x)\n",
    "    \n",
    "    def getdata(self, list_file_name, index):\n",
    "        #file_name\n",
    "        file_name = list_file_name[index]\n",
    "        #data_X\n",
    "        path_file_X = \"{0}/{1}\".format(self.path_dir_X, file_name)\n",
    "        data_X = np.load(path_file_X, allow_pickle=True)\n",
    "        data_X = torch.from_numpy(data_X).to(torch.float32)\n",
    "        #data_Y\n",
    "        path_file_Y = \"{0}/{1}\".format(self.path_dir_Y, file_name)\n",
    "        data_Y = np.load(path_file_Y, allow_pickle=True)\n",
    "        data_Y = torch.from_numpy(data_Y).to(torch.float32)\n",
    "        #return\n",
    "        return data_X, data_Y\n",
    "    \n",
    "    def return_n_data_all(self):\n",
    "        return self.n_data_all\n",
    "    \n",
    "    def return_n_test(self):\n",
    "        return self.n_test\n",
    "    \n",
    "    def return_n_val(self):\n",
    "        return self.n_val\n",
    "    \n",
    "    def return_n_train(self):\n",
    "        return self.n_data_all - self.n_val - self.n_test\n",
    "    \n",
    "    def return_batch_size(self):\n",
    "        return self.batch_size\n",
    "    \n",
    "    def return_shape_X(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[0]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_shape_Y(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[1]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_test_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_test = torch.stack([self.getdata(self.list_file_name_test, i)[0] for i in range(self.n_test)])\n",
    "        data_Y_test = torch.stack([self.getdata(self.list_file_name_test, i)[1] for i in range(self.n_test)])\n",
    "        return data_X_test, data_Y_test\n",
    "    \n",
    "    def return_val_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_val = torch.stack([self.getdata(self.list_file_name_val, i)[0] for i in range(self.n_val)])\n",
    "        data_Y_val = torch.stack([self.getdata(self.list_file_name_val, i)[1] for i in range(self.n_val)])\n",
    "        return data_X_val, data_Y_val\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "#var\n",
    "path_dir_X = \"../data_X\"\n",
    "path_dir_Y = \"../data_Y_Task3\"\n",
    "n_test = 100\n",
    "n_val = 100\n",
    "batch_size = 10000\n",
    "\n",
    "#---------------------------------------------\n",
    "#instance\n",
    "dataset = CustomDataset(path_dir_X=path_dir_X, path_dir_Y=path_dir_Y, n_test=n_test, n_val=n_val, batch_size=batch_size)\n",
    "dataloder = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, (data_X, data_Y) in enumerate(dataloder):\n",
    "   # print(data_X.shape, data_Y.shape)\n",
    "   print(data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_data_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_val_data()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_test_data()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dataset.return_val_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
