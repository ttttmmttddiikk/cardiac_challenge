{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.integrate import odeint #常微分方程式\n",
    "from scipy.integrate import solve_ivp #常微分方程式\n",
    "#from scipy.optimize import minimize #非線形計画問題\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import math\n",
    "import numbers\n",
    "import warnings\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original module\n",
    "import sys\n",
    "sys.path.append(\"G:\\マイドライブ\\python_functions\") #win\n",
    "sys.path.append(\"/Volumes/GoogleDrive/マイドライブ/python_functions\") #mac\n",
    "sys.path.append(\"/home/tatematsudaiki/GoogleDrive/python_functions/\") #ubuntu\n",
    "#import\n",
    "import functions_processing\n",
    "import functions_time\n",
    "import functions_analyze_TS\n",
    "import functions_statistics\n",
    "import functions_clustering\n",
    "import functions_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#演算を行うデバイスを設定する　model = model.to(device) / x = x.to(device) で利用\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "print(f'using: {device}')\n",
    "print(f'number of devices: {torch.cuda.device_count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessingの関数\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#指定したランダムなデータを作成\n",
    "def return_randam_data(batch_size, list_randam, Standardization_mean, Standardization_std):\n",
    "    #data to return\n",
    "    randam_data = torch.Tensor()\n",
    "    #make\n",
    "    for _,format in enumerate(list_randam):\n",
    "        #select\n",
    "        if format == \"float\":\n",
    "            randam_data_now = torch.randn((batch_size,1), dtype=torch.float32)\n",
    "        elif isinstance(format,tuple):\n",
    "            randam_data_now = torch.randint(format[0], format[1]+1, (batch_size,1), dtype=torch.float32)\n",
    "        else:\n",
    "            print(\"error!!!!!\")\n",
    "        #add\n",
    "        randam_data = torch.cat((randam_data,randam_data_now),1)\n",
    "    #Standardization\n",
    "    if (Standardization_mean != False) and (Standardization_std != False):\n",
    "        randam_data = (randam_data - Standardization_mean)/Standardization_std\n",
    "    #return\n",
    "    return randam_data\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#0or1の値 & 一部のみ被らないようにする\n",
    "def return_0or1_data(batch_size, metadatas_size, list_samedatas_tuple):\n",
    "    #randome data raw\n",
    "    randam_data = torch.randint(0, 2, (batch_size,metadatas_size), dtype=torch.float32) #全て0or1\n",
    "    #change\n",
    "    for _,tuple in enumerate(list_samedatas_tuple):\n",
    "        #全て0にする\n",
    "        randam_data[:, tuple[0]:tuple[1]+1] = 0 #tau2として利用\n",
    "        if len(tuple)==2:\n",
    "            #一つだけ1にする\n",
    "            for bs in range(batch_size):\n",
    "                random_int = random.randint(tuple[0],tuple[1]+1)\n",
    "                randam_data[bs, random_int] = 1\n",
    "        if len(tuple)==3:\n",
    "            #一つだけ1にする\n",
    "            randam_data[:, tuple[0]+tuple[2]] = 1\n",
    "    #return\n",
    "    return randam_data\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#maskから一部欠損させる\n",
    "def lose_value(mask, n):\n",
    "    #\n",
    "    for _ in range(n):\n",
    "        #batch size / mask size\n",
    "        batch_size = mask.size()[0]\n",
    "        mask_size = mask.size()[1]\n",
    "        #lose value\n",
    "        mask[random.randint(0, batch_size-1)][random.randint(0, mask_size-1)] = 0\n",
    "    #return\n",
    "    return mask\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#「入力時系列データx」から「マスクm」を作成 \n",
    "# (=「値==0 → 0」「値!=0 → 1」にする)\n",
    "def make_mask(timeSeries,NA_value): #=torch.Size([timeSeries_size])\n",
    "    mask = timeSeries.clone().detach() #独立したテンソルを作成\n",
    "    mask = torch.where(mask==NA_value,0,1).to(torch.float32) #「値==0 → 0」「値!=0 → 1」にする\n",
    "    return mask #=torch.Size([timeSeries_size])\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#「マスクm」から「δ」を作成 (等間隔)\n",
    "def make_delta_equalInterval(mask): #=torch.Size([batch_size, timeSeries_size])\n",
    "    #値が全て0のδを作成\n",
    "    delta = torch.zeros(mask.size()).to(device)\n",
    "    #batch_size, timeSeries_sizeを取得\n",
    "    batch_size = mask.size()[0]\n",
    "    timeSeries_size = mask.size()[1]\n",
    "    #δに値を入れていく\n",
    "    for batch_index in range(batch_size):\n",
    "        for timeSeries_index in range(1,timeSeries_size):\n",
    "            mask_value_before1 = mask[batch_index][timeSeries_index-1]\n",
    "            if mask_value_before1 == 0:\n",
    "                delta[batch_index][timeSeries_index] = 1 + delta[batch_index][timeSeries_index-1]\n",
    "            elif mask_value_before1 == 1:\n",
    "                delta[batch_index][timeSeries_index] = 1\n",
    "    #return\n",
    "    return delta #=torch.Size([batch_size, timeSeries_size])\n",
    "\n",
    "#「マスクm」から「δ」を作成 (不等間隔)\n",
    "def make_delta_notEqualInterval(mask,interval): #=torch.Size([batch_size, timeSeries_size])\n",
    "    #値が全て0のδを作成\n",
    "    delta = torch.zeros(mask.size()).to(device)\n",
    "    #batch_size, timeSeries_sizeを取得\n",
    "    batch_size = mask.size()[0]\n",
    "    timeSeries_size = mask.size()[1]\n",
    "    #δに値を入れていく\n",
    "    for batch_index in range(batch_size):\n",
    "        for timeSeries_index in range(1,timeSeries_size):\n",
    "            mask_value_before1 = mask[batch_index][timeSeries_index-1]\n",
    "            interval_from_before1 = interval[batch_index][timeSeries_index] - interval[batch_index][timeSeries_index-1]\n",
    "            if mask_value_before1 == 0:\n",
    "                delta[batch_index][timeSeries_index] = interval_from_before1 + delta[batch_index][timeSeries_index-1]\n",
    "            elif mask_value_before1 == 1:\n",
    "                delta[batch_index][timeSeries_index] = interval_from_before1\n",
    "    #return\n",
    "    return delta #=torch.Size([batch_size, timeSeries_size])\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#常微分方程式を定義 / vaccination2 ver\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html\n",
    "def func(t, M1_M2_A, H2, m, tau1, tau2, d, mu, H1, eta1, K, eta2):\n",
    "    #print(t)\n",
    "    #extract\n",
    "    M1,M2,A = M1_M2_A\n",
    "    #formulas\n",
    "    dM1dt = -d*M1 if tau1<=t else 0\n",
    "    dM2dt = -d*M2 if tau2<=t else 0\n",
    "    if 0<=t<tau1+eta1:\n",
    "        dAdt = 0\n",
    "    elif tau1+eta1<=t<tau2+eta2:\n",
    "        dAdt = H1*(M1**m/(K**m+M1**m)) - mu*A\n",
    "    elif tau2+eta2<=t:\n",
    "        dAdt = H2*((M1+M2)**m/(K**m+(M1+M2)**m)) - mu*A\n",
    "\n",
    "    #return\n",
    "    return [dM1dt, dM2dt, dAdt]\n",
    "\n",
    "#「生成したparamsから作った時系列データ」と「生成した時系列データ」とのlossを計算\n",
    "def return_timeSeries_of_m_i_loss(params,      #=numpy.ndarray([batch_size,params_size]) #H2, m, tau2\n",
    "                                batch_size,timeSeries_size):\n",
    "\n",
    "    #timeSeriesOf_A_generatedByParamsを作成\n",
    "    timeSeriesOf_A_generatedByParams_ndarray = np.zeros((batch_size,timeSeries_size))\n",
    "    \n",
    "    #timeSeriesOf_A_generatedByParamsに値を追加\n",
    "    for batch_index in range(batch_size):\n",
    "        #paramsを取り出す(H2,m,tau2)\n",
    "        H2 = params[batch_index][0]\n",
    "        m = params[batch_index][1] #\n",
    "        tau2 = params[batch_index][2] #0<tau2\n",
    "        #constants (estimated by 12 health care workers)\n",
    "        M10_M20_A0 = [100,100,0] #M10(=D1),M20(=D2)はテキトーに決められている\n",
    "        tau1 = 0\n",
    "        d = 0.693\n",
    "        mu = 0.885346\n",
    "        H1 = 975.297\n",
    "        eta1 = 12.52\n",
    "        K = 33900\n",
    "        eta2 = 4.25\n",
    "\n",
    "        #ODE\n",
    "        ODE_output = solve_ivp(fun=func,\n",
    "                                t_span=[0.0,timeSeries_size],\n",
    "                                y0=M10_M20_A0,\n",
    "                                method='RK45',\n",
    "                                t_eval=np.arange(0.0, timeSeries_size, 1),\n",
    "                                args=(H2, m, tau1, tau2, d, mu, H1, eta1, K, eta2))\n",
    "        #print(ODE_output.y.shape)\n",
    "\n",
    "        #check ODE success\n",
    "        if ODE_output.success == False:\n",
    "            timeSeriesOf_A_generatedByParams_ndarray[batch_index] = [np.nan for i in range(timeSeries_size)]\n",
    "            print(\"ODE error\")\n",
    "            #break\n",
    "        #pass check\n",
    "        else:\n",
    "            #追加\n",
    "            timeSeriesOf_A_generatedByParams_ndarray[batch_index] = ODE_output.y[2]\n",
    "\n",
    "    #loss\n",
    "    #numpy.ndarray -> torch.Tensor\n",
    "    #timeSeriesOf_V_generatedByParams = torch.tensor(timeSeriesOf_A_generatedByParams_ndarray, dtype=torch.float32).to(device) #=torch.Size([batch_size,timeSeries_size])\n",
    "\n",
    "    #return\n",
    "    return timeSeriesOf_A_generatedByParams_ndarray #=torch.Size([batch_size,timeSeries_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return_0or1_data(batch_size=10, metadatas_size=10, list_samedatas_tuple=[(5,8,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#「生成したparamsから作った時系列データ」と「生成した時系列データ」とのlossを計算\n",
    "def func_m_i_loss(params,      #=numpy.ndarray([batch_size,params_size]) #H2, m, tau1, tau2\n",
    "                  timeSeriesOf_A_generatedByGAN): #=torch.Size([batch_size,timeSeries_size])\n",
    "\n",
    "    #batch_size\n",
    "    batch_size = timeSeriesOf_A_generatedByGAN.size()[0]\n",
    "    #timeSeries_size\n",
    "    timeSeries_size = timeSeriesOf_A_generatedByGAN.size()[1]\n",
    "\n",
    "    #timeSeriesOf_A_generatedByParamsを作成\n",
    "    timeSeriesOf_A_generatedByParams_ndarray = np.zeros((batch_size,timeSeries_size))\n",
    "    \n",
    "    #timeSeriesOf_A_generatedByParamsに値を追加\n",
    "    for batch_index in range(batch_size):\n",
    "        \n",
    "        #paramsを取り出す(H2,m,tau2)\n",
    "        H2 = params[batch_index][0]\n",
    "        m = params[batch_index][1] #\n",
    "        tau2 = params[batch_index][2] #0<tau2\n",
    "        #constants (estimated by 12 health care workers)\n",
    "        M10_M20_A0 = [100,100,0] #M10(=D1),M20(=D2)はテキトーに決められている\n",
    "        tau1 = 0\n",
    "        d = 0.693\n",
    "        mu = 0.885346\n",
    "        H1 = 975.297\n",
    "        eta1 = 12.52\n",
    "        K = 33900\n",
    "        eta2 = 4.25\n",
    "\n",
    "        #ODE\n",
    "        ODE_output = solve_ivp(fun=func,\n",
    "                                t_span=[0.0,timeSeries_size],\n",
    "                                y0=M10_M20_A0,\n",
    "                                method='RK45',\n",
    "                                t_eval=np.arange(0.0, timeSeries_size, 1),\n",
    "                                args=(H2, m, tau1, tau2, d, mu, H1, eta1, K, eta2))\n",
    "        #print(ODE_output.y.shape)\n",
    "        \n",
    "        #check params\n",
    "        if tau2 <= 0:\n",
    "            timeSeriesOf_A_generatedByParams_ndarray[batch_index] = [100 for i in range(timeSeries_size)]\n",
    "            print(\"params error\")\n",
    "            #break\n",
    "        #check ODE success\n",
    "        elif ODE_output.success == False:\n",
    "            timeSeriesOf_A_generatedByParams_ndarray[batch_index] = [100 for i in range(timeSeries_size)]\n",
    "            print(\"ODE error\")\n",
    "            #break\n",
    "        #pass check\n",
    "        else:\n",
    "            #追加\n",
    "            timeSeriesOf_A_generatedByParams_ndarray[batch_index] = ODE_output.y[2]\n",
    "\n",
    "    #loss\n",
    "    #numpy.ndarray -> torch.Tensor\n",
    "    timeSeriesOf_V_generatedByParams = torch.tensor(timeSeriesOf_A_generatedByParams_ndarray, dtype=torch.float32).to(device) #=torch.Size([batch_size,timeSeries_size])\n",
    "    print(timeSeriesOf_V_generatedByParams)\n",
    "    #lossを返す\n",
    "    loss = torch.mean(torch.square(timeSeriesOf_A_generatedByGAN-timeSeriesOf_V_generatedByParams),-1, True) #2乗して平均 #=torch.Size([batch_size,1])\n",
    "    print(loss)\n",
    "\n",
    "    #return\n",
    "    return loss #=torch.Size([batch_size,1])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#「生成したparamsから作った時系列データ」と「生成した時系列データ」とのlossを計算\n",
    "def func_m_i_loss(params,      #=numpy.ndarray([batch_size,params_size]) #H2, m, tau1, tau2\n",
    "                  timeSeriesOf_A_generatedByGAN): #=torch.Size([batch_size,timeSeries_size])\n",
    "\n",
    "    #batch_size\n",
    "    batch_size = timeSeriesOf_A_generatedByGAN.size()[0]\n",
    "    #timeSeries_size\n",
    "    timeSeries_size = timeSeriesOf_A_generatedByGAN.size()[1]\n",
    "\n",
    "    #timeSeriesOf_A_generatedByParamsを作成\n",
    "    timeSeriesOf_A_generatedByParams_ndarray = np.zeros((batch_size,timeSeries_size))\n",
    "    \n",
    "    #timeSeriesOf_A_generatedByParamsに値を追加\n",
    "    for batch_index in range(batch_size):\n",
    "        \n",
    "        #paramsを取り出す(H2,m,tau2)\n",
    "        H2 = params[batch_index][0]\n",
    "        m = params[batch_index][1] #\n",
    "        tau2 = params[batch_index][2] #0<tau2\n",
    "        #constants (estimated by 12 health care workers)\n",
    "        M10_M20_A0 = [100,100,0] #M10(=D1),M20(=D2)はテキトーに決められている\n",
    "        tau1 = 0\n",
    "        d = 0.693\n",
    "        mu = 0.885346\n",
    "        H1 = 975.297\n",
    "        eta1 = 12.52\n",
    "        K = 33900\n",
    "        eta2 = 4.25\n",
    "\n",
    "        #ODE\n",
    "        ODE_output = odeint(func=func, #微分方程式\n",
    "                            y0=M10_M20_A0, #初期値\n",
    "                            t=np.arange(0, 366, 0.01), #変数\n",
    "                            args=(H2, m, tau1, tau2, d, mu, H1, eta1, K, eta2)) #定数\n",
    "    \n",
    "        #print(ODE_output.y.shape)\n",
    "        \n",
    "        #追加\n",
    "        timeSeriesOf_A_generatedByParams_ndarray[batch_index] = ODE_output[::100][:,2]\n",
    "\n",
    "    #loss\n",
    "    #numpy.ndarray -> torch.Tensor\n",
    "    timeSeriesOf_V_generatedByParams = torch.tensor(timeSeriesOf_A_generatedByParams_ndarray, dtype=torch.float32).to(device) #=torch.Size([batch_size,timeSeries_size])\n",
    "    #print(timeSeriesOf_V_generatedByParams)\n",
    "    #lossを返す\n",
    "    loss = torch.mean(torch.square(timeSeriesOf_A_generatedByGAN-timeSeriesOf_V_generatedByParams),-1, True) #2乗して平均 #=torch.Size([batch_size,1])\n",
    "    #print(loss)\n",
    "\n",
    "    #return\n",
    "    return loss #=torch.Size([batch_size,1])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc28f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#[モデル定義]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#GRUD hiddenなし\n",
    "class GRUD_inputSize1(torch.nn.Module):\n",
    "    #################################################################\n",
    "    #[利用する関数]\n",
    "\n",
    "    #################################################################\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUD_inputSize1, self).__init__()\n",
    "\n",
    "        #params------------------------------------------------------\n",
    "        self.input_size = 1 #入力xの次元数\n",
    "        #self.input_size = input_size #入力xの次元数\n",
    "        self.hidden_size = hidden_size #隠れ層hの次元数\n",
    "        #self.hidden_size = output_size #隠れ層hの次元数\n",
    "        #self.output_size = input_size #出力層yの次元数\n",
    "\n",
    "        #self.delta_size = input_size\n",
    "        #self.mask_size = input_size\n",
    "\n",
    "        #重みw,バイアスb-----------------------------------------------\n",
    "        self.gamma_x_l = nn.Linear(self.input_size,self.input_size)\n",
    "        self.gamma_h_l = nn.Linear(self.input_size,self.input_size)\n",
    "\n",
    "        self.z_l = nn.Linear(self.input_size+self.input_size+self.input_size, self.input_size)\n",
    "        self.r_l = nn.Linear(self.input_size+self.input_size+self.input_size, self.input_size)\n",
    "        self.h_l = nn.Linear(self.input_size+self.input_size+self.input_size, self.input_size)\n",
    "        \n",
    "    #################################################################\n",
    "    #[利用する関数]\n",
    "    \n",
    "    #################################################################\n",
    "    def forward(self, \n",
    "                timeSeries_input, #時系列データ =torch.Size([batch_size, timeSeries_size])\n",
    "                mask_input, #マスクm =torch.Size([batch_size, timeSeries_size])\n",
    "                delta_input): #δ =torch.Size([batch_size, timeSeries_size])\n",
    "                \n",
    "        #-------------------------------------------------------------------------------------------------------\n",
    "        #[パラメータを取り出す]\n",
    "        #batch_size :バッチサイズ\n",
    "        batch_size = timeSeries_input.size(0) #int\n",
    "        #input_size\n",
    "        input_size = 1\n",
    "        #step_size :step_size=時系列データのsize\n",
    "        step_size = timeSeries_input.size(1) #int\n",
    "        #hidden_size :隠れ層hのsize\n",
    "        #hidden_size = self.hidden_size #int\n",
    "        #output_size = self.input_size #出力層yの次元数\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------\n",
    "        #[処理]\n",
    "\n",
    "        #比較に利用する零行列\n",
    "        zeros_usedByGamma_x = torch.zeros(batch_size,self.input_size, dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size]) \n",
    "        zeros_usedByGamma_h = torch.zeros(batch_size,self.input_size, dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size])  \n",
    "\n",
    "        #output :作成する「欠損なし時系列データ」(=yの歴代まとめ)\n",
    "        timeSeries_output = torch.Tensor().to(device) #=torch.Size([batch_size, timeSeries_size])\n",
    "\n",
    "        #h :隠れ層h\n",
    "        #h = torch.zeros(batch_size, input_size, dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size])\n",
    "        h = torch.randn((batch_size,input_size), dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size])\n",
    "\n",
    "        #x_last_obsv :直近最後に存在したxの値\n",
    "        x_last_obsv = timeSeries_input[:,0:1] #=torch.Size([batch_size, input_size])\n",
    "        #print(x_last_obsv)\n",
    "\n",
    "        #x_mean :xの平均\n",
    "        x_mean = torch.sum(torch.where(mask_input==0,mask_input,timeSeries_input).to(torch.float32), 1, keepdim=True) / torch.sum(mask_input, 1, keepdim=True) #=torch.Size([batch_size, input_size])\n",
    "        #print(x_mean)\n",
    "\n",
    "        #[処理]\n",
    "        #print(timeSeries_input)\n",
    "        #print(mask_input)\n",
    "        for step_index in range(step_size): #timeSeries_size回実行する\n",
    "            #timeSeries,m,δから今回に処理する部分を取り出す\n",
    "            x = timeSeries_input[:,step_index:step_index+1] #=torch.Size([batch_size, input_size])\n",
    "            m = mask_input[:,step_index:step_index+1] #=torch.Size([batch_size, input_size])\n",
    "            d = delta_input[:,step_index:step_index+1] #=torch.Size([batch_size, input_size])\n",
    "            #print(x.size())\n",
    "            #print(x)\n",
    "            #print(m)\n",
    "            #print(d)\n",
    "\n",
    "            #[x^の作成]---------------------------------------------------------------------------------\n",
    "            #γ_x (=xのdecay rates)を作成\n",
    "            gamma_x = torch.exp(-torch.max(zeros_usedByGamma_x, self.gamma_x_l(d))) #=torch.Size([batch_size, input_size])\n",
    "            #print(gamma_x)\n",
    "\n",
    "            #x_last_obsvを更新\n",
    "            x_last_obsv = torch.where(m==1, x, x_last_obsv).to(torch.float32) #=torch.Size([batch_size, input_size])\n",
    "            #print(x_last_obsv)\n",
    "\n",
    "            #x^を作成\n",
    "            x_ = m*x+(1-m)*(gamma_x*x_last_obsv+(1-gamma_x)*x_mean) #=torch.Size([batch_size,input_size])\n",
    "            #print(x_)\n",
    "\n",
    "            #[h^の作成]---------------------------------------------------------------------------------\n",
    "            #γ_h (=hのdecay rates)を作成\n",
    "            gamma_h = torch.exp(-torch.max(zeros_usedByGamma_h, self.gamma_h_l(d))) #=torch.Size([batch_size,input_size])  \n",
    "            #print(gamma_h)\n",
    "            \n",
    "            #h^を作成\n",
    "            h_ = gamma_h*h #=torch.Size([batch_size,input_size])\n",
    "            #print(h_)\n",
    "\n",
    "            #[GRU部分]---------------------------------------------------------------------------------\n",
    "            #結合する\n",
    "            combined = torch.cat((x_, h_, m), 1)\n",
    "\n",
    "            #z\n",
    "            z = torch.sigmoid(self.z_l(combined)) #=torch.Size([batch_size,input_size])\n",
    "            #print(z)\n",
    "\n",
    "            #r\n",
    "            r = torch.sigmoid(self.r_l(combined)) #=torch.Size([batch_size,input_size])\n",
    "            #print(r)\n",
    "\n",
    "            #結合する\n",
    "            combined_r = torch.cat((x_, r*h_, m), 1)\n",
    "\n",
    "            #h_tilde\n",
    "            h_tilde = torch.tanh(self.h_l(combined_r)) #=torch.Size([batch_size,input_size])\n",
    "            #print(h_tilde)\n",
    "\n",
    "            #hを作成\n",
    "            h = (1-z)*h_ + z*h_tilde #=torch.Size([batch_size,input_size])\n",
    "            #h = self.layer1(h) #=torch.Size([batch_size,input_size])\n",
    "            #h = h.unsqueeze(-1) #=torch.Size([batch_size,input_size,1])\n",
    "            #print(h)\n",
    "\n",
    "            #y(output)\n",
    "            #y = m*x+(1-m)*h #=torch.Size([batch_size,input_size])\n",
    "\n",
    "            #yの歴代まとめ(=timeSeries_output)\n",
    "            timeSeries_output = torch.cat((timeSeries_output,h),1) #=torch.Size([batch_size,1+1+...]) -> torch.Size([batch_size,timeSeries_size]) \n",
    "\n",
    "        #return\n",
    "        return timeSeries_output #=torch.Size([batch_size,timeSeries_size])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#GRUD hiddenあり\n",
    "class GRUD_inputSize1(torch.nn.Module):\n",
    "    #################################################################\n",
    "    #[利用する関数]\n",
    "\n",
    "    #################################################################\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUD_inputSize1, self).__init__()\n",
    "\n",
    "        #params------------------------------------------------------\n",
    "        self.input_size = 1 #入力xの次元数\n",
    "        #self.input_size = input_size #入力xの次元数\n",
    "        self.hidden_size = hidden_size #隠れ層hの次元数\n",
    "        #self.hidden_size = output_size #隠れ層hの次元数\n",
    "        #self.output_size = input_size #出力層yの次元数\n",
    "\n",
    "        #self.delta_size = input_size\n",
    "        #self.mask_size = input_size\n",
    "\n",
    "        self.hidden_size_inner = math.ceil(self.hidden_size/2)\n",
    "\n",
    "        #重みw,バイアスb-----------------------------------------------\n",
    "        self.gamma_x_l = nn.Linear(self.input_size,self.input_size)\n",
    "        self.gamma_h_l = nn.Linear(self.input_size,self.hidden_size)\n",
    "\n",
    "        self.z_l = nn.Linear(self.hidden_size+self.input_size+self.input_size, self.hidden_size)\n",
    "        self.r_l = nn.Linear(self.hidden_size+self.input_size+self.input_size, self.hidden_size)\n",
    "        self.h_l = nn.Linear(self.hidden_size+self.input_size+self.input_size, self.hidden_size)\n",
    "        \n",
    "        #layer1 :batch_size,hidden_size → batch_size,input_size\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size_inner),\n",
    "            #nn.BatchNorm1d(self.hidden_size_inner),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(self.hidden_size_inner, self.hidden_size_inner),\n",
    "            #nn.BatchNorm1d(self.hidden_size_inner),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(self.hidden_size_inner, self.input_size),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "    #################################################################\n",
    "    #[利用する関数]\n",
    "    \n",
    "    #################################################################\n",
    "    def forward(self, \n",
    "                timeSeries_input, #時系列データ =torch.Size([batch_size, timeSeries_size])\n",
    "                mask_input, #マスクm =torch.Size([batch_size, timeSeries_size])\n",
    "                delta_input): #δ =torch.Size([batch_size, timeSeries_size])\n",
    "                \n",
    "        #-------------------------------------------------------------------------------------------------------\n",
    "        #[パラメータを取り出す]\n",
    "        #batch_size :バッチサイズ\n",
    "        batch_size = timeSeries_input.size(0) #int\n",
    "        #input_size\n",
    "        input_size = 1\n",
    "        #step_size :step_size=時系列データのsize\n",
    "        step_size = timeSeries_input.size(1) #int\n",
    "        #hidden_size :隠れ層hのsize\n",
    "        hidden_size = self.hidden_size #int\n",
    "        #output_size = self.input_size #出力層yの次元数\n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------\n",
    "        #[処理]\n",
    "\n",
    "        #比較に利用する零行列\n",
    "        zeros_usedByGamma_x = torch.zeros(batch_size,self.input_size, dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size]) \n",
    "        zeros_usedByGamma_h = torch.zeros(batch_size,self.hidden_size, dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size])  \n",
    "\n",
    "        #output :作成する「欠損なし時系列データ」(=yの歴代まとめ)\n",
    "        timeSeries_output = torch.Tensor().to(device) #=torch.Size([batch_size, timeSeries_size])\n",
    "\n",
    "        #h :隠れ層h\n",
    "        h = torch.zeros(batch_size, hidden_size, dtype=torch.float32).to(device) #=torch.Size([batch_size,hidden_size])\n",
    "        #h = torch.randn((batch_size, hidden_size), dtype=torch.float32).to(device) #=torch.Size([batch_size,input_size])\n",
    "\n",
    "        #x_last_obsv :直近最後に存在したxの値\n",
    "        x_last_obsv = timeSeries_input[:,0:1] #=torch.Size([batch_size, input_size])\n",
    "        #print(x_last_obsv)\n",
    "\n",
    "        #x_mean :xの平均\n",
    "        x_mean = torch.sum(torch.where(mask_input==0,mask_input,timeSeries_input).to(torch.float32), 1, keepdim=True) / torch.sum(mask_input, 1, keepdim=True) #=torch.Size([batch_size, input_size])\n",
    "        #print(x_mean)\n",
    "\n",
    "        #[処理]\n",
    "        #print(timeSeries_input)\n",
    "        #print(mask_input)\n",
    "        for step_index in range(step_size): #timeSeries_size回実行する\n",
    "            #timeSeries,m,δから今回に処理する部分を取り出す\n",
    "            x = timeSeries_input[:,step_index:step_index+1] #=torch.Size([batch_size, input_size])\n",
    "            m = mask_input[:,step_index:step_index+1] #=torch.Size([batch_size, input_size])\n",
    "            d = delta_input[:,step_index:step_index+1] #=torch.Size([batch_size, input_size])\n",
    "            #print(x.size())\n",
    "            #print(x)\n",
    "            #print(m)\n",
    "            #print(d)\n",
    "\n",
    "            #[x^の作成]---------------------------------------------------------------------------------\n",
    "            #γ_x (=xのdecay rates)を作成\n",
    "            gamma_x = torch.exp(-torch.max(zeros_usedByGamma_x, self.gamma_x_l(d))) #=torch.Size([batch_size, input_size])\n",
    "            #print(gamma_x)\n",
    "\n",
    "            #x_last_obsvを更新\n",
    "            x_last_obsv = torch.where(m==1, x, x_last_obsv).to(torch.float32) #=torch.Size([batch_size, input_size])\n",
    "            #print(x_last_obsv)\n",
    "\n",
    "            #x^を作成\n",
    "            x_ = m*x+(1-m)*(gamma_x*x_last_obsv+(1-gamma_x)*x_mean) #=torch.Size([batch_size,input_size])\n",
    "            #print(x_)\n",
    "\n",
    "            #[h^の作成]---------------------------------------------------------------------------------\n",
    "            #γ_h (=hのdecay rates)を作成\n",
    "            gamma_h = torch.exp(-torch.max(zeros_usedByGamma_h, self.gamma_h_l(d))) #=torch.Size([batch_size,hidden_size])  \n",
    "            #print(gamma_h)\n",
    "            \n",
    "            #h^を作成\n",
    "            h_ = gamma_h*h #=torch.Size([batch_size,hidden_size])\n",
    "            #print(h_)\n",
    "\n",
    "            #[GRU部分]---------------------------------------------------------------------------------\n",
    "            #結合する\n",
    "            combined = torch.cat((x_, h_, m), 1)\n",
    "\n",
    "            #z\n",
    "            z = torch.sigmoid(self.z_l(combined)) #=torch.Size([batch_size,hidden_size])\n",
    "            #print(z)\n",
    "\n",
    "            #r\n",
    "            r = torch.sigmoid(self.r_l(combined)) #=torch.Size([batch_size,hidden_size])\n",
    "            #print(r)\n",
    "\n",
    "            #結合する\n",
    "            combined_r = torch.cat((x_, r*h_, m), 1)\n",
    "\n",
    "            #h_tilde\n",
    "            h_tilde = torch.tanh(self.h_l(combined_r)) #=torch.Size([batch_size,hidden_size])\n",
    "            #print(h_tilde)\n",
    "\n",
    "            #hを作成\n",
    "            h = (1-z)*h_ + z*h_tilde #=torch.Size([batch_size,hidden_size])\n",
    "            #h = h.unsqueeze(-1) #=torch.Size([batch_size,input_size,1])\n",
    "            #print(h)\n",
    "\n",
    "            #y(output)\n",
    "            y = self.layer1(h) #=torch.Size([batch_size,input_size])\n",
    "            #output = torch.where(m==1, x, y).to(torch.float32)\n",
    "            #y = m*x+(1-m)*y #=torch.Size([batch_size,input_size])\n",
    "            #print(y)\n",
    "\n",
    "            #yの歴代まとめ(=timeSeries_output)\n",
    "            timeSeries_output = torch.cat((timeSeries_output,y),1) #=torch.Size([batch_size,1+1+...]) -> torch.Size([batch_size,timeSeries_size]) \n",
    "\n",
    "        #return\n",
    "        return timeSeries_output #=torch.Size([batch_size,timeSeries_size])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccc663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator(生成器)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, timeSeries_size, params_size, features_size, input_size, GRUD_hidden_size, GRUD_output_size, batch_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        #params\n",
    "        self.latent_size = latent_size           #Generatorの入力変数(=潜在変数)のsize\n",
    "        \n",
    "        self.timeSeries_size = timeSeries_size   #時系列データ部分のsize\n",
    "        self.params_size = params_size             #パラメータ部分のsize (V0,β,γ,δ)\n",
    "        self.features_size = features_size      #features\n",
    "        self.input_size=input_size               #時系列データの数\n",
    "\n",
    "        self.GRUD_hidden_size=GRUD_hidden_size\n",
    "        self.GRUD_output_size=GRUD_output_size\n",
    "\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.inner_size=10\n",
    "\n",
    "        #layer0 :latent_size + features_size → param_size\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, #入力のチャネル数\n",
    "                        out_channels=10, #畳み込みをした後のチャネル数。フィルター数。\n",
    "                        kernel_size=2, #カーネルの大きさ。\n",
    "                        stride=1, #カーネルをどのくらい移動させるか。\n",
    "                        padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "            #nn.Conv1d(in_channels=10, #入力のチャネル数\n",
    "            #            out_channels=10, #畳み込みをした後のチャネル数。フィルター数。\n",
    "            #            kernel_size=2, #カーネルの大きさ。\n",
    "            #            stride=1, #カーネルをどのくらい移動させるか。\n",
    "            #            padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None),\n",
    "            #nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            #nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        #layer1 :latent_size + features_size → param_size\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #nn.BatchNorm1d(10*10),\n",
    "            nn.Linear(10*77, 10),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            nn.Linear(10, 5),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            #nn.Dropout(p=0.3, inplace=False),       \n",
    "            nn.Linear(5, self.params_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                latent_input, #=torch.Size([batch_size, latent_size])\n",
    "                features_input): #=torch.Size([batch_size, features_size])\n",
    "        #layer0のfp\n",
    "        input0 = torch.cat([latent_input, features_input],1).reshape(self.batch_size, -1, self.latent_size+self.features_size) #(バッチサイズ,チャネル数,データ数)を入力\n",
    "        #print(input0.size())\n",
    "        output0 = self.layer0(input0).reshape(-1, 10*77) #=torch.Size([batch_size, param_size])\n",
    "        #print(output0.size())\n",
    "        #layer1のfp\n",
    "        params_output1 = self.layer1(output0) #=torch.Size([batch_size, param_size])\n",
    "\n",
    "        #return\n",
    "        return params_output1 #=torch.Size([batch_size, param_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator(識別器)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_size, timeSeries_size, params_size, features_size, input_size, GRUD_hidden_size, GRUD_output_size, batch_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        #params\n",
    "        self.timeSeries_size = timeSeries_size #時系列データ部分のsize\n",
    "        self.params_size = params_size           #パラメータ部分のsize (V0,β,γ,δ)\n",
    "        self.features_size=features_size      #features\n",
    "        self.input_size=input_size             #時系列データの数\n",
    "        latent_size=latent_size                #Generatorの入力変数(=潜在変数)のsize\n",
    "\n",
    "        self.GRUD_hidden_size=GRUD_hidden_size\n",
    "        self.GRUD_output_size=GRUD_output_size\n",
    "\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.inner_size = 1\n",
    "\n",
    "        #layer0 :params,features  → validity(0~1の値)\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.params_size+self.features_size),\n",
    "            nn.Linear(self.params_size+self.features_size, 10),\n",
    "            #nn.BatchNorm1d(3),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(10,5),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(5,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \"\"\"#layer3 :params → inner_size\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.params_size),\n",
    "            nn.Linear(self.params_size, self.inner_size),\n",
    "            #nn.BatchNorm1d(2),\n",
    "            #nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            #nn.Linear(2, self.inner_size),\n",
    "        )\"\"\"\n",
    "\n",
    "        \"\"\"#layer4 :features → inner_size\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.features_size),\n",
    "            nn.Linear(self.features_size, self.inner_size),\n",
    "            #nn.BatchNorm1d(3),\n",
    "            #nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            #nn.Linear(3, self.inner_size),\n",
    "        )\"\"\"\n",
    "\n",
    "        \"\"\"#layer5 :(layer2のinner_size,layer3のinner_size) → validity(0~1の値)\n",
    "        self.layer5 = nn.Sequential(\n",
    "            #nn.BatchNorm1d(self.inner_size*3),\n",
    "            nn.Linear(self.inner_size*2, 1),\n",
    "            #nn.BatchNorm1d(3),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\"\"\"\n",
    "\n",
    "    def forward(self,\n",
    "                params_input, #=torch.Size([batch_size, param_size])\n",
    "                features_input): #=torch.Size([batch_size, features_size])\n",
    "\n",
    "        #layer3のfp :params_size → inner_size\n",
    "        #validity_output3 = self.layer3(params_input) #=torch.Size([batch_size, 1])\n",
    "\n",
    "        #layer4のfp :features_size → inner_size\n",
    "        #validity_output4 = self.layer4(features_input) #=torch.Size([batch_size, 1])\n",
    "\n",
    "        #layer5のfp :latent_size → size1(0～1の値)\n",
    "        #validity_output5 = self.layer5(torch.cat([validity_output3, validity_output4],1)) #=torch.Size([batch_size, 1])\n",
    "\n",
    "        #layer0のfp :params_size + features_size → size1(0～1の値)\n",
    "        validity_output0 = self.layer0(torch.cat([params_input, features_input],1)) #=torch.Size([batch_size, 1])\n",
    "        #validity_output0 = self.layer0(params_input) #=torch.Size([batch_size, 1])\n",
    "\n",
    "        return validity_output0 #=torch.Size([batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e950037",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#[設定値]\n",
    "input_size = 1       #時系列データの数(変数の数)\n",
    "#設定値(GRUD)\n",
    "GRUD_hidden_netG_size = 10            #GRUDのhの次元数\n",
    "GRUD_hidden_netD_size = 2            #GRUDのhの次元数\n",
    "GRUD_output_size = input_size     #GRUDのyの次元数\n",
    "\n",
    "#設定値(DataLoader)\n",
    "batch_size = 300 #バッチサイズ\n",
    "dataloader_suffle = True #dataloaderをシャッフル\n",
    "dataloader_drop_last=True #バッチサイズで取り出した時の余りを除去\n",
    "\n",
    "#設定値(GAN学習)\n",
    "EPOCHS = 500000 #エポック数\n",
    "latent_size = 10 #Generator(生成器)の入力値(=潜在変数)の次元数\n",
    "\n",
    "netG_learn_n_start = 1 #識別器の学習1回につき何回生成器を学習させるか(start)\n",
    "netG_learn_n_end = 1 #識別器の学習1回につき何回生成器を学習させるか(end)\n",
    "\n",
    "#設定値(m-i-GAN)\n",
    "#m_i_GAN_start = 300 #m_i_lossの計算をいつ始めるか(エポック数で指定)\n",
    "\n",
    "#設定値(評価)\n",
    "plot_fake_datas_interval = 10000 #生成データの表示間隔(エポック数で指定)\n",
    "plot_fake_datas_n = 300 #生成データの表示数\n",
    "\n",
    "plot_losses_interval = plot_fake_datas_interval #loss履歴の表示間隔(エポック数で指定)\n",
    "\n",
    "print_loss_interval = 100 #lossをprintする間隔(epoch数で指定)\n",
    "\n",
    "plot_params_interval = plot_fake_datas_interval #paramsをscatter plot\n",
    "plot_params_n = plot_fake_datas_n\n",
    "\n",
    "#設定値(保存)\n",
    "#save_models = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af007c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#[データセットの作成]\n",
    "#read\n",
    "df_datas = pd.read_csv('../datas/all_fitted_dummyVariable_0or1_withoutgroup0.csv', index_col=0) #補間後の欠損なし時系列データ\n",
    "#df_datas = pd.read_csv('../features_and_params_and_timeSeries_raw_vaccination2_dummyVariable_withoutgroup0.csv', index_col=0) #補間前の欠損あり時系列データ\n",
    "print(df_datas)\n",
    "\n",
    "#outlier\n",
    "df_datas = functions_statistics.return_df_without_outlier(df_input=df_datas, list_of_row_use=[\"H2\",\"m\"])\n",
    "\n",
    "#extract\n",
    "df_timeSeries = df_datas.loc[:,[str(i) for i in range(0,366)]] #補間後の欠損なし時系列データ\n",
    "df_params = df_datas.loc[:,[\"H2\",\"m\"]]\n",
    "df_features = df_datas.loc[:,[\"age_{}\".format(i) for i in range(0,100+1)]\\\n",
    "                            +\n",
    "                            [\"bloodtype_a\",\"bloodtype_b\",\"bloodtype_o\"]\n",
    "                            +\n",
    "                            [\"drinking_sometime\",\"drinking_everyday\"]\n",
    "                            +\n",
    "                            [#\"tau2\",\n",
    "                            \"sex\",\n",
    "                            \"sideReaction1stOr2nd_localPain\",\"sideReaction1stOr2nd_under37.5\",\"sideReaction1stOr2nd_over37.5\",\"sideReaction1stOr2nd_dull\",\"sideReaction1stOr2nd_headache\",\"sideReaction1stOr2nd_jointPain\",\"sideReaction1stOr2nd_diarrhea\",\"sideReaction1stOr2nd_nausea\",\"sideReaction1stOr2nd_dizziness\",\"sideReaction1stOr2nd_others\",\n",
    "                            \"smoking\",\n",
    "                            \"medicine_steroid\",\"medicine_NSAIDs\",\"medicine_acetaminophen\",\"medicine_antihistamine\",\"medicine_immunosuppression\",\"medicine_biologicalDrag\",\"medicine_anticancer\",\n",
    "                            \"disease_covid19Me\",\"disease_highBloodPressure\",\"disease_diabetes\",\"disease_bronchialAsthma\",\"disease_anaphylaxis\",\"disease_mentalIllness\",\"disease_covid19Family\",\"disease_gout\",\"disease_highCholesterol\",\"disease_rheumatism\",\"disease_respiratory\",\"disease_circulatoryOrgan\",\"disease_connectiveTissueDisease\",\"disease_allergies\",\"disease_immunodeficiency\",\"disease_malignantTumor\",\"disease_thyroid\",\"disease_liver\",\"disease_kidney\",\"disease_others\",\n",
    "                            \"BCGVaccine\"]]\n",
    "#\n",
    "#df_features[\"age\"] = df_features[\"age\"]/100\n",
    "\n",
    "#df_params :column(縦)ごとにNormalize\n",
    "df_params,mean_params,std_params = functions_statistics.Standardization_row(df=df_params)\n",
    "\n",
    "mean_params = torch.tensor(np.tile(mean_params,(batch_size,1)), dtype=torch.float32).to(device)\n",
    "std_params = torch.tensor(np.tile(std_params,(batch_size,1)), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "#randamデータ生成のためのlist\n",
    "\"\"\"list_to_make_randomData = [#(1,365),\n",
    "                            (0,1),\n",
    "                            (0,100),\n",
    "                            (0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),\n",
    "                            (0,1),\n",
    "                            (0,1),\n",
    "                            (0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),\n",
    "                            (0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),(0,1),\n",
    "                            #(-1,3),\n",
    "                            (0,1)]\n",
    "print(len(list_to_make_randomData))\"\"\"\n",
    "list_samedatas_tuple = [(0,100),(101,103),(104,105)]\n",
    "list_samedatas_tuple_plot_20 = [(0,100,20),(101,103),(104,105)]\n",
    "list_samedatas_tuple_plot_80 = [(0,100,80),(101,103),(104,105)]\n",
    "\n",
    "\n",
    "#設定値(データセット)\n",
    "timeSeries_size = 366 #時系列データのsize\n",
    "params_size = len(df_params.columns) #パラメータ部分のsize (今回はH2,m)\n",
    "features_size = len(df_features.columns)\n",
    "print(timeSeries_size)\n",
    "print(params_size)\n",
    "print(features_size)\n",
    "\n",
    "\n",
    "\n",
    "#DataSetの実装\n",
    "class DataSet():\n",
    "    def __init__(self):\n",
    "        self.params = torch.tensor(df_params.values, dtype=torch.float32) #推定したパラメータ(H2, m)\n",
    "        self.features = torch.tensor(df_features.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.params) # データ数を返す\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.params[index],self.features[index] #index番目の入出力ペアを返す\n",
    "\n",
    "#DataLoaderの実装\n",
    "dataset = DataSet() #DataSetクラスのインスタンスを作成\n",
    "dataloader = torch.utils.data.DataLoader(dataset, #dataset\n",
    "                                        batch_size=batch_size, #バッチサイズ\n",
    "                                        shuffle=dataloader_suffle, #dataloaderをシャッフル\n",
    "                                        drop_last=dataloader_drop_last) #バッチサイズで取り出した時の余りを除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "###################################################\n",
    "#plot\n",
    "for index_int, row_series in df_timeSeries.iterrows():\n",
    "    #times\n",
    "    times = df_timeSeries.columns.values\n",
    "    #extract\n",
    "    values = row_series\n",
    "    #plot\n",
    "    plt.plot(times, values, marker=\".\", color=\"black\", markersize=0.03, linewidth=0.03, zorder=1)\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "#plt.xlim(0,100)\n",
    "plt.ylim(0,5000)\n",
    "plt.xticks(np.arange(0, 365, 30))\n",
    "#show\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af343852",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#[使う物の作成]\n",
    "#インスタンス作成 & GPUに送る\n",
    "netG = Generator(latent_size=latent_size, timeSeries_size=timeSeries_size, params_size=params_size, features_size=features_size, input_size=input_size, GRUD_hidden_size=GRUD_hidden_netG_size, GRUD_output_size=GRUD_output_size, batch_size=batch_size).to(device)\n",
    "netD = Discriminator(latent_size=latent_size, timeSeries_size=timeSeries_size, params_size=params_size, features_size=features_size, input_size=input_size, GRUD_hidden_size=GRUD_hidden_netD_size, GRUD_output_size=GRUD_output_size, batch_size=batch_size).to(device)\n",
    "print(sum(p.numel() for p in netG.parameters()))\n",
    "print(sum(p.numel() for p in netD.parameters()))\n",
    "\n",
    "#\n",
    "\"\"\"for p in netG.parameters():\n",
    "    nn.init.uni(p,7.0,10.0)\n",
    "for p in netD.parameters():\n",
    "    nn.init.normal_(p,0.0,1.0)\"\"\"\n",
    "\"\"\"for p in netG.parameters():\n",
    "    nn.init.normal_(p, 0.0, 2.0)\n",
    "for p in netD.parameters():\n",
    "    nn.init.normal_(p, 0.0, 1.0)\"\"\"\n",
    "\n",
    "# 複数GPU使用宣言\n",
    "\"\"\"if device == \"cuda\":\n",
    "    print(\"multi gpu\")\n",
    "    netG = torch.nn.DataParallel(netG) # make parallel\n",
    "    netD = torch.nn.DataParallel(netD) # make parallel\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "print(device)\n",
    "\"\"\"\n",
    "\n",
    "#誤差関数\n",
    "#criterion = nn.BCELoss().to(device) #誤差関数をGPUに送る\n",
    "\n",
    "#最適化アルゴリズム\n",
    "lr = 0.0001 #学習率\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "optimizer_netG = torch.optim.Adam(netG.parameters(), #最適化対象のパラメータ　w <- w-∂E/∂w(E=値)\n",
    "                                lr=lr, \n",
    "                                betas=[b1, b2])\n",
    "optimizer_netD = torch.optim.Adam(netD.parameters(), \n",
    "                                lr=lr, \n",
    "                                betas=[b1, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#別々version & fp複数回ver\n",
    "############################################################################\n",
    "#[学習]\n",
    "losses_real_validity = [] #「全エポックのloss履歴」(real)\n",
    "losses_fake_validity = [] #「全エポックのloss履歴」(fake)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1): #エポック数分だけ実行\n",
    "    loss_real_validity_sum = 0.0 #「今エポックの合計loss」(real)\n",
    "    loss_fake_validity_sum = 0.0 #「今エポックの合計loss」(fake)\n",
    "    loss_m_i_loss_sum = 0.0 #「今エポックの合計loss」(m-i-loss)\n",
    "    \n",
    "    iter_count = 1\n",
    "    for real_params,real_features in dataloader:\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「識別器の学習(real)」\n",
    "        #real data\n",
    "        real_params = real_params.to(device)\n",
    "        real_features = real_features.to(device)\n",
    "\n",
    "        #最適化対象のすべてのパラメータの勾配を0にする\n",
    "        optimizer_netD.zero_grad() \n",
    "        \n",
    "        #real dataでnetDをfp \n",
    "        real_validity = netD(params_input=real_params, features_input=real_features) #=torch.Size([batch_size, 1])\n",
    "        \n",
    "        #loss\n",
    "        loss_netD = -torch.mean(real_validity)\n",
    "        \n",
    "        #bp(=各パラメータの偏微分を計算)\n",
    "        loss_netD.backward()\n",
    "        #最適化\n",
    "        optimizer_netD.step()\n",
    "\n",
    "        #validity\n",
    "        real_validity_netD = torch.sigmoid(-loss_netD).item()\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「識別器の学習(fake)」\n",
    "        #fake data\n",
    "        random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "        #random_features = torch.randn((batch_size,features_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "        #random_features = return_randam_data(batch_size, list_to_make_randomData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "        random_features = return_0or1_data(batch_size=batch_size, metadatas_size=features_size, list_samedatas_tuple=list_samedatas_tuple).to(device) \n",
    "        \n",
    "        #最適化対象のすべてのパラメータの勾配を0にする\n",
    "        optimizer_netD.zero_grad() \n",
    "\n",
    "        #fake dataでnetGをfp\n",
    "        fake_params = netG(latent_input=random_latent, features_input=random_features) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "        fake_validity = netD(params_input=fake_params, features_input=random_features) #=torch.Size([batch_size, 1])\n",
    "\n",
    "        #loss\n",
    "        loss_netD = torch.mean(fake_validity) #これが小さくなってほしい\n",
    "\n",
    "        #bp(=各パラメータの偏微分を計算)\n",
    "        loss_netD.backward()\n",
    "        #最適化\n",
    "        optimizer_netD.step()\n",
    "\n",
    "        #validity\n",
    "        fake_validity_netD = torch.sigmoid(loss_netD).item()\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「生成器の学習」\n",
    "        \n",
    "        #識別器の学習の何回おきに生成器を学習させるか(イテレーション数で指定)\n",
    "        for n in range(netG_learn_n_start):\n",
    "        #for n in range(netG_learn_n_start + int((netG_learn_n_end-netG_learn_n_start)*epoch/EPOCHS)):\n",
    "            #fake data\n",
    "            random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            #random_features = torch.randn((batch_size,features_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            #random_features = return_randam_data(batch_size, list_to_make_randomData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "            random_features = return_0or1_data(batch_size=batch_size, metadatas_size=features_size, list_samedatas_tuple=list_samedatas_tuple).to(device)\n",
    "\n",
    "            #最適化対象のすべてのパラメータの勾配を0にする\n",
    "            optimizer_netG.zero_grad() \n",
    "\n",
    "            #fake dataでnetG & netDをfp\n",
    "            fake_params = netG(latent_input=random_latent, features_input=random_features) #=torch.Size([batch_size, param_size])\n",
    "            fake_validity = netD(params_input=fake_params, features_input=random_features) #=torch.Size([batch_size, 1])\n",
    "            \n",
    "            #loss\n",
    "            loss_netG = -torch.mean(fake_validity) #これが小さくなってほしい #=torch.Size([1])\n",
    "\n",
    "            #bp(=偏微分を計算)\n",
    "            loss_netG.backward()\n",
    "            #最適化\n",
    "            optimizer_netG.step()\n",
    "\n",
    "            #validity\n",
    "            #fake_validity_netG = torch.sigmoid(-loss_netG).item()\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #print\n",
    "        \"\"\"if epoch < m_i_GAN_start:\n",
    "            print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4}\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\n",
    "            #print('[Epoch %d/%d] [loss_netD: %f] [loss_netG: %f]'% (epoch, EPOCHS, loss_netD.item(), loss_netG.item()))\n",
    "        elif m_i_GAN_start <= epoch:\n",
    "            print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4}\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\n",
    "        #print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:None\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\"\"\"\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #sum\n",
    "        loss_real_validity_sum += real_validity_netD\n",
    "        loss_fake_validity_sum += fake_validity_netD\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #イテレーション数を+1\n",
    "        iter_count += 1\n",
    "        #print(iter_count)\n",
    "\n",
    "    ############################################################################\n",
    "    #「今エポックのloss平均」を「loss履歴」に追加\n",
    "    losses_real_validity.append(loss_real_validity_sum/(iter_count-1)) \n",
    "    losses_fake_validity.append(loss_fake_validity_sum/(iter_count-1))\n",
    "    \n",
    "    ############################################################################\n",
    "    #[評価]\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #今エポックの、「エポック数」,「識別機のloss」,「生成器のloss」を表示\n",
    "    if epoch%print_loss_interval == 0:\n",
    "        print(\"epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3}\".format(epoch, EPOCHS, losses_real_validity[-1], losses_fake_validity[-1]))\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #エポックの、paramsをscatterplot\n",
    "    if epoch%plot_params_interval == 0:\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #real data\n",
    "        plt.scatter(df_params[\"H2\"].values*std_params[0,0].item()+mean_params[0,0].item(), df_params[\"m\"].values*std_params[0,1].item()+mean_params[0,1].item(), s=0.2, c=\"black\")\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        for _ in range(math.ceil(plot_params_n/batch_size)):\n",
    "            #fake data\n",
    "            random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            #random_features = return_randam_data(batch_size, list_to_make_randomData, Standardization_mean=False, Standardization_std=False).to(\"cpu\")\n",
    "\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            random_features = return_0or1_data(batch_size=batch_size, metadatas_size=features_size, list_samedatas_tuple=list_samedatas_tuple_plot_20).to(device)\n",
    "            #tau2,age変更\n",
    "            #random_features[:, 1] = 20 #age\n",
    "            #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "            fake_params_Normalized = netG(latent_input=random_latent, features_input=random_features) #=torch.Size([batch_size, param_size])\n",
    "            #-----------------------------------\n",
    "            #値を戻す\n",
    "            fake_params = fake_params_Normalized*std_params + mean_params\n",
    "            #-----------------------------------\n",
    "            #scatter plot\n",
    "            plt.scatter(fake_params[:, 0].detach().numpy(), fake_params[:, 1].detach().numpy(), s=2, c=\"blue\", zorder=10)\n",
    "\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            random_features = return_0or1_data(batch_size=batch_size, metadatas_size=features_size, list_samedatas_tuple=list_samedatas_tuple_plot_80).to(device)\n",
    "            #tau2,age変更\n",
    "            #random_features[:, 1] = 80 #age\n",
    "            #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "            fake_params_Normalized = netG(latent_input=random_latent, features_input=random_features) #=torch.Size([batch_size, param_size])\n",
    "            #-----------------------------------\n",
    "            #値を戻す\n",
    "            fake_params = fake_params_Normalized*std_params + mean_params\n",
    "            #-----------------------------------\n",
    "            #scatter plot\n",
    "            plt.scatter(fake_params[:, 0].detach().numpy(), fake_params[:, 1].detach().numpy(), s=2, c=\"maroon\", zorder=11)\n",
    "            \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"H2\")\n",
    "        plt.ylabel(\"m\")\n",
    "        plt.show()\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #今エポックの、生成データをplot\n",
    "    if epoch%plot_fake_datas_interval == 0:\n",
    "        #matplotlibのfig作成\n",
    "        fig = plt.figure()\n",
    "        #time作成\n",
    "        t = np.arange(0, timeSeries_size, 1)\n",
    "        #生成時系列データをplot\n",
    "        for j in range(math.ceil(plot_fake_datas_n/batch_size)):\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            #fake data\n",
    "            random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            #random_features = return_randam_data(batch_size, list_to_make_randomData, Standardization_mean=False, Standardization_std=False).to(\"cpu\")\n",
    "\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            random_features = return_0or1_data(batch_size=batch_size, metadatas_size=features_size, list_samedatas_tuple=list_samedatas_tuple_plot_20).to(device)\n",
    "            #feature変更\n",
    "            #random_features[:, 1] = 20 #age\n",
    "            #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "            fake_params_Normalized = netG(latent_input=random_latent, features_input=random_features) #=torch.Size([batch_size, param_size])\n",
    "            #-----------------------------------\n",
    "            #値を戻す\n",
    "            fake_params = fake_params_Normalized*std_params + mean_params\n",
    "            #h2,m,tau2にする\n",
    "            random_features[:, 1] = 30 #tau2として利用\n",
    "            fake_params_to_use_ndarray = torch.cat((fake_params,random_features[:,1:2]),1).detach().numpy()\n",
    "            #timeSeries化\n",
    "            fake_timeSeries_ndarray = return_timeSeries_of_m_i_loss(params=fake_params_to_use_ndarray, batch_size=batch_size, timeSeries_size=timeSeries_size)\n",
    "            #-----------------------------------\n",
    "            #時系列データ部分をplot\n",
    "            for b in range(batch_size):\n",
    "                plt.plot(t, fake_timeSeries_ndarray[b], c='blue', linewidth=0.2)\n",
    "            #figの見た目設定\n",
    "            plt.xlabel('day') #x軸ラベル\n",
    "            plt.ylabel('antibody titer') #y軸ラベル\n",
    "            plt.title('fake data (z='+str(latent_size)+', %d epochs)' % epoch) #タイトル\n",
    "            plt.xticks(np.arange(0, 365, 30))\n",
    "            #pauseする\n",
    "            plt.pause(.01)\n",
    "            #図を消す\n",
    "            plt.close()\n",
    "            \n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            random_features = return_0or1_data(batch_size=batch_size, metadatas_size=features_size, list_samedatas_tuple=list_samedatas_tuple_plot_80).to(device)\n",
    "            #feature変更\n",
    "            #random_features[:, 1] = 80 #age\n",
    "            #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "            fake_params_Normalized = netG(latent_input=random_latent, features_input=random_features) #=torch.Size([batch_size, param_size])\n",
    "            #-----------------------------------\n",
    "            #値を戻す\n",
    "            fake_params = fake_params_Normalized*std_params + mean_params\n",
    "            #h2,m,tau2にする\n",
    "            random_features[:, 1] = 30 #tau2として利用\n",
    "            fake_params_to_use_ndarray = torch.cat((fake_params,random_features[:,1:2]),1).detach().numpy()\n",
    "            #timeSeries化\n",
    "            fake_timeSeries_ndarray = return_timeSeries_of_m_i_loss(params=fake_params_to_use_ndarray, batch_size=batch_size, timeSeries_size=timeSeries_size)\n",
    "            #-----------------------------------\n",
    "            #時系列データ部分をplot\n",
    "            for b in range(batch_size):\n",
    "                plt.plot(t, fake_timeSeries_ndarray[b], c='maroon', linewidth=0.2)\n",
    "            #figの見た目設定\n",
    "            plt.xlabel('day') #x軸ラベル\n",
    "            plt.ylabel('antibody titer') #y軸ラベル\n",
    "            plt.title('fake data (z='+str(latent_size)+', %d epochs)' % epoch) #タイトル\n",
    "            plt.xticks(np.arange(0, 365, 30))\n",
    "            #pauseする\n",
    "            plt.pause(.01)\n",
    "            #図を消す\n",
    "            plt.close()\n",
    "            \n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #今エポックまでの、loss履歴をplot\n",
    "    if epoch%plot_losses_interval == 0:\n",
    "        #matplotlibのfig作成\n",
    "        fig2 = plt.figure()\n",
    "        #realのloss履歴をプロット\n",
    "        plt.plot(range(len(losses_real_validity)), losses_real_validity, label='real_loss')\n",
    "        #fakeのloss履歴をプロット\n",
    "        plt.plot(range(len(losses_fake_validity)), losses_fake_validity, label='fake_loss')\n",
    "        #figの見た目設定\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0,1)\n",
    "        plt.yticks(np.arange(0, 1, 0.1))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        #表示\n",
    "        plt.show()\n",
    "        #pauseする\n",
    "        plt.pause(.01)\n",
    "        #図を消す\n",
    "        plt.close()\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    #今エポックの、modelをsave\n",
    "    if epoch%save_models == 0:\n",
    "        #make directory\n",
    "        dir_path = \"/Users/tatematsudaiki/Desktop/VPGAM_models\"\n",
    "        os.makedirs(dir_path,exist_ok=True)\n",
    "        #save models\n",
    "        torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(dir_path,\"netG\",epoch))\n",
    "        torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(dir_path,\"netD\",epoch))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#別々version & fp一回のみver\n",
    "############################################################################\n",
    "#[学習]\n",
    "losses_real_validity = [] #「全エポックのloss履歴」(real)\n",
    "losses_fake_validity = [] #「全エポックのloss履歴」(fake)\n",
    "losses_m_i_loss = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1): #エポック数分だけ実行\n",
    "    loss_real_validity_sum = 0.0 #「今エポックの合計loss」(real)\n",
    "    loss_fake_validity_sum = 0.0 #「今エポックの合計loss」(fake)\n",
    "    loss_m_i_loss_sum = 0.0 #「今エポックの合計loss」(m-i-loss)\n",
    "    \n",
    "    iter_count = 1\n",
    "    for real_timeSeries,real_params,real_features,real_mask in dataloader:\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #[real data]\n",
    "        real_timeSeries = real_timeSeries.to(device)\n",
    "        real_params = real_params.to(device)\n",
    "        real_features = real_features.to(device)\n",
    "        real_mask = lose_value(real_mask, 3).to(device)\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #[fake data]\n",
    "        random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "        random_features = torch.randn((batch_size,features_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "        #random_features = return_randam_data(batch_size, list_to_make_randamData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "        random_mask = torch.randint(0, 2, (batch_size,timeSeries_size), dtype=torch.float32).to(device)  #値が0or1のtorch.Size([batch_size, timeSeries_size])を作成\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #real dataのfp \n",
    "        real_validity = netD(timeSeries_input=real_timeSeries, params_input=real_params, features_input=real_features, mask_input=real_mask) #=torch.Size([batch_size, 1])\n",
    "\n",
    "        #fake dataのfp\n",
    "        fake_params, fake_timeSeries = netG(latent_input=random_latent, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "        fake_validity = netD(timeSeries_input=fake_timeSeries, params_input=fake_params, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, 1])\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「識別器の学習」\n",
    "        #最適化対象のすべてのパラメータの勾配を0にする\n",
    "        optimizer_netD.zero_grad()\n",
    "        \n",
    "        #loss\n",
    "        loss_netD = - torch.mean(real_validity) + torch.mean(fake_validity)\n",
    "        real_validity_netD = torch.sigmoid(torch.mean(real_validity)).item()\n",
    "        fake_validity_netD = torch.sigmoid(torch.mean(fake_validity)).item()\n",
    "        \n",
    "        #bp(=各パラメータの偏微分を計算)\n",
    "        loss_netD.backward(retain_graph=True)\n",
    "        #最適化\n",
    "        optimizer_netD.step()\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「生成器の学習」\n",
    "        #識別器の学習の何回おきに生成器を学習させるか(イテレーション数で指定)\n",
    "        for n in range(netG_learn_n_start):\n",
    "        #for n in range(netG_learn_n_start + int((netG_learn_n_end-netG_learn_n_start)*epoch/EPOCHS)):\n",
    "            #最適化対象のすべてのパラメータの勾配を0にする\n",
    "            optimizer_netG.zero_grad()\n",
    "            \n",
    "            #lossを計算(「本物は大きく, 偽物は小さく なってほしい」の逆)(=fake_validityは大きくなってほしい)\n",
    "            loss_netG = -torch.mean(fake_validity) #これが小さくなってほしい #=torch.Size([1])\n",
    "            fake_validity_netG = torch.sigmoid(-loss_netG).item()\n",
    "\n",
    "            #bp(=偏微分を計算)\n",
    "            loss_netG.backward(retain_graph=True)\n",
    "            #最適化\n",
    "            optimizer_netG.step()\n",
    "\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            #「パラメータに合うように、時系列を学習」\n",
    "            #100エポック以上になったら「生成したparamsから生成した時系列データ」と「ganから生成した時系列データ」とのlossをloss_netGに追加\n",
    "            if epoch >= m_i_GAN_start: #100エポック以上になったら\n",
    "                #最適化対象のすべてのパラメータの勾配を0にする\n",
    "                optimizer_netG.zero_grad()\n",
    "\n",
    "                #取り出す & 値を戻す\n",
    "                params_detached = fake_params.detach()*std_params+mean_params #値は貰って共有、計算グラフは切る #& ndarry化\n",
    "                features_detached = random_features.detach()*std_features+mean_features #値は貰って共有、計算グラフは切る #& ndarry化\n",
    "\n",
    "                #m-i-lossに利用するparams\n",
    "                params_to_use_detached = torch.cat((params_detached,features_detached[:,0:1]),1).to(\"cpu\").numpy()\n",
    "                #m_i_lossの影響度\n",
    "                #degreeOfInfluence = 1/1\n",
    "                #degreeOfInfluence = torch.abs(fake_validity_detached)/10  #多分変\n",
    "\n",
    "                #m_i_lossを計算 & 値を戻す\n",
    "                timeSeries_of_m_i_loss = ((return_timeSeries_of_m_i_loss(params=params_to_use_detached, batch_size=batch_size, timeSeries_size=timeSeries_size).to(device) - mean_timeSeries) / std_timeSeries) #=torch.Size([batch_size,timeSeries_size])\n",
    "\n",
    "                #loss\n",
    "                m_i_loss = torch.mean(torch.mean(torch.square(fake_timeSeries - timeSeries_of_m_i_loss),-1, True)) #2乗して平均 #=torch.Size([1])\n",
    "                fake_validity_m_i_loss = torch.sigmoid(m_i_loss).item()\n",
    "\n",
    "                #bp(=偏微分を計算)\n",
    "                m_i_loss.backward(retain_graph=True)\n",
    "                #最適化\n",
    "                optimizer_netG.step()\n",
    "            else:\n",
    "                m_i_loss = torch.tensor(np.nan)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        # 誤差逆伝播を実行後、計算グラフを削除\n",
    "        #del loss \n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #print\n",
    "        if epoch < m_i_GAN_start:\n",
    "            print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:None\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\n",
    "            #print('[Epoch %d/%d] [loss_netD: %f] [loss_netG: %f]'% (epoch, EPOCHS, loss_netD.item(), loss_netG.item()))\n",
    "        elif m_i_GAN_start <= epoch:\n",
    "            print(\"iter:{6} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:{5}\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, fake_validity_m_i_loss, iter_count))\n",
    "        #print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:None\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #sum\n",
    "        loss_real_validity_sum += real_validity_netD\n",
    "        loss_fake_validity_sum += fake_validity_netD\n",
    "        loss_m_i_loss_sum += fake_validity_m_i_loss\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #イテレーション数を+1\n",
    "        iter_count += 1\n",
    "        #print(iter_count)\n",
    "\n",
    "    ############################################################################\n",
    "    #「今エポックの最後のイテレーションのloss」を「loss履歴」に追加\n",
    "    #losses_real_validity.append(real_validity_netD) \n",
    "    #losses_fake_validity.append(fake_validity_netD)\n",
    "    #losses_m_i_loss.append(round(m_i_loss.item(),30))\n",
    "\n",
    "    losses_real_validity.append(loss_real_validity_sum/(iter_count-1)) \n",
    "    losses_fake_validity.append(loss_fake_validity_sum/(iter_count-1))\n",
    "    losses_m_i_loss.append(loss_m_i_loss_sum/(iter_count-1))\n",
    "    \n",
    "    ############################################################################\n",
    "    #[評価]\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #指定エポック毎に、「エポック数」,「識別機のloss」,「生成器のloss」を表示\n",
    "    if epoch%print_loss_interval == 0:\n",
    "        if epoch < m_i_GAN_start:\n",
    "            print(\"epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} m-i-loss:None\".format(epoch, EPOCHS, losses_real_validity[-1], losses_fake_validity[-1]))\n",
    "            #print('[Epoch %d/%d] [loss_netD: %f] [loss_netG: %f]'% (epoch, EPOCHS, loss_netD.item(), loss_netG.item()))\n",
    "        elif m_i_GAN_start <= epoch:\n",
    "            print(\"epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} m-i-loss:{4}\".format(epoch, EPOCHS, losses_real_validity[-1], losses_fake_validity[-1], losses_m_i_loss[-1]))\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #指定エポック毎に、生成データをplot\n",
    "    if epoch%plot_fake_datas_interval == 0:\n",
    "        #matplotlibのfig作成\n",
    "        fig = plt.figure()\n",
    "        #time作成\n",
    "        t = np.arange(0, timeSeries_size, 1)\n",
    "        #生成時系列データをplot\n",
    "        for j in range(math.ceil(plot_fake_datas_n/batch_size)):\n",
    "            #-----------------------------------\n",
    "            #[random data]生成器入力用データ\n",
    "            random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            random_features = return_randam_data(batch_size, list_to_make_randamData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "            random_mask = torch.randint(0, 2, (batch_size,timeSeries_size), dtype=torch.float32).to(device)  #値が0or1のtorch.Size([batch_size, timeSeries_size])を作成\n",
    "            #値を変換\n",
    "            random_features = random_features*std_features+mean_features\n",
    "            #tauのみ変更\n",
    "            random_features[:, 0] = 100\n",
    "            #-----------------------------------\n",
    "            #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "            fake_params, fake_timeSeries = netG(latent_input=random_latent, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "            #値を戻す\n",
    "            fake_timeSeries = fake_timeSeries*std_timeSeries+mean_timeSeries\n",
    "            #-----------------------------------\n",
    "            #matplotlibで使うためにndarray化\n",
    "            fake_timeSeries_numpy = fake_timeSeries.detach().to(\"cpu\").numpy().copy() #detach() & ndarray化\n",
    "            #時系列データ部分をplot\n",
    "            for b in range(batch_size):\n",
    "                plt.plot(t, fake_timeSeries_numpy[b], c='maroon', linewidth=0.2)\n",
    "        #figの見た目設定\n",
    "        plt.xlabel('day') #x軸ラベル\n",
    "        plt.ylabel('antibody titer') #y軸ラベル\n",
    "        plt.title('fake data (z='+str(latent_size)+', %d epochs)' % epoch) #タイトル\n",
    "        plt.xticks(np.arange(0, 365, 30))\n",
    "        #pauseする\n",
    "        plt.pause(.01)\n",
    "        #図を消す\n",
    "        plt.close()\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #指定エポック毎に、loss履歴をplot\n",
    "    if epoch%plot_losses_interval == 0:\n",
    "        #matplotlibのfig作成\n",
    "        fig2 = plt.figure()\n",
    "        #realのloss履歴をプロット\n",
    "        plt.plot(range(len(losses_real_validity)), losses_real_validity, label='real_loss')\n",
    "        #fakeのloss履歴をプロット\n",
    "        plt.plot(range(len(losses_fake_validity)), losses_fake_validity, label='fake_loss')\n",
    "        #figの見た目設定\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0,1)\n",
    "        plt.yticks(np.arange(0, 1, 0.1))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        #表示\n",
    "        plt.show()\n",
    "        #pauseする\n",
    "        plt.pause(.01)\n",
    "        #図を消す\n",
    "        plt.close()\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    #modelをsave\n",
    "    if epoch%save_models == 0:\n",
    "        #make directory\n",
    "        dir_path = \"/Users/tatematsudaiki/Desktop/VPGAM_models\"\n",
    "        os.makedirs(dir_path,exist_ok=True)\n",
    "        #save models\n",
    "        torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(dir_path,\"netG\",epoch))\n",
    "        torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(dir_path,\"netD\",epoch))\n",
    "    \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb26cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"#ノーマルversion\n",
    "############################################################################\n",
    "#[学習]\n",
    "losses_real_validity = [] #「全エポックのloss履歴」(real)\n",
    "losses_fake_validity = [] #「全エポックのloss履歴」(fake)\n",
    "losses_m_i_loss = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1): #エポック数分だけ実行\n",
    "    loss_real_validity_sum = 0.0 #「今エポックの合計loss」(real)\n",
    "    loss_fake_validity_sum = 0.0 #「今エポックの合計loss」(fake)\n",
    "    loss_m_i_loss_sum = 0.0 #「今エポックの合計loss」(m-i-loss)\n",
    "    \n",
    "    iter_count = 1\n",
    "    for real_timeSeries,real_params,real_features,real_mask in dataloader:\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #[real data]\n",
    "        #real_timeSeries = real_timeSeries/1000\n",
    "        real_timeSeries = real_timeSeries.to(device)\n",
    "        real_params = real_params.to(device)\n",
    "        real_features = real_features.to(device)\n",
    "        real_mask = lose_value(real_mask, 3).to(device)\n",
    "        #print(real_timeSeries)\n",
    "        #print(real_params_and_features)\n",
    "        #print(real_mask)\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「識別器の学習」\n",
    "        #[random data]生成器入力用データ\n",
    "        random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "        random_features = torch.randn((batch_size,features_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "        #random_features = return_randam_data(batch_size, list_to_make_randamData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "        random_mask = torch.randint(0, 2, (batch_size,timeSeries_size), dtype=torch.float32).to(device)  #値が0or1のtorch.Size([batch_size, timeSeries_size])を作成\n",
    "        #print(random_latent)\n",
    "        #print(random_features)\n",
    "        #print(random_mask)\n",
    "\n",
    "        #値を変換\n",
    "        #random_features = random_features*std_features+mean_features\n",
    "        \n",
    "        #最適化対象のすべてのパラメータの勾配を0にする\n",
    "        optimizer_netD.zero_grad() \n",
    "        \n",
    "        #\"本物データ\"を識別器でfp \n",
    "        real_validity = netD(timeSeries_input=real_timeSeries, params_input=real_params, features_input=real_features, mask_input=real_mask) #=torch.Size([batch_size, 1])\n",
    "        \n",
    "        #\"偽データ\"を識別器でfp\n",
    "        fake_params, fake_timeSeries = netG(latent_input=random_latent, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "        fake_validity = netD(timeSeries_input=fake_timeSeries, params_input=fake_params, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, 1])\n",
    "        \n",
    "        #まとめてlossを計算(本物は大きく, 偽物は小さく なってほしい)\n",
    "        loss_netD = 1 - torch.mean(real_validity) + torch.mean(fake_validity) - 0 #これが小さくなってほしい\n",
    "        #loss_netD = F.relu(1.0 - real_validity).mean() + F.relu(1.0 + fake_validity).mean() #Hinge Loss 多分ダメダメ\n",
    "        #loss_netD = torch.mean(F.relu(0.2 - (real_validity - fake_validity))) #Hinge Loss\n",
    "        real_validity_netD = round(torch.mean(real_validity).item(),30)\n",
    "        fake_validity_netD = round(torch.mean(fake_validity).item(),30)\n",
    "        #real_validity_netD = round(torch.mean(torch.sigmoid(real_validity)).item(),30)\n",
    "        #fake_validity_netD = round(torch.mean(torch.sigmoid(fake_validity)).item(),30)\n",
    "        \n",
    "        #bp(=各パラメータの偏微分を計算)\n",
    "        loss_netD.backward()\n",
    "        #最適化\n",
    "        optimizer_netD.step()\n",
    "        \n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #「生成器の学習」\n",
    "        \n",
    "        #識別器の学習の何回おきに生成器を学習させるか(イテレーション数で指定)\n",
    "        for n in range(netG_learn_n_start):\n",
    "        #for n in range(netG_learn_n_start + int((netG_learn_n_end-netG_learn_n_start)*epoch/EPOCHS)):\n",
    "            #[random data]生成器入力用データ\n",
    "            random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            random_features = torch.randn((batch_size,features_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            #random_features = return_randam_data(batch_size, list_to_make_randamData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "            random_mask = torch.randint(0, 2, (batch_size,timeSeries_size), dtype=torch.float32).to(device)  #値が0or1のtorch.Size([batch_size, timeSeries_size])を作成\n",
    "\n",
    "            #値を変換\n",
    "            #random_features = random_features*std_features+mean_features\n",
    "\n",
    "            #最適化対象のすべてのパラメータの勾配を0にする\n",
    "            optimizer_netG.zero_grad() \n",
    "\n",
    "            #\"偽データ\"を識別器でfp\n",
    "            fake_params, fake_timeSeries = netG(latent_input=random_latent, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "            fake_validity = netD(timeSeries_input=fake_timeSeries, params_input=fake_params, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, 1])\n",
    "            \n",
    "            #lossを計算(「本物は大きく, 偽物は小さく なってほしい」の逆)(=fake_validityは大きくなってほしい)\n",
    "            loss_netG = -torch.mean(fake_validity) #これが小さくなってほしい #=torch.Size([1])\n",
    "            fake_validity_netG = round(torch.mean(fake_validity).item(),30)\n",
    "            #fake_validity_netG = round(torch.mean(torch.sigmoid(fake_validity)).item(),30)\n",
    "\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "            #「パラメータに合うように、時系列を学習」\n",
    "            #100エポック以上になったら「生成したparamsから生成した時系列データ」と「ganから生成した時系列データ」とのlossをloss_netGに追加\n",
    "            if epoch >= m_i_GAN_start: #100エポック以上になったら\n",
    "                #取り出す\n",
    "                #fake_params_detached = fake_params.detach().to(\"cpu\").numpy() #値は貰って共有、計算グラフは切る #& ndarry化\n",
    "                fake_params_detached = fake_params.detach() #値は貰って共有、計算グラフは切る #& ndarry化\n",
    "                random_features_detached = random_features.detach() #値は貰って共有、計算グラフは切る\n",
    "                fake_timeSeries_detached = fake_timeSeries.detach() #値は貰って共有、計算グラフは切る\n",
    "                #fake_validity_detached = fake_validity.detach() #値は貰って共有、計算グラフは切る\n",
    "                #値を戻す\n",
    "                fake_params_detached = fake_params_detached*std_params+mean_params\n",
    "                random_features_detached = random_features_detached*std_features+mean_features\n",
    "                fake_timeSeries_detached = fake_timeSeries_detached*std_timeSeries+mean_timeSeries\n",
    "                #m-i-lossに利用するparams\n",
    "                params_to_use = torch.cat((fake_params_detached,random_features_detached[:,0:1]),1).to(\"cpu\").numpy()\n",
    "                #m_i_lossの影響度\n",
    "                degreeOfInfluence = 1/1\n",
    "                #degreeOfInfluence = torch.abs(fake_validity_detached)/10  #多分変\n",
    "                #m_i_lossを計算\n",
    "                m_i_loss_raw = func_m_i_loss(params_to_use, fake_timeSeries_detached)\n",
    "                #m_i_loss = torch.mean(m_i_loss_raw) #=torch.Size([batch_size,1])  #fake_validity_detached大ならfunc_m_i_loss小,小なら大\n",
    "                m_i_loss = torch.mean(degreeOfInfluence*torch.sigmoid(m_i_loss_raw)) #=torch.Size([batch_size,1])  #fake_validity_detached大ならfunc_m_i_loss小,小なら大\n",
    "                #print(m_i_loss_raw)\n",
    "                #print(m_i_loss)\n",
    "                #lossを追加\n",
    "                loss_netG += m_i_loss #追加\n",
    "                #lossをprint\n",
    "                #print('m_i_loss = ' + str(m_i_loss))\n",
    "            else:\n",
    "                m_i_loss = torch.tensor(np.nan)\n",
    "            \n",
    "            #bp(=偏微分を計算)\n",
    "            loss_netG.backward()\n",
    "            #最適化\n",
    "            optimizer_netG.step()\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #print\n",
    "        if epoch < m_i_GAN_start:\n",
    "            print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:None\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\n",
    "            #print('[Epoch %d/%d] [loss_netD: %f] [loss_netG: %f]'% (epoch, EPOCHS, loss_netD.item(), loss_netG.item()))\n",
    "        elif m_i_GAN_start <= epoch:\n",
    "            print(\"iter:{6} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:{5}\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, round(m_i_loss.item(),30), iter_count))\n",
    "            #print(\"iter:{5} epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:None\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, iter_count))\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #sum\n",
    "        loss_real_validity_sum += real_validity_netD\n",
    "        loss_fake_validity_sum += fake_validity_netD\n",
    "        loss_m_i_loss_sum += m_i_loss\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------\n",
    "        #イテレーション数を+1\n",
    "        iter_count += 1\n",
    "        #print(iter_count)\n",
    "\n",
    "    ############################################################################\n",
    "    #「今エポックの最後のイテレーションのloss」を「loss履歴」に追加\n",
    "    #losses_real_validity.append(real_validity_netD) \n",
    "    #losses_fake_validity.append(fake_validity_netD)\n",
    "    #losses_m_i_loss.append(round(m_i_loss.item(),30))\n",
    "\n",
    "    losses_real_validity.append(loss_real_validity_sum/(iter_count-1)) \n",
    "    losses_fake_validity.append(loss_fake_validity_sum/(iter_count-1))\n",
    "    losses_m_i_loss.append(loss_m_i_loss_sum/(iter_count-1))\n",
    "    \n",
    "    ############################################################################\n",
    "    #[評価]\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #指定エポック毎に、「エポック数」,「識別機のloss」,「生成器のloss」を表示\n",
    "    if epoch%print_loss_interval == 0:\n",
    "        if epoch < m_i_GAN_start:\n",
    "            print(\"epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:None\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG))\n",
    "            #print('[Epoch %d/%d] [loss_netD: %f] [loss_netG: %f]'% (epoch, EPOCHS, loss_netD.item(), loss_netG.item()))\n",
    "        elif m_i_GAN_start <= epoch:\n",
    "            print(\"epoch:{0}/{1} loss_D_real:{2} loss_D_fake:{3} loss_G_fake:{4} m-i-loss:{5}\".format(epoch, EPOCHS, real_validity_netD, fake_validity_netD, fake_validity_netG, round(m_i_loss.item(),30)))\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #指定エポック毎に、生成データをplot\n",
    "    if epoch%plot_fake_datas_interval == 0:\n",
    "        #matplotlibのfig作成\n",
    "        fig = plt.figure()\n",
    "        #time作成\n",
    "        t = np.arange(0, timeSeries_size, 1)\n",
    "        #生成時系列データをplot\n",
    "        for j in range(math.ceil(plot_fake_datas_n/batch_size)):\n",
    "            #-----------------------------------\n",
    "            #[random data]生成器入力用データ\n",
    "            random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "            random_features = return_randam_data(batch_size, list_to_make_randamData, Standardization_mean=False, Standardization_std=False).to(device)\n",
    "            random_mask = torch.randint(0, 2, (batch_size,timeSeries_size), dtype=torch.float32).to(device)  #値が0or1のtorch.Size([batch_size, timeSeries_size])を作成\n",
    "            #値を変換\n",
    "            random_features = random_features*std_features+mean_features\n",
    "            #tauのみ変更\n",
    "            random_features[:, 0] = 100\n",
    "            #-----------------------------------\n",
    "            #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "            fake_params, fake_timeSeries = netG(latent_input=random_latent, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "            #値を戻す\n",
    "            fake_timeSeries = fake_timeSeries*std_timeSeries+mean_timeSeries\n",
    "            #-----------------------------------\n",
    "            #matplotlibで使うためにndarray化\n",
    "            fake_timeSeries_numpy = fake_timeSeries.detach().to(\"cpu\").numpy().copy() #detach() & ndarray化\n",
    "            #時系列データ部分をplot\n",
    "            for b in range(batch_size):\n",
    "                plt.plot(t, fake_timeSeries_numpy[b], c='maroon', linewidth=0.2)\n",
    "        #figの見た目設定\n",
    "        plt.xlabel('day') #x軸ラベル\n",
    "        plt.ylabel('antibody titer') #y軸ラベル\n",
    "        plt.title('fake data (z='+str(latent_size)+', %d epochs)' % epoch) #タイトル\n",
    "        plt.xticks(np.arange(0, 365, 30))\n",
    "        #pauseする\n",
    "        plt.pause(.01)\n",
    "        #図を消す\n",
    "        plt.close()\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    #指定エポック毎に、loss履歴をplot\n",
    "    if epoch%plot_losses_interval == 0:\n",
    "        #matplotlibのfig作成\n",
    "        fig2 = plt.figure()\n",
    "        #realのloss履歴をプロット\n",
    "        plt.plot(range(len(losses_real_validity)), losses_real_validity, label='real_loss')\n",
    "        #fakeのloss履歴をプロット\n",
    "        plt.plot(range(len(losses_fake_validity)), losses_fake_validity, label='fake_loss')\n",
    "        #figの見た目設定\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0,1)\n",
    "        #plt.yticks(np.arange(0, 1, 0.1))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        #表示\n",
    "        plt.show()\n",
    "        #pauseする\n",
    "        plt.pause(.01)\n",
    "        #図を消す\n",
    "        plt.close()\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    #modelをsave\n",
    "    if epoch%save_models == 0:\n",
    "        #make directory\n",
    "        dir_path = \"/Users/tatematsudaiki/Desktop/VPGAM_models\"\n",
    "        os.makedirs(dir_path,exist_ok=True)\n",
    "        #save models\n",
    "        torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(dir_path,\"netG\",epoch))\n",
    "        torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(dir_path,\"netD\",epoch))\n",
    "    \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23fcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa2607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee64040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#epoch\n",
    "epoch_write = 26\n",
    "lr_write = \"001\"\n",
    "\n",
    "#save model\n",
    "model_path = \"/Users/tatematsudaiki/Desktop\"\n",
    "torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(model_path,\"netG\",epoch_write,lr_write))\n",
    "torch.save(netG.to('cpu').state_dict(), \"{0}/{1}_epoch{2}.pth\".format(model_path,\"netD\",epoch_write,lr_write))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803659b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#load model\n",
    "model_path = \"/Users/tatematsudaiki/Desktop\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#[評価]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#matplotlibのfig作成\n",
    "fig = plt.figure()\n",
    "#time作成\n",
    "t = np.arange(0, timeSeries_size, 1)\n",
    "#生成時系列データをplot\n",
    "for j in range(math.ceil(plot_fake_datas_n/batch_size)):\n",
    "    #[random data]生成器入力用データ\n",
    "    random_latent = torch.randn((batch_size,latent_size), dtype=torch.float32).to(device) #=torch.Size([batch_size, latent_size])\n",
    "    random_features = return_randam_data(batch_size, list_to_make_randamData)\n",
    "    random_features[:, 0] = 30\n",
    "    random_mask = torch.randint(0, 2, (batch_size,timeSeries_size), dtype=torch.float32).to(device)  #値が0or1のtorch.Size([batch_size, timeSeries_size])を作成\n",
    "    \n",
    "    #\"生成器入力用ランダムデータ\"を生成器でfp\n",
    "    fake_params, fake_timeSeries = netG(latent_input=random_latent, features_input=random_features, mask_input=random_mask) #=torch.Size([batch_size, timeSeries_size]), torch.Size([batch_size, param_size])\n",
    "    fake_timeSeries_numpy = fake_timeSeries.detach().to(\"cpu\").numpy().copy() #detach() & ndarray化\n",
    "\n",
    "    #時系列データ部分をプロット\n",
    "    for b in range(batch_size):\n",
    "        plt.plot(t, fake_timeSeries_numpy[b], c='maroon', linewidth=0.2)\n",
    "        \n",
    "#figの見た目設定\n",
    "plt.xlabel('time') #x軸ラベル\n",
    "plt.ylabel('viralload') #y軸ラベル\n",
    "plt.title('fake data (z='+str(latent_size)+', %d epochs)' % epoch) #タイトル\n",
    "plt.xticks(np.arange(0, 365, 30))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#loss履歴を表示\n",
    "\n",
    "#matplotlibのfig作成\n",
    "fig2 = plt.figure()\n",
    "#Discriminator(識別器)のloss履歴をプロット\n",
    "plt.plot(range(len(losses_netD)), losses_netD, label='Discriminator_Loss')\n",
    "#Generator(生成器)のloss履歴をプロット\n",
    "plt.plot(range(len(losses_netG)), losses_netG, label='Generator_Loss')\n",
    "#figの見た目設定\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#保存\n",
    "#fig2.savefig('results(dropped_data)/loss.png')\n",
    "#表示\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391dec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49348597",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ID = 2000\n",
    "\n",
    "#read\n",
    "df_params = pd.read_csv(\"../params_vaccination2_withoutgroup0.csv\",sep=',')\\\n",
    "    .query('id==@ID')\n",
    "\n",
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html\n",
    "from scipy.integrate import solve_ivp\n",
    "#paramsを取り出す(V0,β,γ,δ)\n",
    "M10_M20_A0 = [100,100,0] #M10(=D1),M20(=D2)はテキトーに決められている\n",
    "H2 = df_params[\"H2\"].item()\n",
    "m = df_params[\"m\"].item()\n",
    "tau1 = df_params[\"tau1\"].item()\n",
    "tau2 = df_params[\"tau2\"].item()\n",
    "print(H2)\n",
    "print(m)\n",
    "print(tau1)\n",
    "print(tau2)\n",
    "#constants (estimated by 12 health care workers)\n",
    "d = 0.693\n",
    "mu = 0.885346\n",
    "H1 = 975.297\n",
    "eta1 = 12.52\n",
    "K = 33900\n",
    "eta2 = 4.25\n",
    "\n",
    "init   = M10_M20_A0\n",
    "t_span = [0.0,365.0]\n",
    "t_eval = np.linspace(*t_span,1000) # time for sampling\n",
    "\n",
    "#常微分方程式を定義 / vaccination2 ver\n",
    "def func(t,M1_M2_A, H2, m, tau1, tau2, d, mu, H1, eta1, K, eta2):\n",
    "    #print(t)\n",
    "    #extract\n",
    "    M1,M2,A = M1_M2_A\n",
    "    #formulas\n",
    "    dM1dt = -d*M1 if tau1<=t else 0\n",
    "    dM2dt = -d*M2 if tau2<=t else 0\n",
    "    if 0<=t<tau1+eta1:\n",
    "        dAdt = 0\n",
    "    elif tau1+eta1<=t<tau2+eta2:\n",
    "        dAdt = H1*(M1**m/(K**m+M1**m)) - mu*A\n",
    "    elif tau2+eta2<=t:\n",
    "        dAdt = H2*((M1+M2)**m/(K**m+(M1+M2)**m)) - mu*A\n",
    "\n",
    "    #return\n",
    "    return [dM1dt, dM2dt, dAdt]\n",
    "\n",
    "sol = solve_ivp(fun=func,\n",
    "                t_span=t_span,\n",
    "                y0=init,\n",
    "                method='RK23',\n",
    "                t_eval=t_eval,\n",
    "                args=(H2, m, tau1, tau2, d, mu, H1, eta1, K, eta2))\n",
    "\n",
    "print(sol.t.size)\n",
    "#print(sol.t)\n",
    "print(sol.y.shape)\n",
    "#print(sol.y)\n",
    "plt.plot(sol.t,sol.y[2])\n",
    "\n",
    "\n",
    "#plot\n",
    "###################################################\n",
    "#read\n",
    "df_raw = pd.read_csv(\"../timeSeries_raw_vaccination2_withoutgroup0.csv\",sep=',',encoding=\"shift-jis\", index_col=0, header=0)\n",
    "df_fitted = pd.read_csv(\"../timeSeries_fitted_vaccination2_withoutgroup0.csv\",sep=',',encoding=\"shift-jis\", index_col=0, header=0)\n",
    "#----------------------------------------------------------------\n",
    "#df_raw\n",
    "#----------------------------------------------------------------\n",
    "#times\n",
    "times = df_raw.columns.values\n",
    "#extract\n",
    "values = df_raw.loc[ID].values\n",
    "#plot\n",
    "plt.plot(times, values, marker=\"o\", color=\"red\", markersize=5, linewidth=1, zorder=2, linestyle=\"None\")\n",
    "#----------------------------------------------------------------\n",
    "#df_fitted\n",
    "#----------------------------------------------------------------\n",
    "#times\n",
    "times = df_fitted.columns.values\n",
    "#extract\n",
    "values = df_fitted.loc[ID].values\n",
    "#plot\n",
    "plt.plot(times, values, marker=\"o\", color=\"black\", markersize=2, linewidth=1, zorder=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c6dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使わなかった部分(preprocessing.py)\n",
    "\"\"\"def compute_real_MSE(fake_data, real_data, mask_table, index):\n",
    "    mask = mask_table[index]\n",
    "    masked_fake_data = fake_data * mask\n",
    "    return ((real_data - masked_fake_data) ** 2).mean()\n",
    "\n",
    "\n",
    "def calculate_sim_realdata(params):\n",
    "    v0 = [1, params[3]]\n",
    "    beta = params[0]\n",
    "    gamma = params[1]\n",
    "    delta = params[2]\n",
    "    t = np.arange(0, 31, 0.01)\n",
    "    simulated_data = odeint(func, v0, t, args=(beta, gamma, delta))\n",
    "\n",
    "    processed_data = simulated_data[::100]\n",
    "    processed_data = np.log10(processed_data[:, 1])\n",
    "    sim_realdata = np.where(processed_data >= 2, processed_data, 2)\n",
    "    return sim_realdata\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使わなかった部分(grud.py)\n",
    "# GPUじゃないからいらないコード\n",
    "    '''\n",
    "        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):\n",
    "            return\n",
    "\n",
    "        # If any parameters alias, we fall back to the slower, copying code path. This is\n",
    "        # a sufficient check, because overlapping parameter buffers that don't completely\n",
    "        # alias would break the assumptions of the uniqueness check in\n",
    "        # Module.named_parameters().\n",
    "        all_weights = self._flat_weights\n",
    "        unique_data_ptrs = set(p.data_ptr() for p in all_weights)\n",
    "        if len(unique_data_ptrs) != len(all_weights):\n",
    "            return\n",
    "\n",
    "        with torch.cuda.device_of(any_param):\n",
    "            import torch.backends.cudnn.rnn as rnn\n",
    "\n",
    "            # NB: This is a temporary hack while we still don't have Tensor\n",
    "            # bindings for ATen functions\n",
    "            with torch.no_grad():\n",
    "                # NB: this is an INPLACE function on all_weights, that's why the\n",
    "                # no_grad() is necessary.\n",
    "                torch._cudnn_rnn_flatten_weight(\n",
    "                    all_weights, (4 if self.bias else 2),\n",
    "                    self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,\n",
    "                    self.batch_first, bool(self.bidirectional))\n",
    "    '''\n",
    "\n",
    "    \"\"\"def _apply(self, fn):\n",
    "        ret = super(GRUD, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\"\"\"\n",
    "\n",
    "    \"\"\"# 次元数とか合ってるか確認するメソッド\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_size = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_size:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_size, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "\n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\"\"\"\n",
    "\n",
    "    \"\"\"def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\"\"\"\n",
    "\n",
    "    \"\"\"def __setstate__(self, d):\n",
    "        super(GRUD, self).__setstate__(d)\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "\n",
    "        weights = ['weight_dg_x', 'weight_dg_h',\n",
    "                   'weight_xz', 'weight_hz', 'weight_mz',\n",
    "                   'weight_xr', 'weight_hr', 'weight_mr',\n",
    "                   'weight_xh', 'weight_hh', 'weight_mh',\n",
    "                   'weight_hy',\n",
    "                   'bias_dg_x', 'bias_dg_h',\n",
    "                   'bias_z', 'bias_r', 'bias_h', 'bias_y']\n",
    "\n",
    "        if self.bias:\n",
    "            self._all_weights += [weights]\n",
    "        else:\n",
    "            self._all_weights += [weights[:2]]\"\"\"\n",
    "\n",
    "    \"\"\"@property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\"\"\"\n",
    "\n",
    "    \"\"\"@property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deepLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "289d4f4032513d9d51ca9b078f6657df059deb96dfa5bdc2fd7f35e6fcae039b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
