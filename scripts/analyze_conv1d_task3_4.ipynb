{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length, batch_size):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #var\n",
    "        self.in_channels = in_channels\n",
    "        self.in_length = in_length\n",
    "        self.out_channels = out_channels\n",
    "        self.out_length = out_length\n",
    "        self.batch_size = batch_size\n",
    "        #-----------------------------------\n",
    "        #Conv1d\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        #https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "        kernel_size=10; stride=2; padding=1; dilation=1; \n",
    "        #conv1d_0\n",
    "        self.in_channels_conv1d_0 = self.in_channels; self.in_length_conv1d_0 = self.in_length\n",
    "        self.out_channels_conv1d_0= 10; self.out_length_conv1d_0 = int((self.in_length_conv1d_0+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "        #conv1d_1\n",
    "        self.in_channels_conv1d_1 = self.out_channels_conv1d_0; self.in_length_conv1d_1 = self.out_length_conv1d_0\n",
    "        self.out_channels_conv1d_1= 10; self.out_length_conv1d_1= int((self.in_length_conv1d_1+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "        #conv1d_2\n",
    "        self.in_channels_conv1d_2 = self.out_channels_conv1d_1; self.in_length_conv1d_2 = self.out_length_conv1d_1\n",
    "        self.out_channels_conv1d_2= 10; self.out_length_conv1d_2= int((self.in_length_conv1d_2+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_1 = self.out_channels_conv1d_2*self.out_length_conv1d_2\n",
    "        self.n_channels_inner_1 = 30\n",
    "        self.out_channels_inner_1 = self.out_channels*self.out_length\n",
    "\n",
    "        #-----------------------------------\n",
    "        #layer0\n",
    "        self.layer0 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            #conv1d_0\n",
    "            nn.Conv1d(in_channels=self.in_channels_conv1d_0, out_channels=self.out_channels_conv1d_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(num_features=self.out_channels_conv1d_0),\n",
    "            nn.Dropout(p=0.1),\n",
    "            #-----------------------------------\n",
    "            #conv1d_0\n",
    "            nn.Conv1d(in_channels=self.in_channels_conv1d_1, out_channels=self.out_channels_conv1d_1, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(num_features=self.out_channels_conv1d_1),\n",
    "            nn.Dropout(p=0.1),\n",
    "            #-----------------------------------\n",
    "            #conv1d_0\n",
    "            nn.Conv1d(in_channels=self.in_channels_conv1d_2, out_channels=self.out_channels_conv1d_2, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(num_features=self.out_channels_conv1d_2),\n",
    "            nn.Dropout(p=0.1),\n",
    "        )\n",
    "        #-----------------------------------\n",
    "        #layer1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.Linear(self.in_channels_inner_1, self.n_channels_inner_1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            #-----------------------------------\n",
    "            nn.Linear(self.n_channels_inner_1, self.n_channels_inner_1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            #-----------------------------------\n",
    "            nn.Linear(self.n_channels_inner_1, self.n_channels_inner_1),\n",
    "            nn.ReLU(),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            #-----------------------------------\n",
    "            nn.Linear(self.n_channels_inner_1, self.out_channels_inner_1),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Sigmoid(),            \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):    \n",
    "        output0 = self.layer0(x).view(-1, self.in_channels_inner_1)\n",
    "        output1 = self.layer1(output0).view(-1, self.out_channels, self.out_length)\n",
    "        return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class model_1(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length, batch_size):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #var\n",
    "        self.in_channels = in_channels\n",
    "        self.in_length = in_length\n",
    "        self.out_channels = out_channels\n",
    "        self.out_length = out_length\n",
    "        self.batch_size = batch_size\n",
    "        #-----------------------------------\n",
    "        #Conv1d\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        #https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "        self.in_channels_inner_0 = self.in_channels; self.in_length_inner_0 = self.in_length\n",
    "        kernel_size=200; stride=50; padding=1; dilation=1; \n",
    "        self.out_channels_inner_0 = 30; self.out_length_inner_0 = int((self.in_length_inner_0+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.in_channels_inner_0, out_channels=self.out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_1 = self.out_channels_inner_0*self.out_length_inner_0\n",
    "        self.out_channels_inner_1 = self.out_channels*self.out_length\n",
    "\n",
    "        #-----------------------------------\n",
    "        #layer0\n",
    "        self.layer0 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.Conv1d(in_channels=self.in_channels_inner_0, out_channels=self.out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.BatchNorm1d(num_features=self.out_channels_inner_0),\n",
    "        )\n",
    "        #-----------------------------------\n",
    "        #layer1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.Linear(self.in_channels_inner_1, self.out_channels_inner_1),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #for i in range(5):\n",
    "        output0 = self.layer0(x).view(-1, self.in_channels_inner_1)\n",
    "        output1 = self.layer1(output0).view(-1, self.out_channels, self.out_length)\n",
    "        return output1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class model_2(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length, batch_size):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #var\n",
    "        self.in_channels = in_channels\n",
    "        self.in_length = in_length\n",
    "        self.out_channels = out_channels\n",
    "        self.out_length = out_length\n",
    "        self.batch_size = batch_size\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_1 = self.in_channels*self.in_length\n",
    "        self.out_channels_inner_1 = 30\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_2 = self.out_channels_inner_1\n",
    "        self.out_channels_inner_2 = self.out_channels*self.out_length\n",
    "        #-----------------------------------\n",
    "        #layer1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.BatchNorm1d(num_features=self.in_channels_inner_1),\n",
    "            nn.Linear(self.in_channels_inner_1, self.out_channels_inner_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #-----------------------------------\n",
    "        #layer2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.BatchNorm1d(num_features=self.in_channels_inner_2),\n",
    "            nn.Linear(self.in_channels_inner_2, self.out_channels_inner_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_channels_inner_1)\n",
    "        output1 = self.layer1(x)\n",
    "        output2 = self.layer2(output1)\n",
    "        output2 = output2.view(-1, self.out_channels, self.out_length)\n",
    "        return output2\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union\n",
    "\n",
    "#---------------------------------------------\n",
    "# custom dataset\n",
    "#https://discuss.pytorch.org/t/custom-data-loader-for-big-data/129361\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dir_X:str, path_dir_Y:str, n_test:Union[int,float], n_val:Union[int,float], batch_size:int): # n_test -> float:ratio of test, int:number of test\n",
    "        #-----------------\n",
    "        # batch_size\n",
    "        self.batch_size = batch_size\n",
    "        # path_dir_X, path_dir_Y\n",
    "        self.path_dir_X = path_dir_X\n",
    "        self.path_dir_Y = path_dir_Y\n",
    "        # list_file_name_all\n",
    "        self.list_file_name_all = os.listdir(path_dir_X)\n",
    "        # n_data_all\n",
    "        self.n_data_all = len(self.list_file_name_all)\n",
    "        #check\n",
    "        if len(os.listdir(path_dir_X)) != len(os.listdir(path_dir_Y)):\n",
    "            raise ValueError(\"error!!!\")\n",
    "        if len(set(os.listdir(path_dir_X)) - set(os.listdir(path_dir_Y))) != 0:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # suffle\n",
    "        random.shuffle(self.list_file_name_all)\n",
    "        #-----------------\n",
    "        # n_test\n",
    "        if type(n_test)==int:\n",
    "            self.n_test = n_test\n",
    "        elif type(n_test)==float:\n",
    "            self.n_test = int(len(self.list_file_name_all)*n_test)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        # n_val\n",
    "        if type(n_val)==int:\n",
    "            self.n_val = n_val\n",
    "        elif type(n_val)==float:\n",
    "            self.n_val = int(len(self.list_file_name_all)*n_val)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #check\n",
    "        if self.n_data_all <= self.n_test+self.n_val:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # list_file_name_test / _val / _train\n",
    "        self.list_file_name_test = self.list_file_name_all[:self.n_test]\n",
    "        self.list_file_name_val = self.list_file_name_all[self.n_test:self.n_test+self.n_val]\n",
    "        self.list_file_name_train = self.list_file_name_all[self.n_test+self.n_val:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_file_name_train)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        #return\n",
    "        return self.getdata(list_file_name=self.list_file_name_train, index=x)\n",
    "    \n",
    "    def getdata(self, list_file_name, index):\n",
    "        #file_name\n",
    "        file_name = list_file_name[index]\n",
    "        #data_X\n",
    "        path_file_X = \"{0}/{1}\".format(self.path_dir_X, file_name)\n",
    "        data_X = np.load(path_file_X, allow_pickle=True)\n",
    "        #data_X = torch.from_numpy(data_X).to(torch.float32).requires_grad_(True)\n",
    "        #data_Y\n",
    "        path_file_Y = \"{0}/{1}\".format(self.path_dir_Y, file_name)\n",
    "        data_Y = np.load(path_file_Y, allow_pickle=True)\n",
    "        #data_Y = torch.from_numpy(data_Y).to(torch.float32).requires_grad_(True)\n",
    "        #return\n",
    "        return data_X, data_Y\n",
    "    \n",
    "    def return_n_data_all(self):\n",
    "        return self.n_data_all\n",
    "    \n",
    "    def return_n_test(self):\n",
    "        return self.n_test\n",
    "    \n",
    "    def return_n_val(self):\n",
    "        return self.n_val\n",
    "    \n",
    "    def return_n_train(self):\n",
    "        return self.n_data_all - self.n_val - self.n_test\n",
    "    \n",
    "    def return_batch_size(self):\n",
    "        return self.batch_size\n",
    "    \n",
    "    def return_shape_X(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[0]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_shape_Y(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[1]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_test_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_test = torch.stack([torch.from_numpy(self.getdata(self.list_file_name_test, i)[0]) for i in range(self.n_test)])\n",
    "        data_Y_test = torch.stack([torch.from_numpy(self.getdata(self.list_file_name_test, i)[1]) for i in range(self.n_test)])\n",
    "        return data_X_test, data_Y_test\n",
    "    \n",
    "    def return_val_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_val = torch.stack([torch.from_numpy(self.getdata(self.list_file_name_val, i)[0]) for i in range(self.n_val)])\n",
    "        data_Y_val = torch.stack([torch.from_numpy(self.getdata(self.list_file_name_val, i)[1]) for i in range(self.n_val)])\n",
    "        return data_X_val, data_Y_val\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "#var\n",
    "path_dir_X = \"../data_X\"\n",
    "path_dir_Y = \"../data_Y_Task3\"\n",
    "n_test = 100\n",
    "n_val = 100\n",
    "batch_size = 5000\n",
    "\n",
    "#---------------------------------------------\n",
    "#instance\n",
    "dataset = CustomDataset(path_dir_X=path_dir_X, path_dir_Y=path_dir_Y, n_test=n_test, n_val=n_val, batch_size=batch_size)\n",
    "dataloder = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### criterion (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "#https://neptune.ai/blog/pytorch-loss-functions\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "#var (condition)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#----------------------------\n",
    "#var (train)\n",
    "num_epochs = 1000\n",
    "n_print_train_result = 1\n",
    "val_flag = True\n",
    "#----------------------------\n",
    "#var (model)\n",
    "in_channels = dataset.return_shape_X()[0]\n",
    "in_length = dataset.return_shape_X()[1]\n",
    "out_channels = dataset.return_shape_Y()[0]\n",
    "out_length = dataset.return_shape_Y()[1]\n",
    "model = model_1(in_channels, in_length, out_channels, out_length, batch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,500], gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch\n",
    "#https://www.geeksforgeeks.org/initialize-weights-in-pytorch/\n",
    "def init_normal(model):\n",
    "    if type(model) == nn.Linear:\n",
    "        nn.init.zeros_(model.weight)\n",
    "\n",
    "def print_weight(model):\n",
    "    if type(model) == nn.Linear:\n",
    "        print(model.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0063, -0.0141,  0.0123,  ...,  0.0107, -0.0036, -0.0360],\n",
      "        [-0.0202,  0.0261,  0.0297,  ..., -0.0360,  0.0068, -0.0108],\n",
      "        [-0.0076,  0.0241, -0.0229,  ..., -0.0099, -0.0029,  0.0343],\n",
      "        ...,\n",
      "        [-0.0018, -0.0116,  0.0035,  ...,  0.0115,  0.0345, -0.0259],\n",
      "        [ 0.0226, -0.0284, -0.0053,  ...,  0.0253,  0.0030,  0.0255],\n",
      "        [ 0.0162, -0.0127,  0.0223,  ..., -0.0155,  0.0100,  0.0001]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 4.9395e-02, -1.1866e-01,  1.0174e-01,  1.7613e-01,  1.7583e-01,\n",
      "         -8.4921e-02,  1.0164e-01, -5.8418e-02, -7.6789e-02, -6.4151e-02,\n",
      "          1.7600e-01, -3.9761e-02,  4.4565e-02,  1.3238e-01,  1.5917e-01,\n",
      "         -1.8088e-01, -6.1273e-02, -3.5775e-02, -3.2673e-02,  5.8985e-02,\n",
      "          4.4759e-02, -1.2172e-01, -7.1055e-02, -1.3045e-01, -6.9892e-02,\n",
      "         -2.5617e-02, -1.3235e-01, -1.4928e-01,  1.2420e-01, -1.7630e-01],\n",
      "        [-1.6977e-01, -1.6635e-01,  1.7292e-01, -1.1567e-01, -1.7271e-01,\n",
      "          1.3168e-03,  4.8777e-02, -2.7929e-02,  1.5914e-01,  8.8202e-02,\n",
      "          1.4359e-01, -1.1308e-01, -1.0579e-02,  8.0504e-02, -3.1588e-02,\n",
      "         -8.6304e-02,  1.5260e-01,  1.0336e-01,  5.0409e-02,  1.5108e-01,\n",
      "         -1.3244e-01, -8.5038e-02,  1.6726e-01, -1.3219e-01,  8.7029e-02,\n",
      "         -9.7137e-02,  1.0073e-01, -4.4806e-02, -6.8912e-02, -2.7452e-02],\n",
      "        [-1.0879e-01,  1.2873e-02, -6.5046e-02,  5.7419e-02, -1.1438e-01,\n",
      "         -1.7551e-01, -1.7270e-01, -3.5017e-02,  2.1130e-02,  1.3547e-01,\n",
      "         -1.0540e-01, -4.8504e-02, -1.3734e-02, -8.9275e-02, -6.9830e-03,\n",
      "         -1.3121e-01,  1.6437e-01, -2.9773e-03, -5.5907e-02,  1.0410e-01,\n",
      "         -6.1268e-02,  1.4618e-01,  2.9381e-03, -6.9282e-02,  1.4312e-01,\n",
      "         -9.4665e-02,  1.0202e-01, -1.0251e-01,  3.8157e-02, -1.0441e-01],\n",
      "        [ 1.2687e-01,  1.4142e-02,  1.3438e-01, -1.4931e-01,  6.9022e-02,\n",
      "         -9.5774e-02,  1.0135e-01, -1.0231e-01,  1.1346e-01,  1.2567e-01,\n",
      "          1.2377e-01,  1.3711e-01,  3.0445e-02,  4.4458e-02,  4.4528e-03,\n",
      "         -1.3536e-01, -1.2332e-01, -1.6493e-01,  1.5870e-01, -8.5384e-02,\n",
      "          8.1121e-02,  5.4862e-02,  2.3048e-02,  8.8577e-02,  3.7590e-02,\n",
      "          1.6632e-01, -9.7631e-02,  1.4149e-01, -1.6888e-01, -3.6709e-02],\n",
      "        [ 6.4988e-02, -1.7781e-01,  1.8121e-01,  1.6444e-01, -1.4833e-01,\n",
      "          2.9120e-02,  6.1528e-02, -1.5162e-01, -7.9899e-02, -1.4140e-01,\n",
      "         -5.6697e-02,  8.7190e-02, -4.9251e-02,  1.6636e-01, -9.7408e-02,\n",
      "         -1.5659e-01,  6.6067e-02,  4.1264e-02, -1.6680e-01, -1.1938e-01,\n",
      "          6.3060e-02, -5.6032e-02, -8.6607e-02, -8.6947e-02, -1.5626e-01,\n",
      "         -1.6627e-01, -6.3958e-02,  5.0385e-02,  2.7380e-02,  1.1842e-01],\n",
      "        [-7.1876e-02,  4.0812e-02, -1.5631e-01,  1.6414e-01, -2.4197e-02,\n",
      "         -7.9124e-03, -3.5185e-02,  5.5418e-02,  4.9609e-02, -1.8237e-01,\n",
      "         -4.2444e-02, -5.1217e-02, -1.0684e-01,  1.4792e-01,  1.6173e-01,\n",
      "         -1.4616e-01, -1.5669e-01,  1.5382e-02,  2.4698e-03,  7.9738e-02,\n",
      "         -5.7683e-02, -3.7813e-02,  1.5407e-01,  1.1089e-02,  6.3181e-02,\n",
      "         -1.0146e-01,  1.3470e-01, -1.4013e-01, -8.8296e-02, -1.5963e-01],\n",
      "        [-9.1060e-02,  1.9175e-02, -4.9607e-02, -9.2495e-02, -1.4807e-01,\n",
      "         -5.6747e-02, -8.2810e-02, -1.3380e-01, -1.5261e-01,  6.7929e-04,\n",
      "         -1.4946e-01,  1.6562e-01,  1.1804e-01,  1.1335e-01,  1.5767e-02,\n",
      "         -8.9723e-02,  6.1507e-02,  1.6016e-02, -3.8704e-02, -2.9583e-02,\n",
      "         -1.4831e-01,  1.5419e-01, -3.6040e-02,  1.8159e-01, -2.6767e-02,\n",
      "         -1.2607e-02,  1.4000e-02,  7.4919e-03, -1.0288e-01,  9.7373e-02],\n",
      "        [-1.4067e-02, -9.7001e-02, -7.6024e-02, -3.3782e-02,  1.0425e-01,\n",
      "         -7.6256e-02,  9.5811e-02,  7.0888e-02, -1.6293e-01,  1.1479e-01,\n",
      "          4.6767e-02,  7.3902e-02, -1.7571e-02, -1.2693e-01,  1.1008e-01,\n",
      "          3.1936e-02, -7.2599e-02,  1.5615e-01, -6.9238e-02,  1.6742e-01,\n",
      "         -1.6733e-01, -1.0828e-01,  1.9749e-02,  8.9917e-02, -2.5784e-02,\n",
      "         -1.5662e-01, -5.2665e-02, -4.1517e-02, -4.8111e-02,  9.6852e-06],\n",
      "        [ 1.0370e-01,  1.8098e-01, -1.7845e-01,  4.7751e-02,  1.7824e-01,\n",
      "         -9.1342e-02,  1.3196e-01, -8.8995e-03,  1.0741e-01, -9.6771e-02,\n",
      "          1.1757e-01,  1.5238e-03,  8.6402e-02, -2.6815e-02,  1.3904e-01,\n",
      "          9.6647e-02,  1.8377e-02,  7.0028e-02,  1.2043e-01,  6.8053e-02,\n",
      "         -1.2601e-01, -2.7705e-02,  8.1935e-02, -3.4587e-02, -1.3738e-01,\n",
      "         -1.0364e-01,  1.3576e-01,  1.7637e-01,  8.9132e-02,  1.8427e-03],\n",
      "        [ 1.1851e-01, -5.3588e-02, -1.1965e-01, -1.4590e-01, -4.4415e-02,\n",
      "          1.2372e-01, -9.2242e-02, -1.5162e-01, -1.5664e-01,  6.1051e-02,\n",
      "          6.9999e-02, -3.4059e-02,  7.8996e-02, -7.8218e-02,  1.0179e-01,\n",
      "          7.1995e-02,  3.3531e-02,  2.7418e-02, -1.1194e-01,  5.6252e-02,\n",
      "         -9.7478e-02,  1.3188e-02,  8.2014e-02,  1.3675e-01,  1.4769e-01,\n",
      "          4.4043e-02, -2.3674e-02, -6.9976e-02,  7.1066e-02, -1.5949e-01],\n",
      "        [ 1.6786e-01,  3.4871e-02, -6.6864e-02, -1.5924e-01,  1.1053e-01,\n",
      "         -1.6073e-01,  1.6956e-01,  1.1704e-01, -1.6390e-02, -2.6058e-02,\n",
      "          7.2551e-02, -1.2577e-01,  7.2574e-02, -3.1966e-02,  3.8923e-02,\n",
      "          1.5900e-01,  1.5006e-01, -1.7066e-01, -1.6477e-01, -8.9738e-02,\n",
      "          1.8214e-01,  1.3570e-01,  1.4437e-01, -1.2019e-02,  1.0004e-01,\n",
      "          7.6369e-02, -1.7733e-01, -1.7436e-01,  1.1667e-01, -1.2389e-01],\n",
      "        [-1.1320e-01, -1.2218e-01, -6.2765e-02,  1.3740e-01, -4.4235e-02,\n",
      "          1.6443e-01, -1.4377e-01,  7.8120e-02, -8.1290e-02,  1.4091e-01,\n",
      "          5.0420e-02, -1.4354e-01,  8.2953e-02,  1.0305e-01, -1.4266e-01,\n",
      "          1.7718e-01,  5.8803e-02, -7.2991e-02,  6.5754e-02,  1.7702e-02,\n",
      "         -1.2681e-01,  8.0862e-02, -1.7619e-01, -5.0491e-02,  4.1084e-02,\n",
      "          1.8147e-01,  6.6144e-02, -9.2498e-02, -3.1410e-03,  1.7178e-01],\n",
      "        [ 2.4566e-02, -1.2926e-01,  5.5599e-02, -1.1658e-01, -8.2643e-02,\n",
      "          3.7757e-02,  1.5394e-01, -1.0674e-01,  1.2513e-01,  1.8229e-01,\n",
      "          1.7263e-01, -1.2333e-01, -2.8769e-02, -1.6731e-01,  1.1101e-02,\n",
      "         -2.3793e-02, -6.6314e-02,  1.9520e-02,  1.5027e-01, -1.0309e-01,\n",
      "          7.0045e-02,  1.5970e-01,  5.3582e-02, -2.8653e-02, -4.5199e-02,\n",
      "          8.5527e-02,  4.5068e-02,  9.0949e-02, -4.0341e-02,  1.2273e-01],\n",
      "        [ 1.1587e-02, -6.7975e-02, -1.1727e-01, -5.9399e-02, -1.1567e-01,\n",
      "          1.1023e-01, -2.3721e-02,  6.5802e-02,  1.4363e-01, -5.4597e-02,\n",
      "          1.3472e-01, -6.4141e-02, -8.8455e-02,  1.5093e-01, -1.7544e-01,\n",
      "         -5.8897e-02,  1.3169e-01,  1.4365e-01,  3.9311e-02,  1.7501e-01,\n",
      "         -2.4193e-02, -1.0022e-01,  1.7138e-01,  4.2852e-02,  9.7395e-02,\n",
      "         -1.1128e-01,  1.1716e-01, -1.2726e-01,  9.5671e-02,  6.6776e-02],\n",
      "        [ 2.9613e-02, -4.4833e-02, -1.2355e-01, -9.3353e-02, -8.8551e-02,\n",
      "         -1.5353e-01,  3.6541e-02, -9.7256e-02,  1.4104e-01,  1.5931e-02,\n",
      "         -1.3608e-01,  1.2900e-01, -7.9977e-02, -4.7561e-02, -9.1571e-02,\n",
      "         -1.2090e-01, -8.0784e-02, -3.9598e-02, -1.8044e-01, -1.2840e-01,\n",
      "          1.6476e-01, -8.8825e-02, -5.6918e-02,  7.1179e-02, -1.2433e-01,\n",
      "          1.4197e-01,  6.6821e-02, -1.0797e-01,  1.6709e-01,  1.6046e-01],\n",
      "        [ 2.6961e-02, -1.3075e-01, -7.1303e-02,  9.4284e-02,  4.4892e-02,\n",
      "         -6.4830e-02, -5.9735e-02,  1.0063e-01,  1.7145e-01, -1.1662e-01,\n",
      "          1.6436e-01,  1.6857e-01, -6.3187e-02, -9.2321e-02, -1.0341e-01,\n",
      "          1.0626e-01, -7.5021e-02,  1.5882e-01, -2.7667e-02,  2.8083e-02,\n",
      "         -1.3701e-01, -9.2667e-02, -8.7669e-02,  1.7826e-01, -1.4385e-01,\n",
      "         -5.3194e-02, -2.8702e-03, -1.6396e-01, -1.1065e-01,  1.4641e-01],\n",
      "        [-4.9486e-02, -1.3957e-01, -6.5844e-02, -1.5639e-01, -1.6227e-01,\n",
      "          1.3813e-01, -2.9964e-02,  4.9319e-02, -7.4921e-03,  3.2038e-02,\n",
      "         -3.1739e-03, -8.6532e-02,  1.5226e-01,  5.4387e-02, -9.2216e-02,\n",
      "         -6.0357e-02, -8.0836e-02, -3.2188e-02,  1.0876e-01, -1.8104e-01,\n",
      "         -1.2892e-02,  1.4860e-01,  2.4125e-02, -2.6529e-02, -1.7202e-01,\n",
      "         -1.3305e-01,  4.8932e-02, -2.3666e-02, -1.0990e-01, -4.7681e-02],\n",
      "        [ 1.3617e-01,  3.2433e-02,  7.8547e-02,  5.7458e-02, -5.2162e-02,\n",
      "          4.0160e-02, -3.9818e-02,  1.2300e-01,  5.3723e-02,  8.4223e-02,\n",
      "          1.5283e-01,  9.9658e-03, -9.6164e-02,  9.5876e-02,  1.1498e-01,\n",
      "         -1.0049e-01,  6.6743e-02,  1.8114e-01, -1.4780e-01, -1.4316e-01,\n",
      "         -1.3243e-01, -2.0324e-02, -1.7037e-01, -1.3872e-01, -1.7116e-01,\n",
      "         -1.1920e-01, -1.0801e-01,  1.6697e-01,  1.4373e-01, -1.4379e-01],\n",
      "        [ 1.4713e-01, -1.0015e-01, -1.3086e-01,  3.4212e-02, -1.5883e-01,\n",
      "         -3.5740e-02, -1.5807e-01,  9.7262e-02, -1.7187e-01,  3.1285e-02,\n",
      "          1.6974e-01, -3.7546e-02, -4.7410e-02, -1.0019e-01, -1.2780e-02,\n",
      "         -7.3203e-03,  4.1713e-02, -1.7316e-01, -7.3914e-02, -4.8155e-02,\n",
      "          9.0133e-02,  2.3914e-03,  2.6888e-02,  1.0740e-01, -1.5294e-01,\n",
      "          8.6002e-02,  5.0052e-02,  1.2119e-01, -1.1549e-01,  9.2439e-03],\n",
      "        [-1.1278e-01,  1.1002e-01, -9.1643e-02,  1.1649e-01, -7.1834e-02,\n",
      "          6.8152e-03, -1.0452e-01,  7.5630e-03, -5.5639e-02,  1.6650e-01,\n",
      "         -2.4852e-02, -1.8203e-01,  2.0470e-02,  4.3146e-02,  2.5825e-02,\n",
      "         -1.7989e-01,  3.8805e-02, -1.5758e-01, -1.2714e-01,  1.7468e-02,\n",
      "          1.4096e-01,  1.1844e-01, -1.5209e-01,  4.4329e-02, -5.8721e-02,\n",
      "          1.3193e-01, -2.0797e-03,  7.3803e-02,  1.5386e-01,  6.6409e-02],\n",
      "        [-1.5138e-01, -1.3353e-01, -1.3858e-01,  8.4031e-02,  7.9040e-04,\n",
      "         -6.8839e-02,  1.5692e-01,  1.1808e-01, -1.1438e-01,  6.9615e-02,\n",
      "          1.2223e-01, -2.3254e-02,  4.6333e-02,  1.3986e-01, -1.8005e-02,\n",
      "          6.3641e-02, -1.2302e-01,  2.2384e-02, -8.4833e-02, -1.4573e-01,\n",
      "         -3.7158e-02, -5.4647e-03, -9.5783e-02, -3.5766e-02,  9.5548e-02,\n",
      "          1.6709e-01,  4.6065e-02, -1.7732e-01, -1.0282e-01, -1.2274e-01],\n",
      "        [ 1.5419e-01, -2.6109e-02, -6.8016e-02, -1.3381e-02, -5.3945e-02,\n",
      "         -3.5234e-02,  1.2107e-01, -1.2196e-01,  1.1324e-01,  9.3168e-02,\n",
      "          1.5800e-01,  1.0342e-01, -9.6591e-02, -1.7805e-01, -5.3345e-02,\n",
      "          5.4573e-02, -7.4255e-02,  1.0038e-01, -6.1549e-02,  2.1218e-02,\n",
      "          4.2081e-02, -1.2916e-01, -1.7227e-02,  9.1185e-02,  8.0303e-02,\n",
      "          1.5481e-01, -1.2507e-01, -1.3439e-01,  7.4109e-02, -1.4625e-01],\n",
      "        [-1.3345e-01, -6.4347e-02, -4.1851e-03, -1.3278e-01, -9.4853e-02,\n",
      "         -1.6731e-01, -9.8782e-02,  1.0546e-01, -1.4339e-01,  9.2760e-02,\n",
      "          5.1792e-02,  1.7395e-01,  1.6315e-01,  7.7431e-02, -3.6349e-02,\n",
      "          5.1425e-02,  6.1218e-02,  7.7304e-02,  9.2062e-02,  9.2503e-02,\n",
      "          5.2121e-02, -9.3020e-02, -6.8388e-02, -1.2376e-01,  1.0173e-01,\n",
      "         -8.1129e-02,  7.4335e-02, -7.4617e-02, -1.0373e-01,  8.3954e-02],\n",
      "        [-7.8651e-02,  9.9928e-02,  1.1545e-01, -1.8000e-01,  1.1809e-01,\n",
      "          4.5577e-02,  9.7660e-02, -1.9519e-02,  1.3257e-01,  9.5637e-02,\n",
      "         -4.9221e-02, -1.4345e-01,  1.3682e-02, -6.9542e-02, -1.4599e-01,\n",
      "         -1.3288e-01, -1.1616e-01, -1.6829e-01, -6.4717e-02,  1.3335e-01,\n",
      "          1.1062e-01,  2.7437e-02, -1.0358e-01,  1.0418e-01,  1.5958e-01,\n",
      "         -5.5943e-03,  1.2145e-01, -4.0167e-02, -1.1076e-01, -1.5418e-01],\n",
      "        [ 7.4192e-02, -2.9598e-02, -1.5914e-01, -8.6198e-02,  1.2830e-01,\n",
      "         -1.2084e-01,  1.7255e-01,  1.6881e-01, -4.6954e-02, -1.2699e-01,\n",
      "         -7.0491e-03,  5.1390e-02,  2.1234e-02, -6.6903e-02, -2.6445e-02,\n",
      "          1.2748e-01, -7.7254e-02,  1.5503e-01, -1.3994e-01, -1.6755e-01,\n",
      "         -4.0966e-02, -1.6645e-01,  1.1946e-01,  8.9743e-02, -2.1455e-02,\n",
      "         -6.1122e-02, -2.1636e-02, -9.6689e-02, -1.2941e-01,  1.2897e-01],\n",
      "        [ 1.4501e-01, -3.1328e-03, -1.1162e-01, -1.5286e-01,  1.2654e-01,\n",
      "         -3.1090e-02, -1.2159e-01,  1.4761e-01,  6.2522e-02,  1.6213e-01,\n",
      "         -1.8164e-01, -1.7763e-01, -6.9457e-02,  1.6615e-01,  1.6220e-01,\n",
      "         -2.8347e-02, -6.7488e-02,  1.7143e-02,  1.2694e-01, -1.6497e-01,\n",
      "          8.1289e-02, -1.7076e-02,  1.1745e-02,  1.0655e-01,  8.0220e-02,\n",
      "         -1.5553e-01,  8.1870e-02, -2.4807e-02,  1.0428e-01, -3.0650e-02],\n",
      "        [ 5.6153e-02,  2.5304e-02, -1.8646e-02, -1.3462e-01,  7.2971e-02,\n",
      "         -1.5577e-01,  1.5468e-01,  1.7615e-01, -1.6910e-01, -1.3720e-01,\n",
      "          7.5541e-02,  7.6105e-02, -4.0681e-02, -1.3070e-01,  1.2590e-01,\n",
      "          1.5577e-01,  1.7606e-01,  1.0023e-01, -2.0250e-02,  1.0231e-01,\n",
      "         -5.7016e-02,  6.0676e-03,  1.1066e-01, -1.4559e-01, -7.7067e-02,\n",
      "          1.6903e-01,  1.7668e-01,  8.8156e-02, -1.2289e-01,  1.3935e-01],\n",
      "        [-7.5451e-02,  1.4444e-01, -1.1126e-01,  3.3340e-02,  7.9852e-02,\n",
      "          1.3902e-01, -5.3641e-02,  1.2139e-01,  6.3308e-02,  2.6756e-02,\n",
      "          1.1949e-01,  4.9107e-02,  2.8014e-02,  1.6915e-01,  1.6297e-01,\n",
      "         -1.4119e-01,  6.5699e-03,  4.1161e-02, -2.1339e-02,  6.1957e-02,\n",
      "          1.3346e-01,  2.3076e-02,  2.8027e-02, -1.2512e-01, -3.0072e-02,\n",
      "          1.4159e-01, -1.7800e-01, -6.1940e-02, -1.0338e-01, -1.3134e-01],\n",
      "        [ 1.8029e-02, -1.0438e-01, -6.4505e-02,  8.1653e-02,  6.7847e-03,\n",
      "          1.3434e-01,  3.2180e-02, -1.3914e-01, -2.5021e-02,  1.9888e-02,\n",
      "         -1.7009e-02,  4.3332e-02,  1.2883e-01, -1.0043e-01,  1.2471e-01,\n",
      "         -1.7888e-01, -1.4006e-02, -1.7595e-01,  7.4828e-02, -2.5487e-02,\n",
      "         -7.6240e-02,  2.4767e-02, -1.3762e-01, -8.2646e-02, -3.7127e-02,\n",
      "         -1.1954e-01,  1.2368e-02, -2.1945e-02,  1.0181e-01,  9.9883e-02],\n",
      "        [ 3.0127e-03,  5.6576e-02, -5.0891e-02,  7.3030e-02,  1.7568e-02,\n",
      "          1.2627e-01, -1.5305e-01, -1.7535e-01, -8.6354e-02, -1.0097e-01,\n",
      "          6.0304e-02,  1.1111e-02, -8.7369e-02,  5.3703e-02,  1.7416e-01,\n",
      "          1.0445e-01, -1.1984e-01,  1.0980e-01,  1.7134e-01,  1.3693e-01,\n",
      "         -4.2845e-02, -7.8092e-02,  6.0642e-02,  1.0474e-02, -8.8376e-02,\n",
      "          1.4297e-01,  1.5995e-01, -1.7983e-01, -1.7836e-01,  8.7839e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0417, -0.0774, -0.0874,  0.1428, -0.1008,  0.0032, -0.0403, -0.0915,\n",
      "          0.1491,  0.1015, -0.0037, -0.0803, -0.0590,  0.0088,  0.0490,  0.1758,\n",
      "         -0.0422, -0.0789,  0.0446, -0.0219, -0.1411, -0.1594,  0.0781,  0.1117,\n",
      "          0.1501,  0.0850, -0.0809,  0.1397, -0.1566,  0.1199],\n",
      "        [ 0.0237, -0.0588, -0.0595,  0.0050,  0.0948, -0.0334,  0.0485,  0.1599,\n",
      "         -0.0044, -0.0577, -0.0981,  0.0913, -0.0257, -0.1799,  0.1447,  0.0716,\n",
      "          0.0445, -0.0121, -0.0454,  0.1157,  0.0744,  0.1616,  0.0553,  0.1035,\n",
      "          0.1732, -0.1339,  0.0598, -0.1777,  0.0648, -0.0970],\n",
      "        [-0.0997,  0.0198, -0.0274, -0.1782, -0.1018, -0.0492,  0.1166, -0.1361,\n",
      "          0.1064,  0.0057,  0.1190, -0.1579,  0.0774,  0.0181,  0.0940, -0.1356,\n",
      "          0.1011, -0.0100, -0.0189, -0.1546, -0.0833, -0.1190,  0.0783, -0.1528,\n",
      "         -0.0948, -0.0748, -0.1571, -0.1410, -0.0430,  0.0868],\n",
      "        [ 0.0371, -0.0892, -0.0397, -0.0273,  0.0346, -0.0931,  0.1619,  0.0736,\n",
      "         -0.1435,  0.0042,  0.0804,  0.1429,  0.0692, -0.0365, -0.0025,  0.1492,\n",
      "         -0.1473,  0.0297, -0.0903, -0.1468, -0.0728, -0.1630, -0.1189, -0.0796,\n",
      "         -0.1738, -0.0577, -0.0712, -0.1516,  0.0064, -0.0050],\n",
      "        [ 0.1169, -0.0377, -0.1116, -0.0784,  0.0356, -0.0955,  0.0699, -0.1672,\n",
      "         -0.1441, -0.1265,  0.1435, -0.1144,  0.0688, -0.1036,  0.1558, -0.1635,\n",
      "         -0.1279,  0.1331,  0.0371,  0.1642, -0.1380, -0.0970, -0.1546, -0.0466,\n",
      "          0.1254,  0.1474,  0.0376,  0.0298,  0.1108, -0.1515],\n",
      "        [-0.0158, -0.1049, -0.0436, -0.0369,  0.1687, -0.0647, -0.1545,  0.0479,\n",
      "         -0.0150,  0.0755, -0.1179,  0.0227,  0.0674,  0.0842, -0.0889, -0.1258,\n",
      "          0.0662,  0.1802, -0.0530, -0.1129, -0.0546, -0.0738,  0.1219,  0.0670,\n",
      "         -0.1388, -0.1392, -0.1473,  0.0913,  0.1417,  0.0196],\n",
      "        [ 0.0061, -0.0384, -0.0281, -0.0507,  0.0769, -0.1797, -0.0090,  0.0487,\n",
      "         -0.1742, -0.0347, -0.0730,  0.0874, -0.1804,  0.0347,  0.0321,  0.0959,\n",
      "          0.0662, -0.0155, -0.1385,  0.0331, -0.1632,  0.1188, -0.0428, -0.0458,\n",
      "          0.1745, -0.1110,  0.1181,  0.1191, -0.1184,  0.0626],\n",
      "        [-0.1632, -0.1160, -0.0765, -0.0088,  0.0639, -0.1082,  0.1454,  0.1002,\n",
      "         -0.0856, -0.0122, -0.0594, -0.1612,  0.1052,  0.0682, -0.0900, -0.0864,\n",
      "         -0.0343, -0.1145, -0.1506, -0.0774, -0.0244,  0.0962, -0.1474,  0.0152,\n",
      "         -0.1002,  0.1465, -0.0124, -0.0920,  0.0516, -0.1730],\n",
      "        [ 0.1565,  0.0615,  0.0267, -0.1536,  0.0500,  0.1572,  0.1573,  0.0615,\n",
      "         -0.1060,  0.1401,  0.1642,  0.1606, -0.0568, -0.0893, -0.0139,  0.1767,\n",
      "          0.0710, -0.1319,  0.1166, -0.0778,  0.1332, -0.0504, -0.0767, -0.1324,\n",
      "         -0.1071,  0.1244, -0.0565, -0.0004, -0.1811, -0.1802],\n",
      "        [ 0.1222,  0.1290, -0.0351,  0.1057, -0.0926,  0.1756,  0.1190, -0.0715,\n",
      "          0.1510, -0.1498, -0.0161, -0.1053, -0.0961, -0.0858, -0.0714,  0.0745,\n",
      "          0.0798, -0.1433,  0.0844,  0.0977, -0.0913,  0.1814,  0.0972,  0.1613,\n",
      "         -0.1701,  0.0900, -0.0801, -0.1462,  0.0192,  0.0405],\n",
      "        [-0.1167, -0.1758,  0.1671,  0.0815, -0.0857,  0.0264,  0.0397,  0.0854,\n",
      "          0.0196,  0.0085, -0.0585, -0.1762,  0.1104, -0.1184,  0.0373,  0.0847,\n",
      "          0.0631, -0.0397,  0.0918, -0.1767,  0.0311,  0.0466,  0.1535, -0.0696,\n",
      "         -0.0520, -0.0485, -0.0351,  0.0112, -0.0133,  0.1149],\n",
      "        [ 0.0545, -0.0414, -0.0981, -0.0194,  0.1348, -0.0983, -0.1055, -0.0877,\n",
      "          0.0311,  0.1134, -0.0058, -0.1310, -0.0824,  0.0491,  0.0168,  0.0738,\n",
      "          0.0424, -0.1785, -0.1601, -0.0724, -0.1578,  0.0416,  0.1200,  0.1038,\n",
      "          0.0890,  0.1649, -0.1525,  0.1569,  0.1425, -0.1108],\n",
      "        [ 0.1515,  0.0190,  0.1489, -0.1530, -0.1677,  0.1706,  0.0894, -0.0731,\n",
      "         -0.1781, -0.1248,  0.0885,  0.0567,  0.0193, -0.0234, -0.0987, -0.0450,\n",
      "         -0.1697, -0.0635, -0.1592,  0.1578,  0.1788,  0.0052, -0.0120,  0.0942,\n",
      "          0.1298,  0.0146, -0.1472,  0.0165, -0.1630,  0.1540],\n",
      "        [-0.1303, -0.1412, -0.1508, -0.0771,  0.0056,  0.0771, -0.0767, -0.1482,\n",
      "          0.0770,  0.0549, -0.0654, -0.1380,  0.1424,  0.1005,  0.1474, -0.1008,\n",
      "         -0.1154, -0.1191, -0.0680, -0.1803, -0.1626,  0.0472, -0.0493, -0.0451,\n",
      "          0.1445,  0.1244,  0.0254, -0.1314,  0.0628,  0.0188],\n",
      "        [ 0.1439, -0.0245, -0.1717,  0.0256,  0.0649,  0.0403,  0.0765, -0.0340,\n",
      "         -0.0191, -0.0924,  0.0777, -0.1031,  0.0867,  0.0033, -0.1194, -0.1428,\n",
      "          0.1204, -0.1617, -0.0132,  0.1345,  0.0715,  0.0149, -0.0317,  0.1035,\n",
      "          0.1737,  0.0734, -0.1472,  0.1041, -0.0656, -0.1489],\n",
      "        [-0.0352,  0.0296, -0.0440,  0.0985, -0.1367,  0.0213,  0.0858, -0.1755,\n",
      "          0.0552, -0.1360,  0.1434, -0.1649, -0.0618,  0.0293,  0.0475, -0.0395,\n",
      "         -0.0644,  0.0075, -0.1715, -0.1295,  0.0435,  0.0279,  0.0380, -0.1345,\n",
      "         -0.0701, -0.1702,  0.1727,  0.0061, -0.0072,  0.1470],\n",
      "        [-0.1292,  0.0354,  0.0915, -0.1557, -0.1200, -0.0269,  0.1788, -0.1344,\n",
      "          0.0630,  0.0146,  0.0317,  0.1683,  0.1396,  0.0915, -0.1682, -0.1026,\n",
      "         -0.1411,  0.1218,  0.1381, -0.0351, -0.1265,  0.1653,  0.0349,  0.0134,\n",
      "          0.0717, -0.0798, -0.1109,  0.0117, -0.0291, -0.0559],\n",
      "        [-0.1579,  0.0075,  0.1582, -0.0783,  0.0023, -0.1369, -0.1176, -0.1605,\n",
      "          0.1739,  0.1330,  0.0524,  0.1669,  0.1415, -0.1136, -0.1572,  0.1454,\n",
      "          0.0122,  0.0955,  0.0838, -0.0857,  0.0148,  0.0289, -0.1260, -0.1006,\n",
      "         -0.1728,  0.1824,  0.0876,  0.1224, -0.0936, -0.0633],\n",
      "        [ 0.1116, -0.0742, -0.1591,  0.0383,  0.1535,  0.1484,  0.1561, -0.0535,\n",
      "          0.0278,  0.0345, -0.0947, -0.1496,  0.0281, -0.0124,  0.1267,  0.0703,\n",
      "         -0.1696, -0.1354, -0.1351,  0.0006, -0.0180, -0.0972,  0.0497, -0.0805,\n",
      "          0.0978,  0.1305,  0.1791, -0.1735,  0.1604,  0.1105],\n",
      "        [-0.1622,  0.0563,  0.1820, -0.1218, -0.0356,  0.0028,  0.1004, -0.1503,\n",
      "         -0.1246,  0.1688,  0.0733, -0.1823, -0.1425,  0.0959, -0.1528, -0.1222,\n",
      "         -0.0789, -0.1588,  0.1309, -0.1377,  0.1174,  0.0256, -0.1513, -0.0406,\n",
      "         -0.0399, -0.0683, -0.1665,  0.0312,  0.0359,  0.1661],\n",
      "        [-0.0276, -0.1130,  0.1512,  0.0029,  0.1272, -0.1204, -0.0735,  0.0614,\n",
      "          0.0277,  0.1014, -0.1590, -0.0036,  0.0739,  0.0946,  0.0240,  0.1683,\n",
      "          0.0944, -0.0141,  0.1191, -0.0125, -0.0711, -0.1228,  0.1209, -0.0090,\n",
      "          0.1344, -0.1768,  0.0058, -0.0754, -0.0479,  0.1066],\n",
      "        [-0.0056,  0.1249, -0.0465,  0.1431, -0.0414, -0.0842,  0.0061,  0.0090,\n",
      "          0.1270,  0.0331, -0.1405,  0.0546,  0.1595, -0.0200, -0.0705, -0.1721,\n",
      "          0.1551, -0.1000, -0.1530,  0.1795, -0.0938, -0.1034, -0.0571,  0.0152,\n",
      "         -0.1081,  0.0169,  0.0125,  0.0093, -0.0445,  0.0046],\n",
      "        [ 0.1532,  0.1697, -0.1403, -0.0224,  0.1349,  0.1622, -0.1182,  0.1114,\n",
      "         -0.1653,  0.1655,  0.0487, -0.0287, -0.0998, -0.0440,  0.0662,  0.0244,\n",
      "         -0.0436,  0.1318,  0.1377,  0.0559, -0.1532, -0.1586, -0.0037,  0.1170,\n",
      "         -0.0182, -0.1568,  0.1173,  0.1174, -0.1782,  0.1533],\n",
      "        [-0.1211, -0.1467,  0.1224,  0.0950,  0.0334,  0.0425, -0.0310,  0.0159,\n",
      "          0.0745,  0.0062, -0.0982, -0.1672,  0.1522,  0.1224, -0.1328, -0.0653,\n",
      "         -0.0059,  0.1570, -0.0835, -0.1250,  0.0816, -0.0287, -0.0953,  0.0939,\n",
      "         -0.0858, -0.1623,  0.1613, -0.1445,  0.0135, -0.1039],\n",
      "        [ 0.1421, -0.1682,  0.1024,  0.1293, -0.1379, -0.0423,  0.0012, -0.0440,\n",
      "         -0.1278,  0.0360,  0.0241,  0.1586,  0.1513,  0.0727, -0.1655, -0.1772,\n",
      "         -0.1746, -0.0306,  0.0979, -0.0092,  0.0429,  0.1468, -0.0620,  0.0510,\n",
      "          0.0370,  0.0087, -0.1210, -0.1669,  0.0451,  0.1472],\n",
      "        [ 0.1741, -0.1662, -0.1645,  0.0787, -0.1729,  0.0303, -0.0419, -0.1335,\n",
      "          0.0520,  0.1476,  0.0076,  0.0642,  0.0708, -0.0436,  0.0182, -0.0209,\n",
      "         -0.0116, -0.0235,  0.1236, -0.0503, -0.1594, -0.1101,  0.1312, -0.1236,\n",
      "         -0.1205, -0.0495, -0.0068, -0.1396,  0.1635,  0.1446],\n",
      "        [-0.0067, -0.1152,  0.1444, -0.1131,  0.1182,  0.1201, -0.1119, -0.1397,\n",
      "         -0.1034,  0.0178,  0.0519,  0.0394,  0.0393, -0.1019, -0.0633, -0.0770,\n",
      "          0.1275,  0.1525,  0.0067, -0.0057, -0.0166, -0.0788, -0.0275, -0.1122,\n",
      "         -0.0227,  0.1036,  0.0145, -0.0993, -0.0995,  0.1290],\n",
      "        [ 0.0984, -0.0041, -0.0575, -0.1530,  0.1559,  0.0867,  0.0506,  0.0184,\n",
      "         -0.0457, -0.1126,  0.1724, -0.1236,  0.0205,  0.1378, -0.0429,  0.0539,\n",
      "          0.1601, -0.1015, -0.1387, -0.0235, -0.0762,  0.1433, -0.1338,  0.1778,\n",
      "         -0.0461,  0.1630,  0.1352, -0.0276, -0.1137,  0.0513],\n",
      "        [ 0.1774, -0.0983, -0.1099,  0.1799, -0.1529,  0.0020, -0.1697, -0.1284,\n",
      "         -0.0545, -0.0253, -0.1343,  0.0308,  0.1186,  0.1046, -0.1655, -0.1214,\n",
      "          0.1344, -0.1773, -0.0158, -0.0334, -0.0448,  0.0410,  0.0668, -0.0978,\n",
      "          0.1675,  0.1095,  0.1223, -0.0226, -0.0185,  0.0571],\n",
      "        [-0.0081, -0.0812,  0.0048, -0.1676,  0.1662, -0.0448, -0.1534, -0.1676,\n",
      "         -0.1444, -0.1257, -0.1717, -0.0387, -0.0965, -0.0648,  0.0713,  0.0334,\n",
      "         -0.1606, -0.0158, -0.0886,  0.1090, -0.0577,  0.0151, -0.0990, -0.0055,\n",
      "          0.1058, -0.1102,  0.0965,  0.0385, -0.1814,  0.1396]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1047, -0.1068,  0.1377,  ..., -0.1312,  0.0652, -0.1323],\n",
      "        [ 0.1502, -0.1797,  0.1574,  ...,  0.1115,  0.1684, -0.1599],\n",
      "        [-0.0301, -0.1719, -0.1003,  ..., -0.0343, -0.1537,  0.0391],\n",
      "        ...,\n",
      "        [-0.0050, -0.1371,  0.1274,  ...,  0.0107,  0.1359, -0.0946],\n",
      "        [ 0.1222, -0.1491, -0.1623,  ..., -0.0200,  0.1765,  0.1572],\n",
      "        [ 0.1197, -0.0412, -0.1630,  ..., -0.1708,  0.1521, -0.0099]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model_1(\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv1d(12, 10, kernel_size=(10,), stride=(2,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv1d(10, 10, kernel_size=(10,), stride=(2,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Conv1d(10, 10, kernel_size=(10,), stride=(2,), padding=(1,))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=570, out_features=30, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=30, out_features=30, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=30, out_features=30, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.1, inplace=False)\n",
       "    (9): Linear(in_features=30, out_features=75, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.apply(init_normal)\n",
    "model.apply(print_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------\n",
    "#var (condition)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#----------------------------\n",
    "#var (train)\n",
    "num_epochs = 1000\n",
    "n_print_train_result = 1\n",
    "val_flag = True\n",
    "#----------------------------\n",
    "#var (model)\n",
    "in_channels = dataset.return_shape_X()[0]\n",
    "in_length = dataset.return_shape_X()[1]\n",
    "out_channels = dataset.return_shape_Y()[0]\n",
    "out_length = dataset.return_shape_Y()[1]\n",
    "model = model_1(in_channels, in_length, out_channels, out_length, batch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,500], gamma=0.7)\n",
    "\n",
    "#----------------------------\n",
    "#results\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "#----------------------------\n",
    "#学習\n",
    "for epoch in range(num_epochs):\n",
    "  #----------------------------\n",
    "  # train\n",
    "  model.train()\n",
    "  for i, (x, y) in enumerate(dataloder):\n",
    "    x = x.to(torch.float32).requires_grad_(True)\n",
    "    y = y.to(torch.float32).requires_grad_(True)\n",
    "    #----------------------------\n",
    "    #change the type\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    #----------------------------\n",
    "    #backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    print(x.grad.mean())\n",
    "    optimizer.step()\n",
    "    #----------------------------\n",
    "    #print & result\n",
    "    if (i+1) % n_print_train_result == 0:\n",
    "      print(f'Epoch: {epoch+1}, iter: {i+1}, train_loss: {loss: 0.4f}')\n",
    "    history[\"train_loss\"].append(loss)\n",
    "\n",
    "  #----------------------------\n",
    "  # eval\n",
    "  if val_flag == True:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      #----------------------------\n",
    "      #forward\n",
    "      x, y = dataset.return_val_data()\n",
    "\n",
    "      x = x.to(torch.float32).requires_grad_(True)\n",
    "      y = y.to(torch.float32).requires_grad_(True)\n",
    "      #----------------------------\n",
    "      #change the type\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "      #----------------------------\n",
    "      #forward\n",
    "      output = model(x)\n",
    "      loss = criterion(output, y)\n",
    "      #----------------------------\n",
    "      #print & result\n",
    "      history[\"val_loss\"].append(loss)\n",
    "      print(f'Epoch: {epoch+1}, val_loss: {loss: 0.4f}')\n",
    "  \n",
    "  #----------------------------\n",
    "  # scheduler\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (CustomDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union\n",
    "\n",
    "#---------------------------------------------\n",
    "# custom dataset\n",
    "#https://discuss.pytorch.org/t/custom-data-loader-for-big-data/129361\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dir_X:str, path_dir_Y:str, n_test:Union[int,float], n_val:Union[int,float], batch_size:int): # n_test -> float:ratio of test, int:number of test\n",
    "        #-----------------\n",
    "        # batch_size\n",
    "        self.batch_size = batch_size\n",
    "        # path_dir_X, path_dir_Y\n",
    "        self.path_dir_X = path_dir_X\n",
    "        self.path_dir_Y = path_dir_Y\n",
    "        # list_file_name_all\n",
    "        self.list_file_name_all = os.listdir(path_dir_X)\n",
    "        # n_data_all\n",
    "        self.n_data_all = len(self.list_file_name_all)\n",
    "        #check\n",
    "        if len(os.listdir(path_dir_X)) != len(os.listdir(path_dir_Y)):\n",
    "            raise ValueError(\"error!!!\")\n",
    "        if len(set(os.listdir(path_dir_X)) - set(os.listdir(path_dir_Y))) != 0:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # suffle\n",
    "        random.shuffle(self.list_file_name_all)\n",
    "        #-----------------\n",
    "        # n_test\n",
    "        if type(n_test)==int:\n",
    "            self.n_test = n_test\n",
    "        elif type(n_test)==float:\n",
    "            self.n_test = int(len(self.list_file_name_all)*n_test)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        # n_val\n",
    "        if type(n_val)==int:\n",
    "            self.n_val = n_val\n",
    "        elif type(n_val)==float:\n",
    "            self.n_val = int(len(self.list_file_name_all)*n_val)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #check\n",
    "        if self.n_data_all <= self.n_test+self.n_val:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # list_file_name_test / _val / _train\n",
    "        self.list_file_name_test = self.list_file_name_all[:self.n_test]\n",
    "        self.list_file_name_val = self.list_file_name_all[self.n_test:self.n_test+self.n_val]\n",
    "        self.list_file_name_train = self.list_file_name_all[self.n_test+self.n_val:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_file_name_train)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        return self.getdata(list_file_name=self.list_file_name_train, index=x)\n",
    "    \n",
    "    def getdata(self, list_file_name, index):\n",
    "        #file_name\n",
    "        file_name = list_file_name[index]\n",
    "        #data_X\n",
    "        path_file_X = \"{0}/{1}\".format(self.path_dir_X, file_name)\n",
    "        data_X = np.load(path_file_X, allow_pickle=True)\n",
    "        data_X = torch.from_numpy(data_X).to(torch.float32)\n",
    "        #data_Y\n",
    "        path_file_Y = \"{0}/{1}\".format(self.path_dir_Y, file_name)\n",
    "        data_Y = np.load(path_file_Y, allow_pickle=True)\n",
    "        data_Y = torch.from_numpy(data_Y).to(torch.float32)\n",
    "        #return\n",
    "        return data_X, data_Y\n",
    "    \n",
    "    def return_n_data_all(self):\n",
    "        return self.n_data_all\n",
    "    \n",
    "    def return_n_test(self):\n",
    "        return self.n_test\n",
    "    \n",
    "    def return_n_val(self):\n",
    "        return self.n_val\n",
    "    \n",
    "    def return_n_train(self):\n",
    "        return self.n_data_all - self.n_val - self.n_test\n",
    "    \n",
    "    def return_batch_size(self):\n",
    "        return self.batch_size\n",
    "    \n",
    "    def return_shape_X(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[0]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_shape_Y(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[1]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_test_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_test = torch.stack([self.getdata(self.list_file_name_test, i)[0] for i in range(self.n_test)])\n",
    "        data_Y_test = torch.stack([self.getdata(self.list_file_name_test, i)[1] for i in range(self.n_test)])\n",
    "        return data_X_test, data_Y_test\n",
    "    \n",
    "    def return_val_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_val = torch.stack([self.getdata(self.list_file_name_val, i)[0] for i in range(self.n_val)])\n",
    "        data_Y_val = torch.stack([self.getdata(self.list_file_name_val, i)[1] for i in range(self.n_val)])\n",
    "        return data_X_val, data_Y_val\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "#var\n",
    "path_dir_X = \"../data_X\"\n",
    "path_dir_Y = \"../data_Y_Task3\"\n",
    "n_test = 100\n",
    "n_val = 100\n",
    "batch_size = 10000\n",
    "\n",
    "#---------------------------------------------\n",
    "#instance\n",
    "dataset = CustomDataset(path_dir_X=path_dir_X, path_dir_Y=path_dir_Y, n_test=n_test, n_val=n_val, batch_size=batch_size)\n",
    "dataloder = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data_X, data_Y) in enumerate(dataloder):\n",
    "   # print(data_X.shape, data_Y.shape)\n",
    "   print(data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_data_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_val_data()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_test_data()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dataset.return_val_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #Conv1d\n",
    "        in_channels_inner_0 = in_channels; in_length_inner_0 = in_length\n",
    "        kernel_size=2; stride=1; padding=1; dilation=1; \n",
    "        out_channels_inner_0 = 30; out_length_inner_0 = (in_length_inner_0+2*padding-dilation*(kernel_size-1)-1)/stride+1\n",
    "        self.conv1d = nn.Conv1d(in_channels=in_channels_inner_0, out_channels=out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        in_channels_inner_1 = out_channels_inner_0*out_length_inner_0; out_channels_inner_1 = out_channels\n",
    "\n",
    "        #-----------------------------------\n",
    "        #layer0\n",
    "        self.layer0 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.Conv1d(in_channels=in_channels_inner_0, out_channels=out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.relu(),\n",
    "            nn.BatchNorm1d(out_channels_inner_0*out_length_inner_0),\n",
    "            #-----------------------------------\n",
    "            nn.Linear(in_channels_inner_1, out_channels_inner_1),\n",
    "            nn.relu(),\n",
    "            nn.Sigmoid()\n",
    "            #nn.BatchNorm1d(self.params_size+self.features_size),\n",
    "            #nn.BatchNorm1d(3),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output0 = self.layer0(x)\n",
    "        return output0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/sshuv/items/79d9364b8675fdc080cf\n",
    "#https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "#https://cvml-expertguide.net/terms/dl/layers/convolution-layer/\n",
    "\n",
    "\n",
    "#-----------------------------------\n",
    "#n_batch=10; n_feature=6; n_time=10; \n",
    "n_batch=1000; in_channels=75; in_length=500; \n",
    "x = torch.rand(n_batch, in_channels, in_length)\n",
    "\n",
    "#-----------------------------------\n",
    "in_channels=in_channels; out_channels=30; kernel_size=3; stride=1; padding=1; dilation=1; \n",
    "conv1d = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "#-----------------------------------\n",
    "n_batch=n_batch; out_channels=out_channels; out_length=int((in_length+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "y = conv1d(x)\n",
    "print(n_batch, out_channels, out_length)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5, requires_grad=True)\n",
    "mse_loss = nn.MSELoss()\n",
    "output = mse_loss(input, target) #mse_loss(input, target), mse_loss(target, input)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "print(input.grad)\n",
    "print(target.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "mse_loss = nn.MSELoss()\n",
    "output = mse_loss(input, target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mse_loss(input.view(15), target.view(15))\n",
    "output.backward()\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(5, 2)\n",
    "output = m(input)\n",
    "output = m(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
