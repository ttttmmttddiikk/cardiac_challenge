{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/g90/tatematsu1/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length, batch_size):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #var\n",
    "        self.in_channels = in_channels\n",
    "        self.in_length = in_length\n",
    "        self.out_channels = out_channels\n",
    "        self.out_length = out_length\n",
    "        self.batch_size = batch_size\n",
    "        #-----------------------------------\n",
    "        #Conv1d\n",
    "        self.in_channels_inner_0 = self.in_channels; self.in_length_inner_0 = self.in_length\n",
    "        kernel_size=2; stride=1; padding=1; dilation=1; \n",
    "        self.out_channels_inner_0 = 20; self.out_length_inner_0 = int((self.in_length_inner_0+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.in_channels_inner_0, out_channels=self.out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_1 = self.out_channels_inner_0*self.out_length_inner_0\n",
    "        self.out_channels_inner_1 = self.out_channels*self.out_length\n",
    "\n",
    "        #-----------------------------------\n",
    "        #layer0\n",
    "        self.layer0 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.Conv1d(in_channels=self.in_channels_inner_0, out_channels=self.out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            #nn.ReLU(),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.BatchNorm1d(num_features=self.in_channels_inner_1),\n",
    "        )\n",
    "        #-----------------------------------\n",
    "        #layer1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            #nn.BatchNorm1d(num_features=self.in_channels_inner_1),\n",
    "            nn.Linear(self.in_channels_inner_1, self.out_channels_inner_1),\n",
    "            #nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "            #nn.BatchNorm1d(self.params_size+self.features_size),\n",
    "            #nn.BatchNorm1d(3),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output0 = self.layer0(x).view(-1, self.in_channels_inner_1)\n",
    "        output1 = self.layer1(output0).view(-1, self.out_channels, self.out_length)\n",
    "        return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class model_2(nn.Module):\\n    def __init__(self, in_channels, in_length, out_channels, out_length, batch_size):\\n        super().__init__()\\n        #-----------------------------------\\n        #var\\n        self.in_channels = in_channels\\n        self.in_length = in_length\\n        self.out_channels = out_channels\\n        self.out_length = out_length\\n        self.batch_size = batch_size\\n        #-----------------------------------\\n        #DNN\\n        self.in_channels_inner_1 = self.in_channels*self.in_length\\n        self.out_channels_inner_1 = 30\\n        #-----------------------------------\\n        #DNN\\n        self.in_channels_inner_2 = self.out_channels_inner_1\\n        self.out_channels_inner_2 = self.out_channels*self.out_length\\n        #-----------------------------------\\n        #layer1\\n        self.layer1 = nn.Sequential(\\n            #-----------------------------------\\n            nn.BatchNorm1d(num_features=self.in_channels_inner_1),\\n            nn.Linear(self.in_channels_inner_1, self.out_channels_inner_1),\\n            nn.ReLU(),\\n            nn.Sigmoid(),\\n        )\\n        #-----------------------------------\\n        #layer2\\n        self.layer2 = nn.Sequential(\\n            #-----------------------------------\\n            nn.BatchNorm1d(num_features=self.in_channels_inner_2),\\n            nn.Linear(self.in_channels_inner_2, self.out_channels_inner_2),\\n            nn.ReLU(),\\n            nn.Sigmoid(),\\n        )\\n\\n    def forward(self, x):\\n        x = x.view(-1, self.in_channels_inner_1)\\n        output1 = self.layer1(x)\\n        output2 = self.layer2(output1)\\n        output2 = output2.view(-1, self.out_channels, self.out_length)\\n        return output2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class model_2(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length, batch_size):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #var\n",
    "        self.in_channels = in_channels\n",
    "        self.in_length = in_length\n",
    "        self.out_channels = out_channels\n",
    "        self.out_length = out_length\n",
    "        self.batch_size = batch_size\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_1 = self.in_channels*self.in_length\n",
    "        self.out_channels_inner_1 = 30\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        self.in_channels_inner_2 = self.out_channels_inner_1\n",
    "        self.out_channels_inner_2 = self.out_channels*self.out_length\n",
    "        #-----------------------------------\n",
    "        #layer1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.BatchNorm1d(num_features=self.in_channels_inner_1),\n",
    "            nn.Linear(self.in_channels_inner_1, self.out_channels_inner_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        #-----------------------------------\n",
    "        #layer2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.BatchNorm1d(num_features=self.in_channels_inner_2),\n",
    "            nn.Linear(self.in_channels_inner_2, self.out_channels_inner_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.in_channels_inner_1)\n",
    "        output1 = self.layer1(x)\n",
    "        output2 = self.layer2(output1)\n",
    "        output2 = output2.view(-1, self.out_channels, self.out_length)\n",
    "        return output2\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union\n",
    "\n",
    "#---------------------------------------------\n",
    "# custom dataset\n",
    "#https://discuss.pytorch.org/t/custom-data-loader-for-big-data/129361\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dir_X:str, path_dir_Y:str, n_test:Union[int,float], n_val:Union[int,float], batch_size:int): # n_test -> float:ratio of test, int:number of test\n",
    "        #-----------------\n",
    "        # batch_size\n",
    "        self.batch_size = batch_size\n",
    "        # path_dir_X, path_dir_Y\n",
    "        self.path_dir_X = path_dir_X\n",
    "        self.path_dir_Y = path_dir_Y\n",
    "        # list_file_name_all\n",
    "        self.list_file_name_all = os.listdir(path_dir_X)\n",
    "        # n_data_all\n",
    "        self.n_data_all = len(self.list_file_name_all)\n",
    "        #check\n",
    "        if len(os.listdir(path_dir_X)) != len(os.listdir(path_dir_Y)):\n",
    "            raise ValueError(\"error!!!\")\n",
    "        if len(set(os.listdir(path_dir_X)) - set(os.listdir(path_dir_Y))) != 0:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # suffle\n",
    "        random.shuffle(self.list_file_name_all)\n",
    "        #-----------------\n",
    "        # n_test\n",
    "        if type(n_test)==int:\n",
    "            self.n_test = n_test\n",
    "        elif type(n_test)==float:\n",
    "            self.n_test = int(len(self.list_file_name_all)*n_test)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        # n_val\n",
    "        if type(n_val)==int:\n",
    "            self.n_val = n_val\n",
    "        elif type(n_val)==float:\n",
    "            self.n_val = int(len(self.list_file_name_all)*n_val)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #check\n",
    "        if self.n_data_all <= self.n_test+self.n_val:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # list_file_name_test / _val / _train\n",
    "        self.list_file_name_test = self.list_file_name_all[:self.n_test]\n",
    "        self.list_file_name_val = self.list_file_name_all[self.n_test:self.n_test+self.n_val]\n",
    "        self.list_file_name_train = self.list_file_name_all[self.n_test+self.n_val:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_file_name_train)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        #return\n",
    "        return self.getdata(list_file_name=self.list_file_name_train, index=x)\n",
    "    \n",
    "    def getdata(self, list_file_name, index):\n",
    "        #file_name\n",
    "        file_name = list_file_name[index]\n",
    "        #data_X\n",
    "        path_file_X = \"{0}/{1}\".format(self.path_dir_X, file_name)\n",
    "        data_X = np.load(path_file_X, allow_pickle=True)\n",
    "        data_X = torch.from_numpy(data_X).to(torch.float32).requires_grad_(True)\n",
    "        #data_Y\n",
    "        path_file_Y = \"{0}/{1}\".format(self.path_dir_Y, file_name)\n",
    "        data_Y = np.load(path_file_Y, allow_pickle=True)\n",
    "        data_Y = torch.from_numpy(data_Y).to(torch.float32).requires_grad_(True)\n",
    "        #return\n",
    "        return data_X, data_Y\n",
    "    \n",
    "    def return_n_data_all(self):\n",
    "        return self.n_data_all\n",
    "    \n",
    "    def return_n_test(self):\n",
    "        return self.n_test\n",
    "    \n",
    "    def return_n_val(self):\n",
    "        return self.n_val\n",
    "    \n",
    "    def return_n_train(self):\n",
    "        return self.n_data_all - self.n_val - self.n_test\n",
    "    \n",
    "    def return_batch_size(self):\n",
    "        return self.batch_size\n",
    "    \n",
    "    def return_shape_X(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[0]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_shape_Y(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[1]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_test_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_test = torch.stack([self.getdata(self.list_file_name_test, i)[0] for i in range(self.n_test)])\n",
    "        data_Y_test = torch.stack([self.getdata(self.list_file_name_test, i)[1] for i in range(self.n_test)])\n",
    "        return data_X_test, data_Y_test\n",
    "    \n",
    "    def return_val_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_val = torch.stack([self.getdata(self.list_file_name_val, i)[0] for i in range(self.n_val)])\n",
    "        data_Y_val = torch.stack([self.getdata(self.list_file_name_val, i)[1] for i in range(self.n_val)])\n",
    "        return data_X_val, data_Y_val\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "#var\n",
    "path_dir_X = \"../data_X\"\n",
    "path_dir_Y = \"../data_Y_Task3\"\n",
    "n_test = 300\n",
    "n_val = 300\n",
    "batch_size = 5000\n",
    "\n",
    "#---------------------------------------------\n",
    "#instance\n",
    "dataset = CustomDataset(path_dir_X=path_dir_X, path_dir_Y=path_dir_Y, n_test=n_test, n_val=n_val, batch_size=batch_size)\n",
    "dataloder = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### criterion (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "#https://neptune.ai/blog/pytorch-loss-functions\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "8378\n",
      "1048\n",
      "10922\n",
      "9000\n",
      "252\n",
      "9573\n",
      "9903\n",
      "10691\n",
      "7932\n",
      "1357\n",
      "9842\n",
      "6739\n",
      "4044\n",
      "13554\n",
      "813\n",
      "6589\n",
      "12319\n",
      "8263\n",
      "5422\n",
      "6487\n",
      "2729\n",
      "13093\n",
      "1207\n",
      "12057\n",
      "11061\n",
      "14455\n",
      "9953\n",
      "8792\n",
      "7780\n",
      "544\n",
      "1391\n",
      "7342\n",
      "13582\n",
      "2873\n",
      "9172\n",
      "6117\n",
      "8532\n",
      "992\n",
      "12038\n",
      "7813\n",
      "9330\n",
      "3999\n",
      "8938\n",
      "13979\n",
      "978\n",
      "5911\n",
      "3073\n",
      "9740\n",
      "12052\n",
      "2826\n",
      "11138\n",
      "9704\n",
      "13611\n",
      "10488\n",
      "5970\n",
      "12512\n",
      "3608\n",
      "9364\n",
      "9373\n",
      "1659\n",
      "11330\n",
      "1814\n",
      "11827\n",
      "12888\n",
      "8864\n",
      "14158\n",
      "6271\n",
      "8401\n",
      "8408\n",
      "8421\n",
      "5972\n",
      "13792\n",
      "4963\n",
      "9765\n",
      "5289\n",
      "6871\n",
      "9390\n",
      "53\n",
      "11082\n",
      "13691\n",
      "14678\n",
      "3386\n",
      "15475\n",
      "1120\n",
      "4967\n",
      "2980\n",
      "14895\n",
      "11046\n",
      "7784\n",
      "14232\n",
      "1330\n",
      "3776\n",
      "13259\n",
      "11141\n",
      "1106\n",
      "9852\n",
      "10543\n",
      "640\n",
      "4882\n",
      "4268\n",
      "1483\n",
      "8358\n",
      "7688\n",
      "10534\n",
      "4772\n",
      "5786\n",
      "520\n",
      "4400\n",
      "1988\n",
      "1687\n",
      "5080\n",
      "10521\n",
      "14959\n",
      "15411\n",
      "10659\n",
      "433\n",
      "652\n",
      "5615\n",
      "9653\n",
      "8026\n",
      "15353\n",
      "14800\n",
      "4571\n",
      "3162\n",
      "15267\n",
      "3519\n",
      "909\n",
      "13037\n",
      "10853\n",
      "10387\n",
      "11948\n",
      "5672\n",
      "12939\n",
      "615\n",
      "8570\n",
      "15482\n",
      "2168\n",
      "13839\n",
      "6233\n",
      "9098\n",
      "11925\n",
      "5526\n",
      "3419\n",
      "10315\n",
      "2681\n",
      "6712\n",
      "2351\n",
      "9059\n",
      "15392\n",
      "10712\n",
      "8039\n",
      "14474\n",
      "4389\n",
      "8348\n",
      "9207\n",
      "3916\n",
      "7619\n",
      "1141\n",
      "14756\n",
      "7069\n",
      "5470\n",
      "7200\n",
      "7341\n",
      "10485\n",
      "15425\n",
      "8055\n",
      "4018\n",
      "8510\n",
      "13300\n",
      "13390\n",
      "9126\n",
      "12766\n",
      "8289\n",
      "2273\n",
      "5079\n",
      "9790\n",
      "14923\n",
      "5787\n",
      "5055\n",
      "8350\n",
      "14446\n",
      "8136\n",
      "13647\n",
      "9349\n",
      "6252\n",
      "7324\n",
      "3255\n",
      "5975\n",
      "13299\n",
      "4462\n",
      "5715\n",
      "4583\n",
      "5002\n",
      "12056\n",
      "12719\n",
      "6575\n",
      "1213\n",
      "1698\n",
      "3170\n",
      "7060\n",
      "5548\n",
      "13498\n",
      "12643\n",
      "5720\n",
      "10506\n",
      "5148\n",
      "5765\n",
      "13241\n",
      "14885\n",
      "7687\n",
      "10588\n",
      "13431\n",
      "903\n",
      "14247\n",
      "9568\n",
      "5524\n",
      "8399\n",
      "13144\n",
      "7520\n",
      "7236\n",
      "3860\n",
      "4378\n",
      "1707\n",
      "5182\n",
      "15515\n",
      "4253\n",
      "1971\n",
      "5658\n",
      "3717\n",
      "8903\n",
      "14334\n",
      "12788\n",
      "3934\n",
      "5076\n",
      "2516\n",
      "616\n",
      "4481\n",
      "1759\n",
      "7145\n",
      "218\n",
      "6795\n",
      "1356\n",
      "3144\n",
      "13192\n",
      "5337\n",
      "8403\n",
      "7593\n",
      "9804\n",
      "956\n",
      "4402\n",
      "14184\n",
      "4175\n",
      "10734\n",
      "3390\n",
      "15358\n",
      "4636\n",
      "13823\n",
      "11256\n",
      "4706\n",
      "6935\n",
      "10738\n",
      "13497\n",
      "5963\n",
      "11463\n",
      "7387\n",
      "623\n",
      "4945\n",
      "8205\n",
      "3040\n",
      "3555\n",
      "3874\n",
      "2636\n",
      "1760\n",
      "4638\n",
      "15270\n",
      "2849\n",
      "11453\n",
      "15408\n",
      "11811\n",
      "13216\n",
      "1993\n",
      "8331\n",
      "13600\n",
      "14514\n",
      "14096\n",
      "636\n",
      "858\n",
      "4314\n",
      "13878\n",
      "4713\n",
      "12086\n",
      "11931\n",
      "9980\n",
      "10213\n",
      "6725\n",
      "5152\n",
      "14150\n",
      "1803\n",
      "11217\n",
      "10219\n",
      "686\n",
      "13338\n",
      "11168\n",
      "4847\n",
      "10078\n",
      "13543\n",
      "7804\n",
      "2486\n",
      "1255\n",
      "14506\n",
      "8997\n",
      "14708\n",
      "10869\n",
      "3139\n",
      "4831\n",
      "1602\n",
      "8181\n",
      "5775\n",
      "551\n",
      "7063\n",
      "1069\n",
      "11929\n",
      "14458\n",
      "891\n",
      "3359\n",
      "11338\n",
      "11837\n",
      "13311\n",
      "1598\n",
      "3353\n",
      "249\n",
      "3020\n",
      "2845\n",
      "1209\n",
      "1959\n",
      "408\n",
      "12338\n",
      "11452\n",
      "11060\n",
      "15373\n",
      "242\n",
      "13164\n",
      "8594\n",
      "4218\n",
      "10664\n",
      "6655\n",
      "13414\n",
      "9925\n",
      "14393\n",
      "11631\n",
      "6424\n",
      "10862\n",
      "14123\n",
      "11331\n",
      "430\n",
      "13418\n",
      "14052\n",
      "9891\n",
      "10969\n",
      "12727\n",
      "10173\n",
      "5471\n",
      "15214\n",
      "11592\n",
      "7533\n",
      "7116\n",
      "5589\n",
      "8636\n",
      "3922\n",
      "12824\n",
      "2036\n",
      "11903\n",
      "13424\n",
      "15382\n",
      "2259\n",
      "1078\n",
      "1223\n",
      "3759\n",
      "4991\n",
      "4733\n",
      "3704\n",
      "486\n",
      "3443\n",
      "9313\n",
      "15442\n",
      "5728\n",
      "10038\n",
      "5004\n",
      "3571\n",
      "13136\n",
      "7849\n",
      "11327\n",
      "12651\n",
      "15128\n",
      "7525\n",
      "83\n",
      "1635\n",
      "14491\n",
      "7743\n",
      "6517\n",
      "13625\n",
      "10937\n",
      "3208\n",
      "10524\n",
      "11512\n",
      "7037\n",
      "4609\n",
      "1823\n",
      "2517\n",
      "14954\n",
      "2236\n",
      "2130\n",
      "5729\n",
      "2664\n",
      "13671\n",
      "4137\n",
      "6892\n",
      "15030\n",
      "1502\n",
      "12796\n",
      "15507\n",
      "10292\n",
      "6024\n",
      "962\n",
      "9636\n",
      "4144\n",
      "1634\n",
      "14020\n",
      "9534\n",
      "10285\n",
      "12941\n",
      "5407\n",
      "8437\n",
      "48\n",
      "4038\n",
      "4525\n",
      "8481\n",
      "2640\n",
      "13042\n",
      "5805\n",
      "11867\n",
      "12143\n",
      "11124\n",
      "1058\n",
      "1623\n",
      "10169\n",
      "12920\n",
      "859\n",
      "3234\n",
      "13232\n",
      "12287\n",
      "1311\n",
      "4792\n",
      "15151\n",
      "12865\n",
      "6641\n",
      "4182\n",
      "13872\n",
      "9066\n",
      "2282\n",
      "814\n",
      "9\n",
      "3840\n",
      "9282\n",
      "4658\n",
      "12509\n",
      "14165\n",
      "14977\n",
      "11053\n",
      "12371\n",
      "10608\n",
      "11013\n",
      "11261\n",
      "6234\n",
      "2287\n",
      "13636\n",
      "9472\n",
      "11548\n",
      "2900\n",
      "6321\n",
      "13049\n",
      "4377\n",
      "13578\n",
      "4298\n",
      "2605\n",
      "13791\n",
      "2207\n",
      "6936\n",
      "2493\n",
      "2562\n",
      "4999\n",
      "13488\n",
      "9592\n",
      "10402\n",
      "1240\n",
      "11661\n",
      "5515\n",
      "5261\n",
      "5561\n",
      "481\n",
      "5284\n",
      "8469\n",
      "12115\n",
      "12653\n",
      "3897\n",
      "8955\n",
      "11125\n",
      "8279\n",
      "12419\n",
      "15359\n",
      "3322\n",
      "12710\n",
      "1938\n",
      "11620\n",
      "10774\n",
      "3783\n",
      "6471\n",
      "3374\n",
      "10802\n",
      "4716\n",
      "4618\n",
      "4285\n",
      "1695\n",
      "12976\n",
      "10041\n",
      "14107\n",
      "7797\n",
      "13326\n",
      "9646\n",
      "271\n",
      "14336\n",
      "14711\n",
      "15516\n",
      "10680\n",
      "8387\n",
      "3021\n",
      "3135\n",
      "15474\n",
      "5696\n",
      "3951\n",
      "7309\n",
      "4254\n",
      "11869\n",
      "10867\n",
      "5183\n",
      "1114\n",
      "5384\n",
      "3596\n",
      "3733\n",
      "9139\n",
      "4064\n",
      "11865\n",
      "12911\n",
      "10092\n",
      "1463\n",
      "140\n",
      "13832\n",
      "11696\n",
      "12094\n",
      "1805\n",
      "12288\n",
      "3435\n",
      "3281\n",
      "14637\n",
      "14911\n",
      "4979\n",
      "7179\n",
      "2742\n",
      "9611\n",
      "7944\n",
      "10128\n",
      "8297\n",
      "10040\n",
      "7012\n",
      "10585\n",
      "11326\n",
      "7929\n",
      "13797\n",
      "7558\n",
      "10999\n",
      "4729\n",
      "6947\n",
      "13739\n",
      "13945\n",
      "4436\n",
      "7865\n",
      "9240\n",
      "12106\n",
      "5028\n",
      "4262\n",
      "3495\n",
      "2031\n",
      "7014\n",
      "7785\n",
      "9994\n",
      "8281\n",
      "337\n",
      "3420\n",
      "2360\n",
      "5856\n",
      "3452\n",
      "15064\n",
      "9581\n",
      "1212\n",
      "1167\n",
      "14484\n",
      "14872\n",
      "6237\n",
      "12659\n",
      "13598\n",
      "12418\n",
      "12668\n",
      "4091\n",
      "15338\n",
      "14428\n",
      "14453\n",
      "11722\n",
      "12293\n",
      "14827\n",
      "15354\n",
      "5583\n",
      "8671\n",
      "13820\n",
      "10831\n",
      "10191\n",
      "13942\n",
      "8783\n",
      "12330\n",
      "2075\n",
      "7613\n",
      "2117\n",
      "15366\n",
      "4131\n",
      "3770\n",
      "10517\n",
      "7170\n",
      "6639\n",
      "7004\n",
      "5913\n",
      "15285\n",
      "818\n",
      "10871\n",
      "228\n",
      "11444\n",
      "11728\n",
      "6224\n",
      "5636\n",
      "241\n",
      "9155\n",
      "6209\n",
      "7651\n",
      "6843\n",
      "1869\n",
      "2575\n",
      "11760\n",
      "14631\n",
      "6306\n",
      "12566\n",
      "3524\n",
      "821\n",
      "224\n",
      "9468\n",
      "575\n",
      "5829\n",
      "5478\n",
      "5225\n",
      "3460\n",
      "12977\n",
      "7195\n",
      "12350\n",
      "5045\n",
      "3761\n",
      "13297\n",
      "8599\n",
      "10018\n",
      "13457\n",
      "6395\n",
      "1918\n",
      "9752\n",
      "3054\n",
      "4867\n",
      "14262\n",
      "5621\n",
      "7906\n",
      "3873\n",
      "6747\n",
      "385\n",
      "8767\n",
      "2422\n",
      "6503\n",
      "14074\n",
      "9514\n",
      "5390\n",
      "2708\n",
      "11373\n",
      "14249\n",
      "13749\n",
      "9764\n",
      "15397\n",
      "7808\n",
      "9582\n",
      "14132\n",
      "3114\n",
      "13090\n",
      "8460\n",
      "14198\n",
      "915\n",
      "6212\n",
      "7649\n",
      "8242\n",
      "13693\n",
      "9851\n",
      "13808\n",
      "13996\n",
      "3346\n",
      "8948\n",
      "10123\n",
      "1510\n",
      "9533\n",
      "3956\n",
      "1681\n",
      "3933\n",
      "9787\n",
      "14388\n",
      "15129\n",
      "5077\n",
      "1554\n",
      "3055\n",
      "4720\n",
      "9700\n",
      "5777\n",
      "9554\n",
      "3178\n",
      "3702\n",
      "14936\n",
      "14747\n",
      "1924\n",
      "9854\n",
      "1725\n",
      "11232\n",
      "13707\n",
      "1359\n",
      "13085\n",
      "11480\n",
      "7397\n",
      "4591\n",
      "9197\n",
      "9819\n",
      "3372\n",
      "1892\n",
      "7567\n",
      "4626\n",
      "3305\n",
      "14500\n",
      "3730\n",
      "14061\n",
      "4331\n",
      "11799\n",
      "13722\n",
      "8887\n",
      "11806\n",
      "8338\n",
      "7244\n",
      "12034\n",
      "8760\n",
      "9433\n",
      "8672\n",
      "1261\n",
      "5397\n",
      "9558\n",
      "3714\n",
      "15375\n",
      "7465\n",
      "7374\n",
      "5484\n",
      "148\n",
      "15118\n",
      "15125\n",
      "949\n",
      "15148\n",
      "13638\n",
      "12477\n",
      "2791\n",
      "10246\n",
      "10954\n",
      "10728\n",
      "5491\n",
      "12919\n",
      "11308\n",
      "13620\n",
      "5511\n",
      "7998\n",
      "9347\n",
      "12400\n",
      "14251\n",
      "2617\n",
      "3163\n",
      "8776\n",
      "12476\n",
      "10962\n",
      "2424\n",
      "9195\n",
      "6330\n",
      "12841\n",
      "6258\n",
      "10187\n",
      "9763\n",
      "15071\n",
      "12385\n",
      "5931\n",
      "1880\n",
      "2871\n",
      "6976\n",
      "8105\n",
      "13861\n",
      "5492\n",
      "8832\n",
      "1995\n",
      "2968\n",
      "1327\n",
      "9448\n",
      "10666\n",
      "14828\n",
      "4631\n",
      "3186\n",
      "11596\n",
      "10895\n",
      "2994\n",
      "5664\n",
      "6400\n",
      "4649\n",
      "5705\n",
      "7482\n",
      "7898\n",
      "3830\n",
      "11793\n",
      "5964\n",
      "15309\n",
      "3329\n",
      "597\n",
      "10980\n",
      "14333\n",
      "3780\n",
      "5940\n",
      "1894\n",
      "8215\n",
      "9829\n",
      "10495\n",
      "12179\n",
      "5867\n",
      "15476\n",
      "10466\n",
      "2801\n",
      "3071\n",
      "7451\n",
      "10027\n",
      "2835\n",
      "7210\n",
      "11158\n",
      "9405\n",
      "13689\n",
      "653\n",
      "1517\n",
      "12980\n",
      "7600\n",
      "2203\n",
      "2635\n",
      "12290\n",
      "13467\n",
      "1733\n",
      "13806\n",
      "9345\n",
      "14525\n",
      "11765\n",
      "10058\n",
      "4518\n",
      "11966\n",
      "3692\n",
      "13284\n",
      "9137\n",
      "14862\n",
      "14845\n",
      "1791\n",
      "2300\n",
      "6733\n",
      "6933\n",
      "327\n",
      "6113\n",
      "733\n",
      "10072\n",
      "7788\n",
      "11603\n",
      "10017\n",
      "4252\n",
      "6205\n",
      "3912\n",
      "75\n",
      "570\n",
      "12555\n",
      "10545\n",
      "11288\n",
      "13980\n",
      "14291\n",
      "13024\n",
      "3689\n",
      "15165\n",
      "1764\n",
      "10106\n",
      "7581\n",
      "8554\n",
      "11703\n",
      "5286\n",
      "12267\n",
      "7997\n",
      "7419\n",
      "2638\n",
      "105\n",
      "14304\n",
      "7198\n",
      "6428\n",
      "15265\n",
      "5006\n",
      "9217\n",
      "7538\n",
      "8834\n",
      "4042\n",
      "9279\n",
      "13572\n",
      "8030\n",
      "3434\n",
      "12819\n",
      "15468\n",
      "10947\n",
      "3973\n",
      "12271\n",
      "13796\n",
      "210\n",
      "9809\n",
      "6211\n",
      "865\n",
      "2859\n",
      "2302\n",
      "5871\n",
      "613\n",
      "2245\n",
      "3546\n",
      "6022\n",
      "1094\n",
      "11491\n",
      "14136\n",
      "7068\n",
      "15221\n",
      "12356\n",
      "15138\n",
      "9474\n",
      "4916\n",
      "12249\n",
      "12678\n",
      "11619\n",
      "2089\n",
      "1907\n",
      "10374\n",
      "1788\n",
      "4193\n",
      "8265\n",
      "5627\n",
      "9286\n",
      "4590\n",
      "826\n",
      "7467\n",
      "13183\n",
      "12383\n",
      "3577\n",
      "10363\n",
      "8000\n",
      "8045\n",
      "763\n",
      "8615\n",
      "1866\n",
      "6495\n",
      "12543\n",
      "3182\n",
      "10228\n",
      "4657\n",
      "1202\n",
      "2975\n",
      "9146\n",
      "12723\n",
      "10272\n",
      "9930\n",
      "7081\n",
      "10610\n",
      "13026\n",
      "7354\n",
      "4580\n",
      "4589\n",
      "4751\n",
      "15347\n",
      "1941\n",
      "9775\n",
      "10225\n",
      "11143\n",
      "11097\n",
      "12442\n",
      "9544\n",
      "10331\n",
      "6911\n",
      "14472\n",
      "5941\n",
      "14515\n",
      "14108\n",
      "15495\n",
      "31\n",
      "8699\n",
      "8031\n",
      "2294\n",
      "1386\n",
      "8769\n",
      "11304\n",
      "8364\n",
      "2077\n",
      "7301\n",
      "1551\n",
      "5468\n",
      "4183\n",
      "3194\n",
      "7714\n",
      "8598\n",
      "11947\n",
      "5679\n",
      "15141\n",
      "14264\n",
      "3404\n",
      "1037\n",
      "11337\n",
      "9962\n",
      "6972\n",
      "4475\n",
      "13177\n",
      "791\n",
      "1273\n",
      "15182\n",
      "3224\n",
      "7318\n",
      "6599\n",
      "15028\n",
      "314\n",
      "14707\n",
      "787\n",
      "10452\n",
      "511\n",
      "4157\n",
      "749\n",
      "3200\n",
      "8360\n",
      "8493\n",
      "928\n",
      "381\n",
      "178\n",
      "11857\n",
      "8676\n",
      "2611\n",
      "10423\n",
      "5645\n",
      "8761\n",
      "5419\n",
      "1903\n",
      "10724\n",
      "10903\n",
      "9527\n",
      "5528\n",
      "10710\n",
      "1696\n",
      "12194\n",
      "13877\n",
      "14802\n",
      "9483\n",
      "11040\n",
      "7033\n",
      "3091\n",
      "14654\n",
      "4850\n",
      "12615\n",
      "14427\n",
      "15084\n",
      "15196\n",
      "7258\n",
      "5546\n",
      "2917\n",
      "1671\n",
      "1285\n",
      "46\n",
      "10714\n",
      "11411\n",
      "13238\n",
      "933\n",
      "7957\n",
      "659\n",
      "14706\n",
      "8318\n",
      "14237\n",
      "7364\n",
      "12270\n",
      "5329\n",
      "6620\n",
      "14940\n",
      "7328\n",
      "831\n",
      "251\n",
      "12468\n",
      "13150\n",
      "6323\n",
      "1279\n",
      "2576\n",
      "4780\n",
      "6657\n",
      "5643\n",
      "5060\n",
      "7217\n",
      "7417\n",
      "982\n",
      "4356\n",
      "13993\n",
      "8356\n",
      "3473\n",
      "8711\n",
      "10816\n",
      "10389\n",
      "2173\n",
      "14858\n",
      "326\n",
      "8368\n",
      "14205\n",
      "8748\n",
      "14315\n",
      "14721\n",
      "15010\n",
      "2868\n",
      "6682\n",
      "10019\n",
      "2758\n",
      "10378\n",
      "2816\n",
      "13985\n",
      "10739\n",
      "15217\n",
      "4247\n",
      "4134\n",
      "3181\n",
      "3550\n",
      "13856\n",
      "8087\n",
      "9235\n",
      "3300\n",
      "6832\n",
      "2874\n",
      "4280\n",
      "3283\n",
      "4354\n",
      "2933\n",
      "5125\n",
      "10310\n",
      "4815\n",
      "10309\n",
      "13795\n",
      "11972\n",
      "14229\n",
      "10700\n",
      "12890\n",
      "5113\n",
      "9890\n",
      "8705\n",
      "10158\n",
      "10157\n",
      "5736\n",
      "8376\n",
      "14075\n",
      "14537\n",
      "3635\n",
      "4696\n",
      "2283\n",
      "3003\n",
      "4048\n",
      "13778\n",
      "7591\n",
      "9481\n",
      "2898\n",
      "12343\n",
      "12670\n",
      "9886\n",
      "14632\n",
      "10034\n",
      "11474\n",
      "3641\n",
      "8930\n",
      "61\n",
      "629\n",
      "10561\n",
      "13282\n",
      "6375\n",
      "1613\n",
      "4804\n",
      "4734\n",
      "10440\n",
      "9102\n",
      "756\n",
      "4670\n",
      "6051\n",
      "97\n",
      "3523\n",
      "8438\n",
      "14021\n",
      "3358\n",
      "3852\n",
      "11778\n",
      "6286\n",
      "13011\n",
      "6215\n",
      "14431\n",
      "3167\n",
      "10337\n",
      "8311\n",
      "10498\n",
      "3088\n",
      "11085\n",
      "6948\n",
      "3502\n",
      "93\n",
      "12299\n",
      "6612\n",
      "1233\n",
      "6687\n",
      "7246\n",
      "9115\n",
      "576\n",
      "4056\n",
      "1296\n",
      "8312\n",
      "743\n",
      "13538\n",
      "12786\n",
      "6807\n",
      "50\n",
      "6483\n",
      "1205\n",
      "6570\n",
      "12582\n",
      "14391\n",
      "2593\n",
      "824\n",
      "3217\n",
      "6296\n",
      "5029\n",
      "13388\n",
      "7091\n",
      "13934\n",
      "740\n",
      "2574\n",
      "10971\n",
      "8047\n",
      "11985\n",
      "14581\n",
      "857\n",
      "8132\n",
      "10341\n",
      "14208\n",
      "15077\n",
      "12087\n",
      "5094\n",
      "5520\n",
      "2957\n",
      "8747\n",
      "9981\n",
      "2204\n",
      "11582\n",
      "155\n",
      "11316\n",
      "2939\n",
      "9010\n",
      "549\n",
      "5838\n",
      "4020\n",
      "6659\n",
      "3370\n",
      "2666\n",
      "12456\n",
      "8514\n",
      "873\n",
      "2804\n",
      "13710\n",
      "9985\n",
      "10009\n",
      "9299\n",
      "9200\n",
      "10839\n",
      "12117\n",
      "10834\n",
      "15239\n",
      "5953\n",
      "6717\n",
      "14790\n",
      "4856\n",
      "12914\n",
      "13201\n",
      "12730\n",
      "4926\n",
      "449\n",
      "1615\n",
      "14980\n",
      "3142\n",
      "5903\n",
      "5298\n",
      "13184\n",
      "8131\n",
      "10783\n",
      "2223\n",
      "6516\n",
      "12720\n",
      "4942\n",
      "13447\n",
      "3622\n",
      "9170\n",
      "2969\n",
      "3966\n",
      "11738\n",
      "13108\n",
      "15052\n",
      "10510\n",
      "13757\n",
      "9794\n",
      "3599\n",
      "213\n",
      "3580\n",
      "1688\n",
      "10528\n",
      "14457\n",
      "6505\n",
      "7758\n",
      "12604\n",
      "3615\n",
      "1816\n",
      "4332\n",
      "923\n",
      "9897\n",
      "727\n",
      "4081\n",
      "8474\n",
      "323\n",
      "9863\n",
      "735\n",
      "8056\n",
      "4538\n",
      "4401\n",
      "12734\n",
      "12448\n",
      "1136\n",
      "9988\n",
      "529\n",
      "13394\n",
      "4612\n",
      "4470\n",
      "1596\n",
      "11258\n",
      "3381\n",
      "8424\n",
      "13165\n",
      "1336\n",
      "13544\n",
      "12379\n",
      "13221\n",
      "12244\n",
      "7403\n",
      "13155\n",
      "11128\n",
      "14382\n",
      "11251\n",
      "13967\n",
      "10551\n",
      "6362\n",
      "7547\n",
      "1173\n",
      "8931\n",
      "1561\n",
      "12113\n",
      "2678\n",
      "8782\n",
      "1630\n",
      "11873\n",
      "4800\n",
      "518\n",
      "5313\n",
      "6858\n",
      "13639\n",
      "5383\n",
      "15368\n",
      "11954\n",
      "5900\n",
      "1314\n",
      "9134\n",
      "8359\n",
      "3104\n",
      "4599\n",
      "5588\n",
      "11359\n",
      "8568\n",
      "5602\n",
      "9816\n",
      "11625\n",
      "3335\n",
      "8383\n",
      "13802\n",
      "403\n",
      "2475\n",
      "5381\n",
      "10083\n",
      "7843\n",
      "556\n",
      "7833\n",
      "7172\n",
      "15013\n",
      "11707\n",
      "11949\n",
      "8678\n",
      "11746\n",
      "196\n",
      "13196\n",
      "864\n",
      "10911\n",
      "127\n",
      "14558\n",
      "14401\n",
      "6407\n",
      "8198\n",
      "10062\n",
      "10462\n",
      "10250\n",
      "9396\n",
      "265\n",
      "14448\n",
      "1587\n",
      "546\n",
      "13278\n",
      "8762\n",
      "6047\n",
      "11087\n",
      "7568\n",
      "4694\n",
      "1320\n",
      "1416\n",
      "11399\n",
      "13629\n",
      "6559\n",
      "14511\n",
      "6652\n",
      "14928\n",
      "14128\n",
      "4396\n",
      "6866\n",
      "3938\n",
      "6615\n",
      "4386\n",
      "13730\n",
      "10798\n",
      "1446\n",
      "7580\n",
      "9045\n",
      "8082\n",
      "1772\n",
      "10022\n",
      "11667\n",
      "12395\n",
      "1490\n",
      "13590\n",
      "12397\n",
      "3937\n",
      "14657\n",
      "3884\n",
      "5570\n",
      "1\n",
      "8435\n",
      "1896\n",
      "14331\n",
      "9258\n",
      "11177\n",
      "4167\n",
      "5200\n",
      "9719\n",
      "9510\n",
      "2866\n",
      "729\n",
      "8066\n",
      "491\n",
      "15160\n",
      "2138\n",
      "10354\n",
      "1728\n",
      "4738\n",
      "12898\n",
      "3449\n",
      "6699\n",
      "10280\n",
      "12223\n",
      "12681\n",
      "9447\n",
      "10844\n",
      "13852\n",
      "2011\n",
      "9751\n",
      "14209\n",
      "14487\n",
      "2571\n",
      "342\n",
      "6241\n",
      "8343\n",
      "8143\n",
      "11858\n",
      "3859\n",
      "15512\n",
      "14266\n",
      "9795\n",
      "14230\n",
      "1166\n",
      "6824\n",
      "6363\n",
      "12248\n",
      "9497\n",
      "2985\n",
      "1757\n",
      "3955\n",
      "5176\n",
      "5923\n",
      "6710\n",
      "3077\n",
      "5415\n",
      "4318\n",
      "6787\n",
      "6263\n",
      "230\n",
      "11375\n",
      "13830\n",
      "10792\n",
      "5226\n",
      "8456\n",
      "10518\n",
      "1711\n",
      "3458\n",
      "7989\n",
      "5023\n",
      "2440\n",
      "4602\n",
      "1128\n",
      "12601\n",
      "109\n",
      "7890\n",
      "15255\n",
      "2225\n",
      "14008\n",
      "13785\n",
      "7486\n",
      "1243\n",
      "10846\n",
      "4138\n",
      "12487\n",
      "6642\n",
      "12712\n",
      "5648\n",
      "8477\n",
      "8389\n",
      "10742\n",
      "4367\n",
      "102\n",
      "8326\n",
      "14097\n",
      "14669\n",
      "9178\n",
      "9348\n",
      "14149\n",
      "11842\n",
      "2232\n",
      "8941\n",
      "5624\n",
      "1750\n",
      "5762\n",
      "1660\n",
      "2227\n",
      "3709\n",
      "2206\n",
      "6671\n",
      "331\n",
      "5817\n",
      "8344\n",
      "15492\n",
      "10281\n",
      "2771\n",
      "2847\n",
      "143\n",
      "1553\n",
      "11995\n",
      "13059\n",
      "6554\n",
      "8320\n",
      "8650\n",
      "2800\n",
      "2258\n",
      "11732\n",
      "3240\n",
      "11526\n",
      "2439\n",
      "13991\n",
      "9750\n",
      "3946\n",
      "11705\n",
      "2777\n",
      "11349\n",
      "6820\n",
      "10811\n",
      "13154\n",
      "14933\n",
      "9273\n",
      "14843\n",
      "8886\n",
      "11501\n",
      "12515\n",
      "6786\n",
      "11421\n",
      "10066\n",
      "12540\n",
      "967\n",
      "10008\n",
      "7234\n",
      "14735\n",
      "7866\n",
      "4949\n",
      "7278\n",
      "10843\n",
      "15143\n",
      "3303\n",
      "7680\n",
      "6907\n",
      "11448\n",
      "5164\n",
      "10140\n",
      "1560\n",
      "9425\n",
      "1224\n",
      "12854\n",
      "8625\n",
      "10355\n",
      "4726\n",
      "2270\n",
      "9243\n",
      "10020\n",
      "1300\n",
      "10555\n",
      "4931\n",
      "2365\n",
      "6403\n",
      "2612\n",
      "8759\n",
      "3442\n",
      "11285\n",
      "15147\n",
      "1539\n",
      "8262\n",
      "2116\n",
      "3698\n",
      "12207\n",
      "4913\n",
      "8629\n",
      "8101\n",
      "8902\n",
      "12190\n",
      "11286\n",
      "9786\n",
      "6006\n",
      "12832\n",
      "5812\n",
      "8373\n",
      "8226\n",
      "5848\n",
      "15471\n",
      "9671\n",
      "3645\n",
      "6294\n",
      "1962\n",
      "9494\n",
      "14410\n",
      "134\n",
      "13526\n",
      "5788\n",
      "11780\n",
      "2620\n",
      "805\n",
      "4223\n",
      "2093\n",
      "14890\n",
      "689\n",
      "14124\n",
      "14553\n",
      "13261\n",
      "5663\n",
      "14683\n",
      "15409\n",
      "7435\n",
      "9697\n",
      "7415\n",
      "11290\n",
      "6668\n",
      "7571\n",
      "8794\n",
      "1482\n",
      "4903\n",
      "1305\n",
      "13160\n",
      "9732\n",
      "5535\n",
      "3510\n",
      "14374\n",
      "5440\n",
      "6447\n",
      "361\n",
      "590\n",
      "9078\n",
      "5908\n",
      "8890\n",
      "12523\n",
      "4630\n",
      "2261\n",
      "7546\n",
      "6765\n",
      "7869\n",
      "4510\n",
      "8429\n",
      "13449\n",
      "5605\n",
      "4910\n",
      "3991\n",
      "5276\n",
      "14770\n",
      "12844\n",
      "7667\n",
      "6189\n",
      "5373\n",
      "9642\n",
      "4506\n",
      "13550\n",
      "2923\n",
      "8975\n",
      "6284\n",
      "14422\n",
      "6236\n",
      "8735\n",
      "5325\n",
      "6722\n",
      "2977\n",
      "10920\n",
      "3744\n",
      "11051\n",
      "10552\n",
      "3099\n",
      "12137\n",
      "15334\n",
      "6335\n",
      "4531\n",
      "8286\n",
      "13433\n",
      "2842\n",
      "12795\n",
      "2728\n",
      "14212\n",
      "3230\n",
      "4281\n",
      "14552\n",
      "52\n",
      "8669\n",
      "9924\n",
      "6353\n",
      "11441\n",
      "4295\n",
      "3165\n",
      "59\n",
      "6995\n",
      "7212\n",
      "5874\n",
      "7535\n",
      "10711\n",
      "12260\n",
      "6327\n",
      "9770\n",
      "13663\n",
      "14167\n",
      "3326\n",
      "9777\n",
      "12809\n",
      "1994\n",
      "2028\n",
      "13444\n",
      "14877\n",
      "1372\n",
      "13604\n",
      "12092\n",
      "3339\n",
      "12872\n",
      "1664\n",
      "3242\n",
      "1073\n",
      "11372\n",
      "901\n",
      "7510\n",
      "4059\n",
      "13157\n",
      "8396\n",
      "6756\n",
      "4984\n",
      "12280\n",
      "15480\n",
      "6414\n",
      "557\n",
      "1170\n",
      "1282\n",
      "7823\n",
      "15099\n",
      "532\n",
      "7921\n",
      "9859\n",
      "15451\n",
      "4671\n",
      "8621\n",
      "15352\n",
      "6322\n",
      "7574\n",
      "10081\n",
      "12055\n",
      "14659\n",
      "10171\n",
      "13293\n",
      "12868\n",
      "9628\n",
      "1025\n",
      "2153\n",
      "4244\n",
      "8785\n",
      "2906\n",
      "5319\n",
      "3369\n",
      "7105\n",
      "8187\n",
      "2007\n",
      "8923\n",
      "9152\n",
      "7361\n",
      "11241\n",
      "5137\n",
      "1488\n",
      "14490\n",
      "14784\n",
      "8546\n",
      "4965\n",
      "12817\n",
      "132\n",
      "9600\n",
      "11822\n",
      "3397\n",
      "2544\n",
      "5957\n",
      "4278\n",
      "5826\n",
      "6157\n",
      "2700\n",
      "9284\n",
      "13541\n",
      "15219\n",
      "13531\n",
      "12570\n",
      "13397\n",
      "11215\n",
      "11324\n",
      "2012\n",
      "3023\n",
      "12409\n",
      "14447\n",
      "10572\n",
      "5538\n",
      "14424\n",
      "7770\n",
      "6373\n",
      "4754\n",
      "4468\n",
      "14283\n",
      "2878\n",
      "4732\n",
      "5886\n",
      "2190\n",
      "3636\n",
      "13262\n",
      "3078\n",
      "8416\n",
      "10115\n",
      "10289\n",
      "8482\n",
      "13657\n",
      "11588\n",
      "7550\n",
      "5246\n",
      "10828\n",
      "1879\n",
      "5280\n",
      "14946\n",
      "4724\n",
      "1310\n",
      "4371\n",
      "1103\n",
      "277\n",
      "200\n",
      "12520\n",
      "2343\n",
      "7925\n",
      "12061\n",
      "8561\n",
      "13256\n",
      "387\n",
      "1928\n",
      "11748\n",
      "13395\n",
      "2690\n",
      "7214\n",
      "6267\n",
      "1774\n",
      "1308\n",
      "5823\n",
      "13212\n",
      "10819\n",
      "13224\n",
      "9615\n",
      "7979\n",
      "1367\n",
      "11890\n",
      "11896\n",
      "7251\n",
      "9394\n",
      "3344\n",
      "9792\n",
      "12502\n",
      "1191\n",
      "14719\n",
      "1987\n",
      "11642\n",
      "2813\n",
      "13080\n",
      "10032\n",
      "13096\n",
      "12569\n",
      "7519\n",
      "1181\n",
      "3765\n",
      "2974\n",
      "11027\n",
      "5677\n",
      "11509\n",
      "14268\n",
      "4491\n",
      "13758\n",
      "2466\n",
      "504\n",
      "989\n",
      "704\n",
      "10776\n",
      "12593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m#----------------------------\u001b[39;00m\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     28\u001b[0m   model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 29\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#----------------------------\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#change the type\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[16], line 57\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m#return\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_file_name_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 65\u001b[0m, in \u001b[0;36mCustomDataset.getdata\u001b[0;34m(self, list_file_name, index)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#data_X\u001b[39;00m\n\u001b[1;32m     64\u001b[0m path_file_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_dir_X, file_name)\n\u001b[0;32m---> 65\u001b[0m data_X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_file_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m data_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data_X)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#data_Y\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/numpy/lib/_npyio_impl.py:484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    482\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/workspace/cardiac_challenge/.venv/lib64/python3.11/site-packages/numpy/lib/format.py:834\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    847\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#----------------------------\n",
    "#var (condition)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#----------------------------\n",
    "#var (train)\n",
    "num_epochs = 1000\n",
    "n_print_train_result = 1\n",
    "val_flag = True\n",
    "#----------------------------\n",
    "#var (model)\n",
    "in_channels = dataset.return_shape_X()[0]\n",
    "in_length = dataset.return_shape_X()[1]\n",
    "out_channels = dataset.return_shape_Y()[0]\n",
    "out_length = dataset.return_shape_Y()[1]\n",
    "model = model_1(in_channels, in_length, out_channels, out_length, batch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15,30,45], gamma=0.7)\n",
    "\n",
    "#----------------------------\n",
    "#results\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "#----------------------------\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "  #----------------------------\n",
    "  # train\n",
    "  model.train()\n",
    "  for i, (x, y) in enumerate(dataloder):\n",
    "    #----------------------------\n",
    "    #change the type\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    #----------------------------\n",
    "    #forward\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    #----------------------------\n",
    "    #backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    print(x)\n",
    "    print(x.requires_grad)\n",
    "    print(x.grad)\n",
    "    optimizer.step()\n",
    "    #----------------------------\n",
    "    #print & result\n",
    "    if (i+1) % n_print_train_result == 0:\n",
    "      print(f'Epoch: {epoch+1}, iter: {i+1}, train_loss: {loss: 0.4f}')\n",
    "    history[\"train_loss\"].append(loss)\n",
    "\n",
    "  #----------------------------\n",
    "  # eval\n",
    "  if val_flag == True:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      #----------------------------\n",
    "      #forward\n",
    "      x, y = dataset.return_val_data()\n",
    "      #----------------------------\n",
    "      #change the type\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "      #----------------------------\n",
    "      #forward\n",
    "      output = model(x)\n",
    "      loss = criterion(output, y)\n",
    "      #----------------------------\n",
    "      #print & result\n",
    "      history[\"val_loss\"].append(loss)\n",
    "      print(f'Epoch: {epoch+1}, val_loss: {loss: 0.4f}')\n",
    "  \n",
    "  #----------------------------\n",
    "  # scheduler\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (CustomDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Union\n",
    "\n",
    "#---------------------------------------------\n",
    "# custom dataset\n",
    "#https://discuss.pytorch.org/t/custom-data-loader-for-big-data/129361\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dir_X:str, path_dir_Y:str, n_test:Union[int,float], n_val:Union[int,float], batch_size:int): # n_test -> float:ratio of test, int:number of test\n",
    "        #-----------------\n",
    "        # batch_size\n",
    "        self.batch_size = batch_size\n",
    "        # path_dir_X, path_dir_Y\n",
    "        self.path_dir_X = path_dir_X\n",
    "        self.path_dir_Y = path_dir_Y\n",
    "        # list_file_name_all\n",
    "        self.list_file_name_all = os.listdir(path_dir_X)\n",
    "        # n_data_all\n",
    "        self.n_data_all = len(self.list_file_name_all)\n",
    "        #check\n",
    "        if len(os.listdir(path_dir_X)) != len(os.listdir(path_dir_Y)):\n",
    "            raise ValueError(\"error!!!\")\n",
    "        if len(set(os.listdir(path_dir_X)) - set(os.listdir(path_dir_Y))) != 0:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # suffle\n",
    "        random.shuffle(self.list_file_name_all)\n",
    "        #-----------------\n",
    "        # n_test\n",
    "        if type(n_test)==int:\n",
    "            self.n_test = n_test\n",
    "        elif type(n_test)==float:\n",
    "            self.n_test = int(len(self.list_file_name_all)*n_test)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        # n_val\n",
    "        if type(n_val)==int:\n",
    "            self.n_val = n_val\n",
    "        elif type(n_val)==float:\n",
    "            self.n_val = int(len(self.list_file_name_all)*n_val)\n",
    "        else:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #check\n",
    "        if self.n_data_all <= self.n_test+self.n_val:\n",
    "            raise ValueError(\"error!!!\")\n",
    "        #-----------------\n",
    "        # list_file_name_test / _val / _train\n",
    "        self.list_file_name_test = self.list_file_name_all[:self.n_test]\n",
    "        self.list_file_name_val = self.list_file_name_all[self.n_test:self.n_test+self.n_val]\n",
    "        self.list_file_name_train = self.list_file_name_all[self.n_test+self.n_val:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_file_name_train)\n",
    "    \n",
    "    def __getitem__(self, x):\n",
    "        return self.getdata(list_file_name=self.list_file_name_train, index=x)\n",
    "    \n",
    "    def getdata(self, list_file_name, index):\n",
    "        #file_name\n",
    "        file_name = list_file_name[index]\n",
    "        #data_X\n",
    "        path_file_X = \"{0}/{1}\".format(self.path_dir_X, file_name)\n",
    "        data_X = np.load(path_file_X, allow_pickle=True)\n",
    "        data_X = torch.from_numpy(data_X).to(torch.float32)\n",
    "        #data_Y\n",
    "        path_file_Y = \"{0}/{1}\".format(self.path_dir_Y, file_name)\n",
    "        data_Y = np.load(path_file_Y, allow_pickle=True)\n",
    "        data_Y = torch.from_numpy(data_Y).to(torch.float32)\n",
    "        #return\n",
    "        return data_X, data_Y\n",
    "    \n",
    "    def return_n_data_all(self):\n",
    "        return self.n_data_all\n",
    "    \n",
    "    def return_n_test(self):\n",
    "        return self.n_test\n",
    "    \n",
    "    def return_n_val(self):\n",
    "        return self.n_val\n",
    "    \n",
    "    def return_n_train(self):\n",
    "        return self.n_data_all - self.n_val - self.n_test\n",
    "    \n",
    "    def return_batch_size(self):\n",
    "        return self.batch_size\n",
    "    \n",
    "    def return_shape_X(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[0]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_shape_Y(self):\n",
    "        data_sample = self.getdata(self.list_file_name_all, 0)[1]\n",
    "        return data_sample.shape\n",
    "    \n",
    "    def return_test_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_test = torch.stack([self.getdata(self.list_file_name_test, i)[0] for i in range(self.n_test)])\n",
    "        data_Y_test = torch.stack([self.getdata(self.list_file_name_test, i)[1] for i in range(self.n_test)])\n",
    "        return data_X_test, data_Y_test\n",
    "    \n",
    "    def return_val_data(self):\n",
    "        #https://www.tutorialspoint.com/how-to-join-tensors-in-pytorch\n",
    "        data_X_val = torch.stack([self.getdata(self.list_file_name_val, i)[0] for i in range(self.n_val)])\n",
    "        data_Y_val = torch.stack([self.getdata(self.list_file_name_val, i)[1] for i in range(self.n_val)])\n",
    "        return data_X_val, data_Y_val\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "#var\n",
    "path_dir_X = \"../data_X\"\n",
    "path_dir_Y = \"../data_Y_Task3\"\n",
    "n_test = 300\n",
    "n_val = 300\n",
    "batch_size = 2\n",
    "\n",
    "#---------------------------------------------\n",
    "#instance\n",
    "dataset = CustomDataset(path_dir_X=path_dir_X, path_dir_Y=path_dir_Y, n_test=n_test, n_val=n_val, batch_size=batch_size)\n",
    "dataloder = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data_X, data_Y) in enumerate(dataloder):\n",
    "   # print(data_X.shape, data_Y.shape)\n",
    "   print(data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_data_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_n_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_val_data()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.return_test_data()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dataset.return_val_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        xe11 = relu(self.e11(x))\n",
    "        xe12 = relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.e21(xp1))\n",
    "        xe22 = relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.e31(xp2))\n",
    "        xe32 = relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = relu(self.e41(xp3))\n",
    "        xe42 = relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = relu(self.e51(xp4))\n",
    "        xe52 = relu(self.e52(xe51))\n",
    "\n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = relu(self.d11(xu11))\n",
    "        xd12 = relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = relu(self.d21(xu22))\n",
    "        xd22 = relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = relu(self.d31(xu33))\n",
    "        xd32 = relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = relu(self.d41(xu44))\n",
    "        xd42 = relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_1(nn.Module):\n",
    "    def __init__(self, in_channels, in_length, out_channels, out_length):\n",
    "        super().__init__()\n",
    "        #-----------------------------------\n",
    "        #Conv1d\n",
    "        in_channels_inner_0 = in_channels; in_length_inner_0 = in_length\n",
    "        kernel_size=2; stride=1; padding=1; dilation=1; \n",
    "        out_channels_inner_0 = 30; out_length_inner_0 = (in_length_inner_0+2*padding-dilation*(kernel_size-1)-1)/stride+1\n",
    "        self.conv1d = nn.Conv1d(in_channels=in_channels_inner_0, out_channels=out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        #-----------------------------------\n",
    "        #DNN\n",
    "        in_channels_inner_1 = out_channels_inner_0*out_length_inner_0; out_channels_inner_1 = out_channels\n",
    "\n",
    "        #-----------------------------------\n",
    "        #layer0\n",
    "        self.layer0 = nn.Sequential(\n",
    "            #-----------------------------------\n",
    "            nn.Conv1d(in_channels=in_channels_inner_0, out_channels=out_channels_inner_0, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            nn.relu(),\n",
    "            nn.BatchNorm1d(out_channels_inner_0*out_length_inner_0),\n",
    "            #-----------------------------------\n",
    "            nn.Linear(in_channels_inner_1, out_channels_inner_1),\n",
    "            nn.relu(),\n",
    "            nn.Sigmoid()\n",
    "            #nn.BatchNorm1d(self.params_size+self.features_size),\n",
    "            #nn.BatchNorm1d(3),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output0 = self.layer0(x)\n",
    "        return output0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://qiita.com/sshuv/items/79d9364b8675fdc080cf\n",
    "#https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "#https://cvml-expertguide.net/terms/dl/layers/convolution-layer/\n",
    "\n",
    "\n",
    "#-----------------------------------\n",
    "#n_batch=10; n_feature=6; n_time=10; \n",
    "n_batch=1000; in_channels=75; in_length=500; \n",
    "x = torch.rand(n_batch, in_channels, in_length)\n",
    "\n",
    "#-----------------------------------\n",
    "in_channels=in_channels; out_channels=30; kernel_size=3; stride=1; padding=1; dilation=1; \n",
    "conv1d = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "\n",
    "#-----------------------------------\n",
    "n_batch=n_batch; out_channels=out_channels; out_length=int((in_length+2*padding-dilation*(kernel_size-1)-1)/stride+1)\n",
    "y = conv1d(x)\n",
    "print(n_batch, out_channels, out_length)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[-0.5562,  0.6232,  0.4818, -0.6198, -1.1521],\n",
      "        [-0.4201, -0.5103, -1.0115, -0.8218, -0.4525],\n",
      "        [-1.6068,  1.5072, -1.0673,  0.4227, -0.5018]], requires_grad=True)\n",
      "target:  tensor([[-1.3220,  1.7630,  0.7955, -1.6737, -0.9745],\n",
      "        [-0.8070, -0.5658, -0.2082, -0.9196, -1.0993],\n",
      "        [-0.0078, -0.2429, -0.4193,  0.8549, -0.9227]], requires_grad=True)\n",
      "output:  tensor(0.7171, grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.1021, -0.1520, -0.0418,  0.1405, -0.0237],\n",
      "        [ 0.0516,  0.0074, -0.1071,  0.0130,  0.0862],\n",
      "        [-0.2132,  0.2333, -0.0864, -0.0576,  0.0561]])\n",
      "tensor([[-0.1021,  0.1520,  0.0418, -0.1405,  0.0237],\n",
      "        [-0.0516, -0.0074,  0.1071, -0.0130, -0.0862],\n",
      "        [ 0.2132, -0.2333,  0.0864,  0.0576, -0.0561]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5, requires_grad=True)\n",
    "mse_loss = nn.MSELoss()\n",
    "output = mse_loss(input, target) #mse_loss(input, target), mse_loss(target, input)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "print(input.grad)\n",
    "print(target.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
